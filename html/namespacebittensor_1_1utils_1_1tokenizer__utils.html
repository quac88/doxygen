<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.6"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Bittensor: bittensor.utils.tokenizer_utils Namespace Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript">var page_layout=1;</script>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
  $(document).ready(function() { init_search(); });
/* @license-end */
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="side-nav" class="ui-resizable side-nav-resizable"><!-- do not remove this div, it is closed by doxygen! -->
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">Bittensor
   </div>
   <div id="projectbrief">Python API</div>
  </td>
 </tr>
   <tr><td colspan="2">        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <span id="MSearchSelect"                onmouseover="return searchBox.OnSearchSelectShow()"                onmouseout="return searchBox.OnSearchSelectHide()">&#160;</span>
          <input type="text" id="MSearchField" value="" placeholder="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.svg" alt=""/></a>
          </span>
        </div>
</td></tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.6 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
</div><!-- top -->
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(document).ready(function(){initNavTree('namespacebittensor_1_1utils_1_1tokenizer__utils.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div class="header">
  <div class="summary">
<a href="#func-members">Functions</a> &#124;
<a href="#var-members">Variables</a>  </div>
  <div class="headertitle"><div class="title">bittensor.utils.tokenizer_utils Namespace Reference</div></div>
</div><!--header-->
<div class="contents">
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="func-members" name="func-members"></a>
Functions</h2></td></tr>
<tr class="memitem:a31406026136d3c9b3ec5469bd1e0c0f7"><td class="memItemLeft" align="right" valign="top">Dict[int, tuple]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacebittensor_1_1utils_1_1tokenizer__utils.html#a31406026136d3c9b3ec5469bd1e0c0f7">get_tokenizer_alignment_splits</a> (List[tuple] offset_mapping, List[tuple] offset_mapping_std)</td></tr>
<tr class="separator:a31406026136d3c9b3ec5469bd1e0c0f7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a80673928d5022558ece6eb5cdc9e12be"><td class="memItemLeft" align="right" valign="top">List[tuple]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacebittensor_1_1utils_1_1tokenizer__utils.html#a80673928d5022558ece6eb5cdc9e12be">get_tokenizer_sequence_mappings</a> (List[tuple] offset_mapping, List[tuple] offset_mapping_std)</td></tr>
<tr class="separator:a80673928d5022558ece6eb5cdc9e12be"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af76d2962aae10ddc4826d5f349fd9051"><td class="memItemLeft" align="right" valign="top">List[Dict[str, torch.LongTensor]]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacebittensor_1_1utils_1_1tokenizer__utils.html#af76d2962aae10ddc4826d5f349fd9051">get_tokenizer_depth_split_map</a> (PreTrainedTokenizerBase <a class="el" href="classbittensor_1_1__tokenizer_1_1tokenizer.html">tokenizer</a>, tuple depths)</td></tr>
<tr class="separator:af76d2962aae10ddc4826d5f349fd9051"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af3a7513c8021247073b9f0f68d3e751a"><td class="memItemLeft" align="right" valign="top">torch.FloatTensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacebittensor_1_1utils_1_1tokenizer__utils.html#af3a7513c8021247073b9f0f68d3e751a">split_probs</a> (torch.FloatTensor probs, List[Dict[str, torch.Tensor]] split_map)</td></tr>
<tr class="separator:af3a7513c8021247073b9f0f68d3e751a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8a5ed01b4d5f6883475c9db9f896edbe"><td class="memItemLeft" align="right" valign="top">Tuple[torch.FloatTensor, List[tuple], torch.LongTensor]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacebittensor_1_1utils_1_1tokenizer__utils.html#a8a5ed01b4d5f6883475c9db9f896edbe">align_tokenizer_sequences</a> (torch.FloatTensor probs, List[tuple] offset_mapping, List[tuple] offset_mapping_std, PreTrainedTokenizerBase <a class="el" href="classbittensor_1_1__tokenizer_1_1tokenizer.html">tokenizer</a>, Dict[tuple, List[Dict[str, torch.Tensor]]] split_map_cache, torch.LongTensor tokens, torch.LongTensor tokens_std)</td></tr>
<tr class="separator:a8a5ed01b4d5f6883475c9db9f896edbe"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a52d7949730e4e84f9a53fd0bff3e0f22"><td class="memItemLeft" align="right" valign="top">Dict[str, Any]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacebittensor_1_1utils_1_1tokenizer__utils.html#a52d7949730e4e84f9a53fd0bff3e0f22">get_translation_map</a> (PreTrainedTokenizerBase from_tokenizer, PreTrainedTokenizerBase to_tokenizer)</td></tr>
<tr class="separator:a52d7949730e4e84f9a53fd0bff3e0f22"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a127face1324f46e879f92ee881771d90"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacebittensor_1_1utils_1_1tokenizer__utils.html#a127face1324f46e879f92ee881771d90">translate_one_to_many</a> (torch.FloatTensor probs_from, torch.FloatTensor probs_to, Dict[str, Any] translation_map)</td></tr>
<tr class="separator:a127face1324f46e879f92ee881771d90"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aacc9d76a7e49b18c20a5b44beafb8306"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacebittensor_1_1utils_1_1tokenizer__utils.html#aacc9d76a7e49b18c20a5b44beafb8306">translate_many_to_one</a> (torch.FloatTensor probs_from, torch.FloatTensor probs_to, Dict[str, Any] translation_map)</td></tr>
<tr class="separator:aacc9d76a7e49b18c20a5b44beafb8306"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac2ac0a649cfc204c56ad13913288a4f4"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacebittensor_1_1utils_1_1tokenizer__utils.html#ac2ac0a649cfc204c56ad13913288a4f4">translate_tokenizer_probs</a> (torch.FloatTensor probs, torch.FloatTensor probs_std, List[tuple] offset_mapping, List[tuple] offset_mapping_std, PreTrainedTokenizerBase <a class="el" href="classbittensor_1_1__tokenizer_1_1tokenizer.html">tokenizer</a>, PreTrainedTokenizerBase std_tokenizer, Dict[tuple, List[Dict[str, torch.Tensor]]] split_map_cache, Dict[str, Any] to_translation_map, Dict[str, Any] from_translation_map, torch.LongTensor tokens, torch.LongTensor tokens_std)</td></tr>
<tr class="separator:ac2ac0a649cfc204c56ad13913288a4f4"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa9eb42e9db4c2b1517d8ff7105c8ebd2"><td class="memItemLeft" align="right" valign="top">str&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacebittensor_1_1utils_1_1tokenizer__utils.html#aa9eb42e9db4c2b1517d8ff7105c8ebd2">get_top_probs</a> (torch.FloatTensor probs, PreTrainedTokenizerBase <a class="el" href="classbittensor_1_1__tokenizer_1_1tokenizer.html">tokenizer</a>, int amount=10)</td></tr>
<tr class="separator:aa9eb42e9db4c2b1517d8ff7105c8ebd2"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7c296be466fd118c374bf1480d26904b"><td class="memItemLeft" align="right" valign="top">torch.FloatTensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacebittensor_1_1utils_1_1tokenizer__utils.html#a7c296be466fd118c374bf1480d26904b">translate_logits_to_probs_std</a> (torch.FloatTensor logits, List[List[tuple]] offset_mapping, List[List[tuple]] offset_mapping_std, PreTrainedTokenizerBase <a class="el" href="classbittensor_1_1__tokenizer_1_1tokenizer.html">tokenizer</a>, PreTrainedTokenizerBase std_tokenizer, Dict[tuple, List[Dict[str, torch.Tensor]]] split_map_cache, Dict[str, Any] to_translation_map, Dict[str, Any] from_translation_map, torch.LongTensor tokens, torch.LongTensor tokens_std, bool skip_equivalent=True)</td></tr>
<tr class="separator:a7c296be466fd118c374bf1480d26904b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a619c2a8870fee58a3993722ad5e2d7bb"><td class="memItemLeft" align="right" valign="top">torch.Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacebittensor_1_1utils_1_1tokenizer__utils.html#a619c2a8870fee58a3993722ad5e2d7bb">topk_token_phrases</a> (torch.Tensor logits, PreTrainedTokenizerBase <a class="el" href="classbittensor_1_1__tokenizer_1_1tokenizer.html">tokenizer</a>, int topk, int ignore_index=-100)</td></tr>
<tr class="separator:a619c2a8870fee58a3993722ad5e2d7bb"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a335c0010cfa5615482ecc882408a8e0c"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacebittensor_1_1utils_1_1tokenizer__utils.html#a335c0010cfa5615482ecc882408a8e0c">compact_topk_token_phrases</a> (torch.Tensor topk_tensor)</td></tr>
<tr class="separator:a335c0010cfa5615482ecc882408a8e0c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a952b31d97e6588cd4166ee2ef319a1f0"><td class="memItemLeft" align="right" valign="top">torch.Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacebittensor_1_1utils_1_1tokenizer__utils.html#a952b31d97e6588cd4166ee2ef319a1f0">unravel_topk_token_phrases</a> (torch.Tensor compact_topk, int topk, int ignore_index=-100)</td></tr>
<tr class="separator:a952b31d97e6588cd4166ee2ef319a1f0"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab00bac4739db21de5c408746fb2d631e"><td class="memItemLeft" align="right" valign="top">Tuple[torch.Tensor, torch.Tensor]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacebittensor_1_1utils_1_1tokenizer__utils.html#ab00bac4739db21de5c408746fb2d631e">phrase_cross_entropy</a> (Union[List[List[int]], torch.Tensor] target_phrases, torch.Tensor topk_tensor, int ignore_index=-100, reduce=True, reduction='mean', int vocab_size_min=50257)</td></tr>
<tr class="separator:ab00bac4739db21de5c408746fb2d631e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a66ebcf1b07d1ee0f0ba07130967a40cc"><td class="memItemLeft" align="right" valign="top">torch.Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacebittensor_1_1utils_1_1tokenizer__utils.html#a66ebcf1b07d1ee0f0ba07130967a40cc">topk_tokens_to_vocab_size</a> (torch.Tensor topk_tensor, int vocab_size_std, int vocab_size_min=50257)</td></tr>
<tr class="separator:a66ebcf1b07d1ee0f0ba07130967a40cc"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a54a25644b9cfa6b662c66e0707692a72"><td class="memItemLeft" align="right" valign="top">bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacebittensor_1_1utils_1_1tokenizer__utils.html#a54a25644b9cfa6b662c66e0707692a72">check_tokenizer_equivalence</a> (PreTrainedTokenizerBase tokenizer_to_check, PreTrainedTokenizerBase target_tokenizer)</td></tr>
<tr class="separator:a54a25644b9cfa6b662c66e0707692a72"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:abcdd922e5075b5eb136e9f056aa547c1"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacebittensor_1_1utils_1_1tokenizer__utils.html#abcdd922e5075b5eb136e9f056aa547c1">prune_tokens</a> (torch.FloatTensor inputs, int prune_len=1, int margin=3)</td></tr>
<tr class="separator:abcdd922e5075b5eb136e9f056aa547c1"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a48b79b3d00a8fa64ade7ce9ba3db7410"><td class="memItemLeft" align="right" valign="top">List[List[List[Any]]]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacebittensor_1_1utils_1_1tokenizer__utils.html#a48b79b3d00a8fa64ade7ce9ba3db7410">pad_offsets</a> (List[List[tuple]] offsets_batch, List[List[List[Any]]] source_offsets_batch, List[List[List[Any]]] pad_offsets_batch)</td></tr>
<tr class="separator:a48b79b3d00a8fa64ade7ce9ba3db7410"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a58bc4ccb79ae645c72c7a5e4244b99d3"><td class="memItemLeft" align="right" valign="top">List[List[int]]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacebittensor_1_1utils_1_1tokenizer__utils.html#a58bc4ccb79ae645c72c7a5e4244b99d3">find_offsets</a> (str string, str substring)</td></tr>
<tr class="separator:a58bc4ccb79ae645c72c7a5e4244b99d3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8e57c9718294c345d7f4ff730970723d"><td class="memItemLeft" align="right" valign="top">Tuple[str, List[List[int]]]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacebittensor_1_1utils_1_1tokenizer__utils.html#a8e57c9718294c345d7f4ff730970723d">replace_at_offsets</a> (str string, List[List[Any]] offsets)</td></tr>
<tr class="separator:a8e57c9718294c345d7f4ff730970723d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4da460aad949bf91a0f414c1a0f30df1"><td class="memItemLeft" align="right" valign="top">Dict[str, str]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacebittensor_1_1utils_1_1tokenizer__utils.html#a4da460aad949bf91a0f414c1a0f30df1">get_special_token_pairings</a> (PreTrainedTokenizerBase from_tokenizer, PreTrainedTokenizerBase to_tokenizer)</td></tr>
<tr class="separator:a4da460aad949bf91a0f414c1a0f30df1"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6717d83c925189f86590b5ab3d88df17"><td class="memItemLeft" align="right" valign="top">Tuple[List[str], List[List[List[int]]], List[List[List[int]]], List[List[List[Any]]]]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacebittensor_1_1utils_1_1tokenizer__utils.html#a6717d83c925189f86590b5ab3d88df17">translate_special_token_text</a> (List[str] text_batch, PreTrainedTokenizerBase from_tokenizer, PreTrainedTokenizerBase to_tokenizer)</td></tr>
<tr class="separator:a6717d83c925189f86590b5ab3d88df17"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aef7aceb6bc8b3ad0d08f7fed25c876d8"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacebittensor_1_1utils_1_1tokenizer__utils.html#aef7aceb6bc8b3ad0d08f7fed25c876d8">set_vocab_len</a> (PreTrainedTokenizerBase <a class="el" href="classbittensor_1_1__tokenizer_1_1tokenizer.html">tokenizer</a>)</td></tr>
<tr class="separator:aef7aceb6bc8b3ad0d08f7fed25c876d8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a558123e572c10af14c002993c9b04f36"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacebittensor_1_1utils_1_1tokenizer__utils.html#a558123e572c10af14c002993c9b04f36">set_whitespace_preserving</a> (PreTrainedTokenizerBase <a class="el" href="classbittensor_1_1__tokenizer_1_1tokenizer.html">tokenizer</a>)</td></tr>
<tr class="separator:a558123e572c10af14c002993c9b04f36"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a066a0913f46f47503972e5df32808063"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacebittensor_1_1utils_1_1tokenizer__utils.html#a066a0913f46f47503972e5df32808063">set_std_token_phrases</a> (<a class="el" href="classbittensor_1_1__tokenizer_1_1tokenizer.html">tokenizer</a>, std_tokenizer)</td></tr>
<tr class="separator:a066a0913f46f47503972e5df32808063"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af7fd6b7e6dee158a3f991bdd93a1f559"><td class="memItemLeft" align="right" valign="top"><a id="af7fd6b7e6dee158a3f991bdd93a1f559" name="af7fd6b7e6dee158a3f991bdd93a1f559"></a>
def&#160;</td><td class="memItemRight" valign="bottom"><b>prep_tokenizer</b> (<a class="el" href="classbittensor_1_1__tokenizer_1_1tokenizer.html">tokenizer</a>, std_tokenizer=None)</td></tr>
<tr class="separator:af7fd6b7e6dee158a3f991bdd93a1f559"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="var-members" name="var-members"></a>
Variables</h2></td></tr>
<tr class="memitem:ace858cca96a45c6171798f08dfe507fb"><td class="memItemLeft" align="right" valign="top"><a id="ace858cca96a45c6171798f08dfe507fb" name="ace858cca96a45c6171798f08dfe507fb"></a>
int&#160;</td><td class="memItemRight" valign="bottom"><b>EPSILON</b> = 1e-40</td></tr>
<tr class="separator:ace858cca96a45c6171798f08dfe507fb"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><pre class="fragment"> Utils for tokenizer equivalence checking, logit translation, etc.
</pre> </div><h2 class="groupheader">Function Documentation</h2>
<a id="a8a5ed01b4d5f6883475c9db9f896edbe" name="a8a5ed01b4d5f6883475c9db9f896edbe"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a8a5ed01b4d5f6883475c9db9f896edbe">&#9670;&#160;</a></span>align_tokenizer_sequences()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> Tuple[torch.FloatTensor,
                                                                                               List[tuple],
                                                                                               torch.LongTensor] bittensor.utils.tokenizer_utils.align_tokenizer_sequences </td>
          <td>(</td>
          <td class="paramtype">torch.FloatTensor&#160;</td>
          <td class="paramname"><em>probs</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">List[tuple]&#160;</td>
          <td class="paramname"><em>offset_mapping</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">List[tuple]&#160;</td>
          <td class="paramname"><em>offset_mapping_std</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">PreTrainedTokenizerBase&#160;</td>
          <td class="paramname"><em>tokenizer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Dict[tuple, List[Dict[str, torch.Tensor]]]&#160;</td>
          <td class="paramname"><em>split_map_cache</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">torch.LongTensor&#160;</td>
          <td class="paramname"><em>tokens</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">torch.LongTensor&#160;</td>
          <td class="paramname"><em>tokens_std</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Align an input tokenization distribution to standard tokenization segments by depth-splitting
the input distribution at greedily chosen locations. Prepares the input distribution for mapping to a standard
distribution.
Args:
probs (:obj:`torch.FloatTensor`, `required`):
[sequence_len, vocab_size] Input probability distribution over a tokenizer vocabulary.
offset_mapping (:obj:`List[tuple]`, `required`):
Tokenizer offset mappings for a specific sequence [(left_0, right_0), (left_1, right_1), ...].
offset_mapping_std (:obj:`List[tuple]`, `required`):
Standard tokenizer offset mappings for a specific sequence [(left_0, right_0), (left_1, right_1), ...]
tokenizer (:obj:`PreTrainedTokenizerBase`, `required`):
Source tokenizer.
split_map_cache (:obj:`Dict[tuple, List[Dict[str, torch.Tensor]]]`, `required`):
A dictionary of depths keying split_maps of mappings from original tokens to
target tokens at each depth of the split.
tokens (:obj:`torch.LongTensor`, `required`):
[sequence_len] A sequence of tokens produced by the source tokenizer.
tokens_std (:obj:`torch.LongTensor`, `required`):
[std_sequence_len] A sequence of tokens produced by the standard tokenizer.

Returns:
aligned_probs (:obj:`torch.FloatTensor`, `required`):
[new_sequence_len, vocab_size] Aligned probability distribution over a tokenizer vocabulary.
aligned_offset_mapping (:obj:`List[tuple]`, `required`):
Tokenizer aligned offset mappings for a specific sequence [(left_0, right_0), (left_1, right_1), ...].
aligned_tokens (:obj:`torch.LongTensor`, `required`):
A sequence of aligned tokens produced by the source tokenizer.
</pre> 
</div>
</div>
<a id="a54a25644b9cfa6b662c66e0707692a72" name="a54a25644b9cfa6b662c66e0707692a72"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a54a25644b9cfa6b662c66e0707692a72">&#9670;&#160;</a></span>check_tokenizer_equivalence()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> bool bittensor.utils.tokenizer_utils.check_tokenizer_equivalence </td>
          <td>(</td>
          <td class="paramtype">PreTrainedTokenizerBase&#160;</td>
          <td class="paramname"><em>tokenizer_to_check</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">PreTrainedTokenizerBase&#160;</td>
          <td class="paramname"><em>target_tokenizer</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Is tokenizer_to_check equivalent to target_tokenizer?
    Args:
        tokenizer_to_check (:obj:`PreTrainedTokenizerBase`, `required`):
            Tokenizer to check for equivalence.
        target_tokenizer (:obj:`PreTrainedTokenizerBase`, `required`):
            Target tokenizer to check equivalence against.

    Returns:
        result (:obj:`bool`, `required`)
</pre> 
</div>
</div>
<a id="a335c0010cfa5615482ecc882408a8e0c" name="a335c0010cfa5615482ecc882408a8e0c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a335c0010cfa5615482ecc882408a8e0c">&#9670;&#160;</a></span>compact_topk_token_phrases()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def bittensor.utils.tokenizer_utils.compact_topk_token_phrases </td>
          <td>(</td>
          <td class="paramtype">torch.Tensor&#160;</td>
          <td class="paramname"><em>topk_tensor</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Compact 2D topk_tensor [batch_size, (topk + 1), max_len] by removing ignore_index padding, and also offset
tokens by 2 to preserve [0, 1] for probabilities to allow for proper unraveling demarcated by
probability boundaries.
    Args:
        topk_tensor (:obj:`torch.Tensor`, `required`):
            [batch_size, (topk + 1), max_len] tensor includes topk token probabilities (prob_k) + floor_prob
            in first column with gradients attached, with std_tokens in remaining columns with ignore_index padding.
            Content structure:
            [[[prob_k=0_b=0, tok_0_k=0_b=0, tok_1_k=0_b=0, ..., ignore_index?],
              [prob_k=1_b=0, tok_0_k=1_b=0, tok_1_k=1_b=0, ..., ignore_index?],
              [...],
              [prob_floor_b=0, ignore_index, ..., ignore_index]],
             [[prob_k=0_b=1, tok_0_k=0_b=1, tok_1_k=0_b=1, ..., ignore_index?],
              [prob_k=1_b=1, tok_0_k=1_b=1, tok_1_k=1_b=1, ..., ignore_index?],
              [...],
              [prob_floor_b=1, ignore_index, ..., ignore_index]],
             [...]]

    Returns:
        compact_topk (:obj:`torch.Tensor`, `required`):
            [sum_b(sum_k(len(phrase_k) + 1)_b)] Compacted 1-D tensor &gt;= batch_size * (2 * topk + 1),
            since 2 * topk + 1: topk x [probability, token sequence (at least one token)] +
            floor probability (rest).
            Content structure:
                [prob_k=0_b=0, tok_0_k=0_b=0, tok_1_k=0_b=0, ..., prob_k=1_b=0, tok_0_k=1_b=0, ..., prob_floor_b=0,
                 prob_k=0_b=1, tok_0_k=0_b=1, tok_1_k=0_b=1, ..., prob_k=1_b=1, tok_0_k=1_b=1, ..., prob_floor_b=1,
                 ...]
</pre> 
</div>
</div>
<a id="a58bc4ccb79ae645c72c7a5e4244b99d3" name="a58bc4ccb79ae645c72c7a5e4244b99d3"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a58bc4ccb79ae645c72c7a5e4244b99d3">&#9670;&#160;</a></span>find_offsets()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> List[List[int]] bittensor.utils.tokenizer_utils.find_offsets </td>
          <td>(</td>
          <td class="paramtype">str&#160;</td>
          <td class="paramname"><em>string</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str&#160;</td>
          <td class="paramname"><em>substring</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Finds all the [start, end] offsets of substring in string.
Assumes there is no overlap of substring, nor recursive overlap.
    Args:
        string (:obj:`str`, `required`):
            Main string to find offsets in.
        substring (:obj:`str`, `required`):
            Substring to search for in string.

    Returns:
        offsets (:obj:`List[List[int]]`, `required`):
            Offsets denoting the [start, end] positions of substring in string.
</pre> 
</div>
</div>
<a id="a4da460aad949bf91a0f414c1a0f30df1" name="a4da460aad949bf91a0f414c1a0f30df1"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4da460aad949bf91a0f414c1a0f30df1">&#9670;&#160;</a></span>get_special_token_pairings()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> Dict[str, str] bittensor.utils.tokenizer_utils.get_special_token_pairings </td>
          <td>(</td>
          <td class="paramtype">PreTrainedTokenizerBase&#160;</td>
          <td class="paramname"><em>from_tokenizer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">PreTrainedTokenizerBase&#160;</td>
          <td class="paramname"><em>to_tokenizer</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Determines a prioritized matching of special token texts between two tokenizers.
Purpose is to produce replacement pairs so special token test is correctly represented for target tokenizer.
    Args:
        from_tokenizer (:obj:`PreTrainedTokenizerBase`, `required`):
            From tokenizer.
        to_tokenizer (:obj:`PreTrainedTokenizerBase`, `required`):
            To tokenizer.

    Returns:
        pairings (:obj:`Dict[str, str]`, `required`):
            Prioritized dictionary of From_special_token_text -&gt; To_special_token_text.
</pre> 
</div>
</div>
<a id="a31406026136d3c9b3ec5469bd1e0c0f7" name="a31406026136d3c9b3ec5469bd1e0c0f7"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a31406026136d3c9b3ec5469bd1e0c0f7">&#9670;&#160;</a></span>get_tokenizer_alignment_splits()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> Dict[int, tuple] bittensor.utils.tokenizer_utils.get_tokenizer_alignment_splits </td>
          <td>(</td>
          <td class="paramtype">List[tuple]&#160;</td>
          <td class="paramname"><em>offset_mapping</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">List[tuple]&#160;</td>
          <td class="paramname"><em>offset_mapping_std</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">    Calculates split depths necessary for tokens to align input offsets to standard offsets.
    Only input offsets may be split, not standard offsets, to create one-to-one, one-to-many, or many-to-one
    token alignments between input-to-standard tokenization.
    Allows for multiple depth splits on a token.
        Args:
            offset_mapping (:obj:`List[tuple]`, `required`):
                Tokenizer offset mappings for a specific sequence [(left_0, right_0), (left_1, right_1), ...].
            offset_mapping_std (:obj:`List[tuple]`, `required`):
                Standard tokenizer offset mappings for a specific sequence [(left_0, right_0), (left_1, right_1), ...]

        Returns:
            splits (:obj:`Dict[int, tuple]`, `required`):
                For tokens that have to be split, {Token index: (split depth 1, split depth 2, ...), ...}.</pre> 
</div>
</div>
<a id="af76d2962aae10ddc4826d5f349fd9051" name="af76d2962aae10ddc4826d5f349fd9051"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af76d2962aae10ddc4826d5f349fd9051">&#9670;&#160;</a></span>get_tokenizer_depth_split_map()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> List[Dict[str, torch.LongTensor]] bittensor.utils.tokenizer_utils.get_tokenizer_depth_split_map </td>
          <td>(</td>
          <td class="paramtype">PreTrainedTokenizerBase&#160;</td>
          <td class="paramname"><em>tokenizer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">tuple&#160;</td>
          <td class="paramname"><em>depths</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Split individual token strings at specified depths, retokenize each resulting segment,
keep only the first token of each segment (if there is one).
Purpose is to provide targets for scattering probabilities when a single distribution requires a depth split.
Args:
tokenizer (:obj:`PreTrainedTokenizerBase`, `required`):
Tokenizer.
depths (:obj:`tuple`, `required`):
Tuple of depths at which tokens strings will be split.

Returns:
split_map (:obj:`List[Dict[str, torch.LongTensor]]`, `required`):
</pre> 
</div>
</div>
<a id="a80673928d5022558ece6eb5cdc9e12be" name="a80673928d5022558ece6eb5cdc9e12be"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a80673928d5022558ece6eb5cdc9e12be">&#9670;&#160;</a></span>get_tokenizer_sequence_mappings()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> List[tuple] bittensor.utils.tokenizer_utils.get_tokenizer_sequence_mappings </td>
          <td>(</td>
          <td class="paramtype">List[tuple]&#160;</td>
          <td class="paramname"><em>offset_mapping</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">List[tuple]&#160;</td>
          <td class="paramname"><em>offset_mapping_std</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Greedily determine the one-to-one, one-to-many, or many-to-one token alignments
between input-to-standard tokenizations.
Disallow many-to-many mappings, but allow for right-aligned overlapping tokens.
Args:
offset_mapping (:obj:`List[tuple]`, `required`):
Tokenizer offset mappings for a specific sequence [(left_0, right_0), (left_1, right_1), ...].
offset_mapping_std (:obj:`List[tuple]`, `required`):
Standard tokenizer offset mappings for a specific sequence [(left_0, right_0), (left_1, right_1), ...]

Returns:
mappings (:obj:`List[tuple]`, `required`):
List of mapping tuples:
[tuple( right_idx, right_idx_std,
        segment_count_base, segment_count_std_base,
        segment_count_overlap, segment_count_std_overlap), ...]
</pre> 
</div>
</div>
<a id="aa9eb42e9db4c2b1517d8ff7105c8ebd2" name="aa9eb42e9db4c2b1517d8ff7105c8ebd2"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa9eb42e9db4c2b1517d8ff7105c8ebd2">&#9670;&#160;</a></span>get_top_probs()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> str bittensor.utils.tokenizer_utils.get_top_probs </td>
          <td>(</td>
          <td class="paramtype">torch.FloatTensor&#160;</td>
          <td class="paramname"><em>probs</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">PreTrainedTokenizerBase&#160;</td>
          <td class="paramname"><em>tokenizer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>amount</em> = <code>10</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Constructs output string with top amount of highest probability token strings.
Used to display the top probabilities.
Args:
    probs (:obj:`torch.FloatTensor`, `required`):
        [vocab_size] Probability distribution over a tokenizer vocabulary.
    tokenizer (:obj:`PreTrainedTokenizerBase`, `required`):
        Tokenizer.
    amount: (:obj:`int`, `optional`):
        Amount of top tokens to return

Returns:
    string (:obj:`str`, `required`):
    Highest probability token strings, prob[token-string] ...
</pre> 
</div>
</div>
<a id="a52d7949730e4e84f9a53fd0bff3e0f22" name="a52d7949730e4e84f9a53fd0bff3e0f22"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a52d7949730e4e84f9a53fd0bff3e0f22">&#9670;&#160;</a></span>get_translation_map()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> Dict[str, Any] bittensor.utils.tokenizer_utils.get_translation_map </td>
          <td>(</td>
          <td class="paramtype">PreTrainedTokenizerBase&#160;</td>
          <td class="paramname"><em>from_tokenizer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">PreTrainedTokenizerBase&#160;</td>
          <td class="paramname"><em>to_tokenizer</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Map individual token phrases from a tokenizer to another tokenizer.
Args:
    from_tokenizer (:obj:`PreTrainedTokenizerBase`, `required`):
        From tokenizer.
    to_tokenizer (:obj:`PreTrainedTokenizerBase`, `required`):
        To tokenizer.

Returns:
    translation_map (:obj:`Dict[str, Any]`, `required`):
        Maps for each observed length, a source token to a token sequence of that length,
        with source index to target indices.
</pre> 
</div>
</div>
<a id="a48b79b3d00a8fa64ade7ce9ba3db7410" name="a48b79b3d00a8fa64ade7ce9ba3db7410"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a48b79b3d00a8fa64ade7ce9ba3db7410">&#9670;&#160;</a></span>pad_offsets()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> List[List[List[Any]]] bittensor.utils.tokenizer_utils.pad_offsets </td>
          <td>(</td>
          <td class="paramtype">List[List[tuple]]&#160;</td>
          <td class="paramname"><em>offsets_batch</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">List[List[List[Any]]]&#160;</td>
          <td class="paramname"><em>source_offsets_batch</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">List[List[List[Any]]]&#160;</td>
          <td class="paramname"><em>pad_offsets_batch</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Pads specific tuples in offsets_batch, selected by source_offsets_batch with
associated paddings in pad_offsets_batch.
Purpose is typically to add padding to align two tokenization offsets at special tokens.
    Args:
        offsets_batch (:obj:`List[List[tuple]]`, `required`):
                Batch of full input tokenizer offset mappings to be used for alteration
                [[(left_0, right_0), (left_1, right_1), ...], ...].
        source_offsets_batch (:obj:`List[List[List[Any]]]`, `required`):
                Batch of tokenizer offset mappings indicating replacement tuples in offsets_batch
                [[(left_0, right_0), (left_1, right_1), ...], ...].
        pad_offsets_batch (:obj:`List[List[List[Any]]]`, `required`):
                Batch of offset paddings associated with each source_offsets_batch replacement tuple
                [[(left_pad_0, right_pad_0), (left_pad_1, right_pad_1), ...], ...].

    Returns:
        new_offsets_batch (:obj:`List[List[List[Any]]]`, `required`):
                Batch of padded full input tokenizer offset mappings
                [[(left_0, right_0), (left_1, right_1), ...], ...].
</pre> 
</div>
</div>
<a id="ab00bac4739db21de5c408746fb2d631e" name="ab00bac4739db21de5c408746fb2d631e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab00bac4739db21de5c408746fb2d631e">&#9670;&#160;</a></span>phrase_cross_entropy()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> Tuple[torch.Tensor, torch.Tensor] bittensor.utils.tokenizer_utils.phrase_cross_entropy </td>
          <td>(</td>
          <td class="paramtype">Union[List[List[int]], torch.Tensor]&#160;</td>
          <td class="paramname"><em>target_phrases</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">torch.Tensor&#160;</td>
          <td class="paramname"><em>topk_tensor</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>ignore_index</em> = <code>-100</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>reduce</em> = <code>True</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>reduction</em> = <code>'mean'</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>vocab_size_min</em> = <code>50257</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Calculates the cross entropy of a phrase prediction against a target phrase, so that this is a multi-token
extension of typical cross entropy calculated for next token prediction.
    Args:
        target_phrases (:obj:`List[List[int]]`, `required`):
            [batch_size, *] Target phrases in standard token sequence list.
        topk_tensor (:obj:`torch.Tensor`, `required`):
            [batch_size, (topk + 1), max_len] tensor includes topk token probabilities (prob_k) + floor_prob
            in first column with gradients attached, with std_tokens in remaining columns with ignore_index padding.
            Content structure:
            [[[prob_k=0_b=0, tok_0_k=0_b=0, tok_1_k=0_b=0, ..., ignore_index?],
              [prob_k=1_b=0, tok_0_k=1_b=0, tok_1_k=1_b=0, ..., ignore_index?],
              [...],
              [prob_floor_b=0, ignore_index, ..., ignore_index]],
             [[prob_k=0_b=1, tok_0_k=0_b=1, tok_1_k=0_b=1, ..., ignore_index?],
              [prob_k=1_b=1, tok_0_k=1_b=1, tok_1_k=1_b=1, ..., ignore_index?],
              [...],
              [prob_floor_b=1, ignore_index, ..., ignore_index]],
             [...]]
        ignore_index (:obj:`int`, `optional`):
            Padding value to use for unfilled token positions in a shorter token phrase.
        reduce (:obj:`bool`, `optional`):
            Whether to reduce the cross entropy over the batch dimension.
        reduction (:obj:`str`, `optional`):
            Reduction function to perform when reduce is True.
        vocab_size_min (:obj:`int`, `optional`):
            Minimum server vocab_size expected, should set to nominal 50257,
            used to prevent the floor_probs from being too large.
    Returns:
        loss_val (:obj:`torch.Tensor`, `required`):
            Validation cross entropy loss, either scalar if reduce or [batch_size].
        loss (:obj:`torch.Tensor`, `required`):
            Phrase cross entropy loss, either scalar if reduce or [batch_size].
</pre> 
</div>
</div>
<a id="abcdd922e5075b5eb136e9f056aa547c1" name="abcdd922e5075b5eb136e9f056aa547c1"></a>
<h2 class="memtitle"><span class="permalink"><a href="#abcdd922e5075b5eb136e9f056aa547c1">&#9670;&#160;</a></span>prune_tokens()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def bittensor.utils.tokenizer_utils.prune_tokens </td>
          <td>(</td>
          <td class="paramtype">torch.FloatTensor&#160;</td>
          <td class="paramname"><em>inputs</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>prune_len</em> = <code>1</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>margin</em> = <code>3</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Prune tokens from a batch of sequences randomly by removing prune_len tokens from each sequence,
leaving the end margin intact.
    Args:
        inputs (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, seq_len)`, `required`):
            Tensor inputs to have tokens pruned.
        prune_len (:obj:`int`, `optional`):
            Number of tokens to prune from each validation input sequence.
        margin (:obj:`int`, `optional`):
            Number of tokens at the end of the sequence to leave unpruned.
    Returns:
        pruned_inputs (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, seq_len - prune_len)`, `required`)
</pre> 
</div>
</div>
<a id="a8e57c9718294c345d7f4ff730970723d" name="a8e57c9718294c345d7f4ff730970723d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a8e57c9718294c345d7f4ff730970723d">&#9670;&#160;</a></span>replace_at_offsets()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> Tuple[str, List[List[int]]] bittensor.utils.tokenizer_utils.replace_at_offsets </td>
          <td>(</td>
          <td class="paramtype">str&#160;</td>
          <td class="paramname"><em>string</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">List[List[Any]]&#160;</td>
          <td class="paramname"><em>offsets</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Replace indicated [left, right] offset positions with a new substring, by
deleting [left, right] content and adding [left, left+len(substring)] substring,
adjusting offsets incrementally.
Assumes an incremental ordered, non-overlapping list of offsets, constructing
the new string incrementally and recording new offsets.
    Args:
        string (:obj:`str`, `required`):
            Main string to perform replacements for.
        offsets (:obj:`List[List[Any]]`, `required`):
            Offsets where replacements are made with replacement substring
            [[left_0, right_0, substring_0], ...]

    Returns:
        new_string (:obj:`str`, `required`):
            New string where replacements were made.
        new_offsets (:obj:`List[List[Any]]`, `required`):
            New offsets where replacements are now located
            [[left_0, right_0], [left_1, right_1], ...]
</pre> 
</div>
</div>
<a id="a066a0913f46f47503972e5df32808063" name="a066a0913f46f47503972e5df32808063"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a066a0913f46f47503972e5df32808063">&#9670;&#160;</a></span>set_std_token_phrases()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def bittensor.utils.tokenizer_utils.set_std_token_phrases </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>tokenizer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>std_tokenizer</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Sets std_token_phrases which are the tokenizer token strings tokenized with std_tokenizer, so
the std_tokenizer equivalent of the tokenizer token strings.
Used for converting model predictions/logits into std_tokenizer representations, for example in TextCausalLMNext.
    Args:
        tokenizer(:obj:`PreTrainedTokenizerBase`, `required`):
            Tokenizer to set std_token_phrases for.
        std_tokenizer(:obj:`PreTrainedTokenizerBase`, `required`):
            Standard bittensor tokenizer to convert to.

    Returns:</pre> 
</div>
</div>
<a id="aef7aceb6bc8b3ad0d08f7fed25c876d8" name="aef7aceb6bc8b3ad0d08f7fed25c876d8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aef7aceb6bc8b3ad0d08f7fed25c876d8">&#9670;&#160;</a></span>set_vocab_len()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def bittensor.utils.tokenizer_utils.set_vocab_len </td>
          <td>(</td>
          <td class="paramtype">PreTrainedTokenizerBase&#160;</td>
          <td class="paramname"><em>tokenizer</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Sets the tokenizer.vocab_len if unset, to store the real vocabulary size according to the vocab or encoder.
    Args:
        tokenizer (:obj:`PreTrainedTokenizerBase`, `required`):
            Tokenizer to set vocab_len for.
    Returns:</pre> 
</div>
</div>
<a id="a558123e572c10af14c002993c9b04f36" name="a558123e572c10af14c002993c9b04f36"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a558123e572c10af14c002993c9b04f36">&#9670;&#160;</a></span>set_whitespace_preserving()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def bittensor.utils.tokenizer_utils.set_whitespace_preserving </td>
          <td>(</td>
          <td class="paramtype">PreTrainedTokenizerBase&#160;</td>
          <td class="paramname"><em>tokenizer</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Sets the tokenizer.whitespace_preserving if unset, indicates if tokenizer preserves whitespace like GPT-style,
or not like BERT-style.
    Args:
        tokenizer (:obj:`PreTrainedTokenizerBase`, `required`):
            Tokenizer to set vocab_len for.
    Returns:</pre> 
</div>
</div>
<a id="af3a7513c8021247073b9f0f68d3e751a" name="af3a7513c8021247073b9f0f68d3e751a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af3a7513c8021247073b9f0f68d3e751a">&#9670;&#160;</a></span>split_probs()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> torch.FloatTensor bittensor.utils.tokenizer_utils.split_probs </td>
          <td>(</td>
          <td class="paramtype">torch.FloatTensor&#160;</td>
          <td class="paramname"><em>probs</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">List[Dict[str, torch.Tensor]]&#160;</td>
          <td class="paramname"><em>split_map</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Split a given probability distribution over a tokenizer vocabulary, given a split_map
of mappings from original tokens to target tokens at each depth of the split.
Args:
probs (:obj:`torch.FloatTensor`, `required`):
[vocab_size] Input probability distribution over a tokenizer vocabulary.
split_map (:obj:`List[Dict[str, torch.Tensor]]`, `required`):
A split_map of mappings from original tokens to target tokens at each depth of the split.

Returns:
new_probs (:obj:`torch.FloatTensor`, `required`):
[splits, vocab_size] A new tensor with resultant probability distribution at each index
of the first dim, representing corresponding split depth.
</pre> 
</div>
</div>
<a id="a619c2a8870fee58a3993722ad5e2d7bb" name="a619c2a8870fee58a3993722ad5e2d7bb"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a619c2a8870fee58a3993722ad5e2d7bb">&#9670;&#160;</a></span>topk_token_phrases()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> torch.Tensor bittensor.utils.tokenizer_utils.topk_token_phrases </td>
          <td>(</td>
          <td class="paramtype">torch.Tensor&#160;</td>
          <td class="paramname"><em>logits</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">PreTrainedTokenizerBase&#160;</td>
          <td class="paramname"><em>tokenizer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>topk</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>ignore_index</em> = <code>-100</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Select topk tokenizer logits/phrases and include std_token_phrases counterparts (std_tokenization of token text)
in topk_tensor output of shape [batch_size, (topk + 1), max_len], where max len of all phrase lists
(with prob in front) is max_{b,k}(len([prob_k, tok_0_k, tok_1_k, ...])).
The output topk_tensor also includes a floor_prob for each batch item. The floor probability is the
mean probability of token phrases not captured in topk, required since the tokenizer vocab_size may
not be known to the receiver.
Requires prep_tokenizer(tokenizer, std_tokenizer) to set_std_token_phrases first, to make
std_token_phrases available here.
    Args:
        logits (:obj:`torch.Tensor`, `required`):
            [batch_size, vocab_size] Input source logits for last token over a source tokenizer vocabulary.
        tokenizer (:obj:`PreTrainedTokenizerBase`, `required`):
            Source tokenizer (usually server tokenizer)
        topk (:obj:`int`, `required`):
            Amount of top phrases to expect (to check for mismatch)
        ignore_index (:obj:`int`, `optional`):
            Padding value to use for unfilled token positions in a shorter token phrase.

    Returns:
        topk_tensor (:obj:`torch.Tensor`, `required`):
            [batch_size, (topk + 1), max_len] tensor includes topk token probabilities (prob_k) + floor_prob
            in first column with gradients attached, with std_tokens in remaining columns with ignore_index padding.
            Content structure:
            [[[prob_k=0_b=0, tok_0_k=0_b=0, tok_1_k=0_b=0, ..., ignore_index?],
              [prob_k=1_b=0, tok_0_k=1_b=0, tok_1_k=1_b=0, ..., ignore_index?],
              [...],
              [prob_floor_b=0, ignore_index, ..., ignore_index]],
             [[prob_k=0_b=1, tok_0_k=0_b=1, tok_1_k=0_b=1, ..., ignore_index?],
              [prob_k=1_b=1, tok_0_k=1_b=1, tok_1_k=1_b=1, ..., ignore_index?],
              [...],
              [prob_floor_b=1, ignore_index, ..., ignore_index]],
             [...]]
</pre> 
</div>
</div>
<a id="a66ebcf1b07d1ee0f0ba07130967a40cc" name="a66ebcf1b07d1ee0f0ba07130967a40cc"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a66ebcf1b07d1ee0f0ba07130967a40cc">&#9670;&#160;</a></span>topk_tokens_to_vocab_size()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> torch.Tensor bittensor.utils.tokenizer_utils.topk_tokens_to_vocab_size </td>
          <td>(</td>
          <td class="paramtype">torch.Tensor&#160;</td>
          <td class="paramname"><em>topk_tensor</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>vocab_size_std</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>vocab_size_min</em> = <code>50257</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Convert topk_tokens first token probabilities into a standard logits tensor shape [batch_size, vocab_size_std].
Args:
topk_tensor (:obj:`torch.Tensor`, `required`):
    [batch_size, (topk + 1), max_len] tensor includes topk token probabilities (prob_k) + floor_prob
    in first column with gradients attached, with std_tokens in remaining columns with ignore_index padding.
    Content structure:
    [[[prob_k=0_b=0, tok_0_k=0_b=0, tok_1_k=0_b=0, ..., ignore_index?],
      [prob_k=1_b=0, tok_0_k=1_b=0, tok_1_k=1_b=0, ..., ignore_index?],
      [...],
      [prob_floor_b=0, ignore_index, ..., ignore_index]],
     [[prob_k=0_b=1, tok_0_k=0_b=1, tok_1_k=0_b=1, ..., ignore_index?],
      [prob_k=1_b=1, tok_0_k=1_b=1, tok_1_k=1_b=1, ..., ignore_index?],
      [...],
      [prob_floor_b=1, ignore_index, ..., ignore_index]],
     [...]]
vocab_size_std (:obj:`int`, `optional`):
    Standard tokenizer vocab_size for forming logits.
vocab_size_min (:obj:`int`, `optional`):
    Minimum server vocab_size expected, should set to nominal 50257,
    used to prevent the floor_probs from being too large.
Returns:
logits (:obj:`torch.Tensor`, `required`):
    [batch_size, vocab_size_std] Standard logits.
</pre> 
</div>
</div>
<a id="a7c296be466fd118c374bf1480d26904b" name="a7c296be466fd118c374bf1480d26904b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a7c296be466fd118c374bf1480d26904b">&#9670;&#160;</a></span>translate_logits_to_probs_std()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> torch.FloatTensor bittensor.utils.tokenizer_utils.translate_logits_to_probs_std </td>
          <td>(</td>
          <td class="paramtype">torch.FloatTensor&#160;</td>
          <td class="paramname"><em>logits</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">List[List[tuple]]&#160;</td>
          <td class="paramname"><em>offset_mapping</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">List[List[tuple]]&#160;</td>
          <td class="paramname"><em>offset_mapping_std</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">PreTrainedTokenizerBase&#160;</td>
          <td class="paramname"><em>tokenizer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">PreTrainedTokenizerBase&#160;</td>
          <td class="paramname"><em>std_tokenizer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Dict[tuple, List[Dict[str, torch.Tensor]]]&#160;</td>
          <td class="paramname"><em>split_map_cache</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Dict[str, Any]&#160;</td>
          <td class="paramname"><em>to_translation_map</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Dict[str, Any]&#160;</td>
          <td class="paramname"><em>from_translation_map</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">torch.LongTensor&#160;</td>
          <td class="paramname"><em>tokens</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">torch.LongTensor&#160;</td>
          <td class="paramname"><em>tokens_std</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool &#160;</td>
          <td class="paramname"><em>skip_equivalent</em> = <code>True</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Translates source token logit scores to probability distributions over the standard tokenizer.
    Args:
        logits (:obj:`torch.FloatTensor`, `required`):
            [batch_size, sequence_len, vocab_size] Input source logits over a source tokenizer vocabulary.
        offset_mapping (:obj:`List[List[tuple]]`, `required`):
            Batch of tokenizer offset mappings
            [[(left_0, right_0), (left_1, right_1), ...], ...].
        offset_mapping_std (:obj:`List[List[tuple]]`, `required`):
            Batch of standard tokenizer offset mappings
            [[(left_0, right_0), (left_1, right_1), ...], ...].
        tokenizer (:obj:`PreTrainedTokenizerBase`, `required`):
            Source tokenizer.
        std_tokenizer (:obj:`PreTrainedTokenizerBase`, `required`):
            Standard/target tokenizer.
        split_map_cache (:obj:`Dict[tuple, List[Dict[str, torch.Tensor]]]`, `required`):
            A dictionary of depths keying split_maps of mappings from original tokens to
            target tokens at each depth of the split. Adds split_maps to cache for faster future recall.
        tokens (:obj:`torch.LongTensor`, `required`):
            [batch_size, sequence_len] A sequence of tokens produced by the source tokenizer.
        tokens_std (:obj:`torch.LongTensor`, `required`):
            [batch_size, std_sequence_len] A sequence of tokens produced by the standard tokenizer.
        to_translation_map (:obj:`Dict[str, Any]`, `required`):
            Maps for each observed length, a source token to a token sequence of that length,
            with source index to target indices.
        from_translation_map (:obj:`Dict[str, Any]`, `required`):
            Maps for each observed length, a source token to a token sequence of that length,
            from target index to source indices.
        skip_equivalent (:obj:`bool`, `optional`):
            Skips translation if tokenizer and std_tokenizer are equivalent.

    Returns:
        probs_std (:obj:`torch.FloatTensor`, `required`):
            [batch_size, std_sequence_len, std_vocab_size] Output probability distribution over the
            standard tokenizer vocabulary.
</pre> 
</div>
</div>
<a id="aacc9d76a7e49b18c20a5b44beafb8306" name="aacc9d76a7e49b18c20a5b44beafb8306"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aacc9d76a7e49b18c20a5b44beafb8306">&#9670;&#160;</a></span>translate_many_to_one()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> None bittensor.utils.tokenizer_utils.translate_many_to_one </td>
          <td>(</td>
          <td class="paramtype">torch.FloatTensor&#160;</td>
          <td class="paramname"><em>probs_from</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">torch.FloatTensor&#160;</td>
          <td class="paramname"><em>probs_to</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Dict[str, Any]&#160;</td>
          <td class="paramname"><em>translation_map</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">    Translate a sequence of token probability distributions from a source tokenization to a
    single token probability distribution over a target tokenization.
        Args:
            probs_from (:obj:`torch.FloatTensor`, `required`):
                [many, vocab_size] Input probability distributions over a from-tokenizer vocabulary.
            probs_to (:obj:`torch.FloatTensor`, `required`):
                [vocab_size] Output probability distribution over a to-tokenizer vocabulary.
            translation_map (:obj:`Dict[str, Any]`, `required`):
                Maps for each observed length, a source token to a token sequence of that length,
                with source index to target indices.

        Returns:</pre> 
</div>
</div>
<a id="a127face1324f46e879f92ee881771d90" name="a127face1324f46e879f92ee881771d90"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a127face1324f46e879f92ee881771d90">&#9670;&#160;</a></span>translate_one_to_many()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> None bittensor.utils.tokenizer_utils.translate_one_to_many </td>
          <td>(</td>
          <td class="paramtype">torch.FloatTensor&#160;</td>
          <td class="paramname"><em>probs_from</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">torch.FloatTensor&#160;</td>
          <td class="paramname"><em>probs_to</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Dict[str, Any]&#160;</td>
          <td class="paramname"><em>translation_map</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Translate a single token probability distribution from a source tokenization to a
sequence of probability distributions over a target tokenization.
Args:
    probs_from (:obj:`torch.FloatTensor`, `required`):
        [vocab_size] Input probability distribution over a from-tokenizer vocabulary.
    probs_to (:obj:`torch.FloatTensor`, `required`):
        [many, vocab_size] Output probability distributions over a to-tokenizer vocabulary.
    translation_map (:obj:`Dict[str, Any]`, `required`):
        Maps for each observed length, a source token to a token sequence of that length,
        with source index to target indices.

Returns:</pre> 
</div>
</div>
<a id="a6717d83c925189f86590b5ab3d88df17" name="a6717d83c925189f86590b5ab3d88df17"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a6717d83c925189f86590b5ab3d88df17">&#9670;&#160;</a></span>translate_special_token_text()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> Tuple[List[str],
                                                                                 List[List[List[int]]],
                                                                                 List[List[List[int]]],
                                                                                 List[List[List[Any]]]] bittensor.utils.tokenizer_utils.translate_special_token_text </td>
          <td>(</td>
          <td class="paramtype">List[str]&#160;</td>
          <td class="paramname"><em>text_batch</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">PreTrainedTokenizerBase&#160;</td>
          <td class="paramname"><em>from_tokenizer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">PreTrainedTokenizerBase&#160;</td>
          <td class="paramname"><em>to_tokenizer</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Translates special_token signifier text in from_tokenizer to to_tokenizer special_token text, for
a given text_batch. Resulting to_text_batch can then be to_tokenized where special_tokens should
map to its single corresponding token, despite signifier text difference compared to from_tokenizer.
    Args:
        text_batch (:obj:`List[str]`, `required`):
            List of strings to translate special tokens for.
        from_tokenizer (:obj:`PreTrainedTokenizerBase`, `required`):
            From tokenizer.
        to_tokenizer (:obj:`PreTrainedTokenizerBase`, `required`):
            To tokenizer.

    Returns:
        to_text_batch (:obj:`List[str]`, `required`):
            List of strings where special text has been replaced.
        from_offsets_batch (:obj:`List[List[List[int]]]`, `required`):
            Batch of tokenizer offset mappings selecting replacement tuples in from_tokenizer text
                [[(left_0, right_0), (left_1, right_1), ...], ...].
        to_offsets_batch (:obj:`List[List[List[int]]]`, `required`):
            Batch of tokenizer offset mappings selecting replacement tuples in to_tokenizer text
                [[(left_0, right_0), (left_1, right_1), ...], ...].
        pad_offsets_batch (:obj:`List[List[List[Any]]]`, `required`):
            Batch of offset paddings associated with each replacement tuple
                [[(left_pad_0, right_pad_0), (left_pad_1, right_pad_1), ...], ...].
</pre> 
</div>
</div>
<a id="ac2ac0a649cfc204c56ad13913288a4f4" name="ac2ac0a649cfc204c56ad13913288a4f4"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac2ac0a649cfc204c56ad13913288a4f4">&#9670;&#160;</a></span>translate_tokenizer_probs()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> None bittensor.utils.tokenizer_utils.translate_tokenizer_probs </td>
          <td>(</td>
          <td class="paramtype">torch.FloatTensor&#160;</td>
          <td class="paramname"><em>probs</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">torch.FloatTensor&#160;</td>
          <td class="paramname"><em>probs_std</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">List[tuple]&#160;</td>
          <td class="paramname"><em>offset_mapping</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">List[tuple]&#160;</td>
          <td class="paramname"><em>offset_mapping_std</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">PreTrainedTokenizerBase&#160;</td>
          <td class="paramname"><em>tokenizer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">PreTrainedTokenizerBase&#160;</td>
          <td class="paramname"><em>std_tokenizer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Dict[tuple, List[Dict[str, torch.Tensor]]]&#160;</td>
          <td class="paramname"><em>split_map_cache</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Dict[str, Any]&#160;</td>
          <td class="paramname"><em>to_translation_map</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Dict[str, Any]&#160;</td>
          <td class="paramname"><em>from_translation_map</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">torch.LongTensor&#160;</td>
          <td class="paramname"><em>tokens</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">torch.LongTensor&#160;</td>
          <td class="paramname"><em>tokens_std</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Translates source token probability distributions to target probability distributions, by
aligning segments through source token splits, then greedily performing one-to-one,
one-to-many, many-to-one distribution mappings.
    Args:
        probs (:obj:`torch.FloatTensor`, `required`):
            [sequence_len, vocab_size] Input probability distribution over a source tokenizer vocabulary.
        probs_std (:obj:`torch.FloatTensor`, `required`):
            [std_sequence_len, std_vocab_size] Output probability distribution over a target tokenizer vocabulary.
            Reference that will be written in-place.
        offset_mapping (:obj:`List[tuple]`, `required`):
            Tokenizer offset mappings for a specific sequence [(left_0, right_0), (left_1, right_1), ...].
        offset_mapping_std (:obj:`List[tuple]`, `required`):
            Standard tokenizer offset mappings for a specific sequence [(left_0, right_0), (left_1, right_1), ...]
        tokenizer (:obj:`PreTrainedTokenizerBase`, `required`):
            Source tokenizer.
        std_tokenizer (:obj:`PreTrainedTokenizerBase`, `required`):
            Standard/target tokenizer.
        split_map_cache (:obj:`Dict[tuple, List[Dict[str, torch.Tensor]]]`, `required`):
            A dictionary of depths keying split_maps of mappings from original tokens to
            target tokens at each depth of the split. Adds split_maps to cache for faster future recall.
        tokens (:obj:`torch.LongTensor`, `required`):
            [sequence_len] A sequence of tokens produced by the source tokenizer.
        tokens_std (:obj:`torch.LongTensor`, `required`):
            [std_sequence_len] A sequence of tokens produced by the standard tokenizer.
        to_translation_map (:obj:`Dict[str, Any]`, `required`):
            Maps for each observed length, a source token to a token sequence of that length,
            with source index to target indices.
        from_translation_map (:obj:`Dict[str, Any]`, `required`):
            Maps for each observed length, a source token to a token sequence of that length,
            from target index to source indices.

    Returns:</pre> 
</div>
</div>
<a id="a952b31d97e6588cd4166ee2ef319a1f0" name="a952b31d97e6588cd4166ee2ef319a1f0"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a952b31d97e6588cd4166ee2ef319a1f0">&#9670;&#160;</a></span>unravel_topk_token_phrases()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> torch.Tensor bittensor.utils.tokenizer_utils.unravel_topk_token_phrases </td>
          <td>(</td>
          <td class="paramtype">torch.Tensor&#160;</td>
          <td class="paramname"><em>compact_topk</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>topk</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>ignore_index</em> = <code>-100</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Unravel topk token phrases input_tensor from 1-D to [batch_size, (topk + 1), max_len] topk_tensor, which
includes topk token probabilities (prob_k) + floor_prob in first column with gradients attached, with
std_tokens in remaining columns with ignore_index padding.
    Args:
        compact_topk (:obj:`torch.Tensor`, `required`):
            [sum_b(sum_k(len(phrase_k) + 1)_b)] Compacted 1-D tensor &gt;= batch_size * (2 * topk + 1),
            since 2 * topk + 1: topk x [probability, token sequence (at least one token)] +
            floor probability (rest).
            Content structure:
                [prob_k=0_b=0, tok_0_k=0_b=0, tok_1_k=0_b=0, ..., prob_k=1_b=0, tok_0_k=1_b=0, ..., prob_floor_b=0,
                 prob_k=0_b=1, tok_0_k=0_b=1, tok_1_k=0_b=1, ..., prob_k=1_b=1, tok_0_k=1_b=1, ..., prob_floor_b=1,
                 ...]
        topk (:obj:`int`, `required`):
            Amount of top phrases to expect (to check for mismatch)
        ignore_index (:obj:`int`, `optional`):
            Padding value to use for unfilled token positions in a shorter token phrase.
    Returns:
        topk_tensor (:obj:`torch.Tensor`, `required`):
            [batch_size, (topk + 1), max_len] tensor includes topk token probabilities (prob_k) + floor_prob
            in first column with gradients attached, with std_tokens in remaining columns with ignore_index padding.
            Content structure:
            [[[prob_k=0_b=0, tok_0_k=0_b=0, tok_1_k=0_b=0, ..., ignore_index?],
              [prob_k=1_b=0, tok_0_k=1_b=0, tok_1_k=1_b=0, ..., ignore_index?],
              [...],
              [prob_floor_b=0, ignore_index, ..., ignore_index]],
             [[prob_k=0_b=1, tok_0_k=0_b=1, tok_1_k=0_b=1, ..., ignore_index?],
              [prob_k=1_b=1, tok_0_k=1_b=1, tok_1_k=1_b=1, ..., ignore_index?],
              [...],
              [prob_floor_b=1, ignore_index, ..., ignore_index]],
             [...]]
</pre> 
</div>
</div>
</div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><b>bittensor</b></li><li class="navelem"><b>utils</b></li><li class="navelem"><a class="el" href="namespacebittensor_1_1utils_1_1tokenizer__utils.html">tokenizer_utils</a></li>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.6 </li>
  </ul>
</div>
</body>
</html>
