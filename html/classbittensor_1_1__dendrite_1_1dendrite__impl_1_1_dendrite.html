<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.6"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Bittensor: bittensor._dendrite.dendrite_impl.Dendrite Class Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript">var page_layout=1;</script>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
  $(document).ready(function() { init_search(); });
/* @license-end */
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="side-nav" class="ui-resizable side-nav-resizable"><!-- do not remove this div, it is closed by doxygen! -->
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">Bittensor
   </div>
   <div id="projectbrief">Python API</div>
  </td>
 </tr>
   <tr><td colspan="2">        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <span id="MSearchSelect"                onmouseover="return searchBox.OnSearchSelectShow()"                onmouseout="return searchBox.OnSearchSelectHide()">&#160;</span>
          <input type="text" id="MSearchField" value="" placeholder="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.svg" alt=""/></a>
          </span>
        </div>
</td></tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.6 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
</div><!-- top -->
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(document).ready(function(){initNavTree('classbittensor_1_1__dendrite_1_1dendrite__impl_1_1_dendrite.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="#pub-static-methods">Static Public Member Functions</a> &#124;
<a href="#pub-attribs">Public Attributes</a> &#124;
<a href="#pro-methods">Protected Member Functions</a> &#124;
<a href="#pro-attribs">Protected Attributes</a> &#124;
<a href="classbittensor_1_1__dendrite_1_1dendrite__impl_1_1_dendrite-members.html">List of all members</a>  </div>
  <div class="headertitle"><div class="title">bittensor._dendrite.dendrite_impl.Dendrite Class Reference</div></div>
</div><!--header-->
<div class="contents">
<div class="dynheader">
Inheritance diagram for bittensor._dendrite.dendrite_impl.Dendrite:</div>
<div class="dyncontent">
 <div class="center">
  <img src="classbittensor_1_1__dendrite_1_1dendrite__impl_1_1_dendrite.png" alt=""/>
 </div></div>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pub-methods" name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:a80e3be1a7ae6a4090f8689bd550949c5"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classbittensor_1_1__dendrite_1_1dendrite__impl_1_1_dendrite.html#a80e3be1a7ae6a4090f8689bd550949c5">__init__</a> (self, '<a class="el" href="classbittensor_1_1__config_1_1config__impl_1_1_config.html">bittensor.Config</a>' <a class="el" href="classbittensor_1_1__config_1_1config.html">config</a>, '<a class="el" href="classbittensor_1_1__wallet_1_1wallet__impl_1_1_wallet.html">bittensor.Wallet</a>' <a class="el" href="classbittensor_1_1__wallet_1_1wallet.html">wallet</a>, '<a class="el" href="classbittensor_1_1__receptor_1_1receptor__pool__impl_1_1_receptor_pool.html">bittensor.ReceptorPool</a>' <a class="el" href="classbittensor_1_1__receptor_1_1receptor__pool.html">receptor_pool</a>, 'BaseManager' manager=None)</td></tr>
<tr class="separator:a80e3be1a7ae6a4090f8689bd550949c5"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6ad20828b96711b3b05cb5448da88066"><td class="memItemLeft" align="right" valign="top"><a id="a6ad20828b96711b3b05cb5448da88066" name="a6ad20828b96711b3b05cb5448da88066"></a>
def&#160;</td><td class="memItemRight" valign="bottom"><b>__str__</b> (self)</td></tr>
<tr class="separator:a6ad20828b96711b3b05cb5448da88066"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a140022be021c06a4d51922be0473a5ec"><td class="memItemLeft" align="right" valign="top"><a id="a140022be021c06a4d51922be0473a5ec" name="a140022be021c06a4d51922be0473a5ec"></a>
def&#160;</td><td class="memItemRight" valign="bottom"><b>__repr__</b> (self)</td></tr>
<tr class="separator:a140022be021c06a4d51922be0473a5ec"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:adc463ae82af32a95dd0c0b538556ef88"><td class="memItemLeft" align="right" valign="top"><a id="adc463ae82af32a95dd0c0b538556ef88" name="adc463ae82af32a95dd0c0b538556ef88"></a>
def&#160;</td><td class="memItemRight" valign="bottom"><b>__del__</b> (self)</td></tr>
<tr class="separator:adc463ae82af32a95dd0c0b538556ef88"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a13fd8cc44e4289f9662236107c6577b1"><td class="memItemLeft" align="right" valign="top">Tuple[List[str], List[float], List[str]]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classbittensor_1_1__dendrite_1_1dendrite__impl_1_1_dendrite.html#a13fd8cc44e4289f9662236107c6577b1">generate</a> (self, Union[torch.LongTensor, List[torch.LongTensor], List['<a class="el" href="classbittensor_1_1__endpoint_1_1endpoint__impl_1_1_endpoint.html">bittensor.Endpoint</a>'], '<a class="el" href="classbittensor_1_1__endpoint_1_1endpoint__impl_1_1_endpoint.html">bittensor.Endpoint</a>'] endpoints, Union[str, List[str], List[torch.LongTensor], torch.LongTensor] prompt, int timeout=None, int topk=50, int num_to_generate=256, int num_beams=5, int no_repeat_ngram_size=2, bool early_stopping=False, int num_return_sequences=1, bool do_sample=False, float top_p=0.95, float temperature=1.0, float repetition_penalty=1.0, float length_penalty=1.0, float max_time=150, int num_beam_groups=1)</td></tr>
<tr class="separator:a13fd8cc44e4289f9662236107c6577b1"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af64a1b06ea6d77b00c6b191f2b95daad"><td class="memItemLeft" align="right" valign="top">Tuple[Union[List[torch.FloatTensor], torch.FloatTensor], torch.LongTensor, torch.FloatTensor]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classbittensor_1_1__dendrite_1_1dendrite__impl_1_1_dendrite.html#af64a1b06ea6d77b00c6b191f2b95daad">text</a> (self, Union[torch.LongTensor, List[torch.LongTensor], List['<a class="el" href="classbittensor_1_1__endpoint_1_1endpoint__impl_1_1_endpoint.html">bittensor.Endpoint</a>'], '<a class="el" href="classbittensor_1_1__endpoint_1_1endpoint__impl_1_1_endpoint.html">bittensor.Endpoint</a>'] endpoints, List[ '<a class="el" href="classbittensor_1_1__synapse_1_1synapse__impl_1_1_synapse.html">bittensor.Synapse</a>'] synapses, Union[str, List[str], List[torch.LongTensor], torch.LongTensor] inputs, int timeout=None, bool requires_grad=None)</td></tr>
<tr class="separator:af64a1b06ea6d77b00c6b191f2b95daad"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad12fdba0d1db1b1e0eb4cd5a8e209d5b"><td class="memItemLeft" align="right" valign="top">Tuple[Union[List[torch.FloatTensor], torch.FloatTensor], torch.LongTensor, torch.FloatTensor]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classbittensor_1_1__dendrite_1_1dendrite__impl_1_1_dendrite.html#ad12fdba0d1db1b1e0eb4cd5a8e209d5b">text_causal_lm</a> (self, Union[torch.LongTensor, List[torch.LongTensor], List[ '<a class="el" href="classbittensor_1_1__endpoint_1_1endpoint__impl_1_1_endpoint.html">bittensor.Endpoint</a>'], '<a class="el" href="classbittensor_1_1__endpoint_1_1endpoint__impl_1_1_endpoint.html">bittensor.Endpoint</a>'] endpoints, Union[str, List[str], List[torch.LongTensor], torch.LongTensor] inputs, Optional[ '<a class="el" href="classbittensor_1_1__synapse_1_1synapse.html#a238e9d58958a570ca7407f3d9aa3d35f">bittensor.synapse.TextCausalLM</a>'] <a class="el" href="classbittensor_1_1__synapse_1_1synapse.html">synapse</a>=<a class="el" href="classbittensor_1_1__synapse_1_1synapse.html#a238e9d58958a570ca7407f3d9aa3d35f">synapse.TextCausalLM</a>(), Optional[int] timeout=None, Optional[bool] requires_grad=None)</td></tr>
<tr class="separator:ad12fdba0d1db1b1e0eb4cd5a8e209d5b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0cf9d7590cb94638a20627417b057c78"><td class="memItemLeft" align="right" valign="top">Tuple[Union[List[torch.FloatTensor], torch.FloatTensor], torch.LongTensor, torch.FloatTensor]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classbittensor_1_1__dendrite_1_1dendrite__impl_1_1_dendrite.html#a0cf9d7590cb94638a20627417b057c78">text_causal_lm_next</a> (self, Union[torch.LongTensor, List[torch.LongTensor], List['<a class="el" href="classbittensor_1_1__endpoint_1_1endpoint__impl_1_1_endpoint.html">bittensor.Endpoint</a>'], '<a class="el" href="classbittensor_1_1__endpoint_1_1endpoint__impl_1_1_endpoint.html">bittensor.Endpoint</a>'] endpoints, Union[str, List[str], List[torch.LongTensor], torch.LongTensor] inputs, Optional['<a class="el" href="classbittensor_1_1__synapse_1_1synapse.html#a53e1f1a97ac117a0618ee982aae7d38b">bittensor.synapse.TextCausalLMNext</a>'] <a class="el" href="classbittensor_1_1__synapse_1_1synapse.html">synapse</a>=<a class="el" href="classbittensor_1_1__synapse_1_1synapse.html#a53e1f1a97ac117a0618ee982aae7d38b">synapse.TextCausalLMNext</a>(), Optional[int] timeout=None, Optional[bool] requires_grad=None)</td></tr>
<tr class="separator:a0cf9d7590cb94638a20627417b057c78"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a847c81a7312898dcca79439dab3e1420"><td class="memItemLeft" align="right" valign="top">Tuple[Union[List[torch.FloatTensor], torch.FloatTensor], torch.LongTensor, torch.FloatTensor]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classbittensor_1_1__dendrite_1_1dendrite__impl_1_1_dendrite.html#a847c81a7312898dcca79439dab3e1420">text_last_hidden_state</a> (self, Union[torch.LongTensor, List[torch.LongTensor], List['<a class="el" href="classbittensor_1_1__endpoint_1_1endpoint__impl_1_1_endpoint.html">bittensor.Endpoint</a>'], '<a class="el" href="classbittensor_1_1__endpoint_1_1endpoint__impl_1_1_endpoint.html">bittensor.Endpoint</a>'] endpoints, Union[str, List[str], List[torch.LongTensor], torch.LongTensor] inputs, Optional[ '<a class="el" href="classbittensor_1_1__synapse_1_1synapse.html#aa477e1b335c842bbc9ce3b90d05e7dd1">bittensor.synapse.TextLastHiddenState</a>'] <a class="el" href="classbittensor_1_1__synapse_1_1synapse.html">synapse</a>=<a class="el" href="classbittensor_1_1__synapse_1_1synapse.html#aa477e1b335c842bbc9ce3b90d05e7dd1">synapse.TextLastHiddenState</a>(), int timeout=None, bool requires_grad=None)</td></tr>
<tr class="separator:a847c81a7312898dcca79439dab3e1420"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:abab26bb7988e58f64548aef9489d8830"><td class="memItemLeft" align="right" valign="top">Tuple['<a class="el" href="classbittensor_1_1__endpoint_1_1endpoint__impl_1_1_endpoint.html">bittensor.Endpoint</a>', List[torch.LongTensor]]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classbittensor_1_1__dendrite_1_1dendrite__impl_1_1_dendrite.html#abab26bb7988e58f64548aef9489d8830">format_text_inputs</a> (self, Union[torch.LongTensor, List[torch.LongTensor], List['<a class="el" href="classbittensor_1_1__endpoint_1_1endpoint__impl_1_1_endpoint.html">bittensor.Endpoint</a>'], '<a class="el" href="classbittensor_1_1__endpoint_1_1endpoint__impl_1_1_endpoint.html">bittensor.Endpoint</a>'] endpoints, Union[str, List[str], List[torch.LongTensor], torch.LongTensor] inputs)</td></tr>
<tr class="separator:abab26bb7988e58f64548aef9489d8830"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a52f56fdee119d8db1bc4528266623519"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classbittensor_1_1__dendrite_1_1dendrite__impl_1_1_dendrite.html#a52f56fdee119d8db1bc4528266623519">update_stats</a> (self, List[ '<a class="el" href="classbittensor_1_1__endpoint_1_1endpoint__impl_1_1_endpoint.html">bittensor.Endpoint</a>'] endpoints, List[ 'bittensor.proto.Synapse'] synapses, List[torch.Tensor] inputs, List[List[torch.Tensor]] outputs, List[List[torch.LongTensor]] codes, List[List[torch.FloatTensor]] times)</td></tr>
<tr class="separator:a52f56fdee119d8db1bc4528266623519"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a162b4285d8ae80681dc18c33379aa38d"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classbittensor_1_1__dendrite_1_1dendrite__impl_1_1_dendrite.html#a162b4285d8ae80681dc18c33379aa38d">to_dataframe</a> (self, <a class="el" href="classbittensor_1_1__metagraph_1_1metagraph.html">metagraph</a>)</td></tr>
<tr class="separator:a162b4285d8ae80681dc18c33379aa38d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9489a72a71cd70e2b6bf11aa4a892d5b"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classbittensor_1_1__dendrite_1_1dendrite__impl_1_1_dendrite.html#a9489a72a71cd70e2b6bf11aa4a892d5b">to_wandb</a> (self)</td></tr>
<tr class="separator:a9489a72a71cd70e2b6bf11aa4a892d5b"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pub-static-methods" name="pub-static-methods"></a>
Static Public Member Functions</h2></td></tr>
<tr class="memitem:af5a6b8827b65e4e7991aecc1872de492"><td class="memItemLeft" align="right" valign="top">Tuple[torch.Tensor,...]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classbittensor_1_1__dendrite_1_1dendrite__impl_1_1_dendrite.html#af5a6b8827b65e4e7991aecc1872de492">forward</a> (ctx, '<a class="el" href="classbittensor_1_1__dendrite_1_1dendrite__impl_1_1_dendrite.html">bittensor.Dendrite</a>' <a class="el" href="classbittensor_1_1__dendrite_1_1dendrite.html">dendrite</a>, torch.Tensor dummy, List['<a class="el" href="classbittensor_1_1__endpoint_1_1endpoint__impl_1_1_endpoint.html">bittensor.Endpoint</a>'] endpoints, List[ '<a class="el" href="classbittensor_1_1__synapse_1_1synapse__impl_1_1_synapse.html">bittensor.Synapse</a>'] synapses, int timeout, bool requires_grad, *torch.Tensor inputs)</td></tr>
<tr class="separator:af5a6b8827b65e4e7991aecc1872de492"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2bff366b9b2753b63a570c84ceec955e"><td class="memItemLeft" align="right" valign="top">Tuple[Optional[torch.Tensor],...]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classbittensor_1_1__dendrite_1_1dendrite__impl_1_1_dendrite.html#a2bff366b9b2753b63a570c84ceec955e">backward</a> (ctx, torch.FloatTensor unused_code_grads, torch.FloatTensor unused_time_grads, *torch.FloatTensor output_grads)</td></tr>
<tr class="separator:a2bff366b9b2753b63a570c84ceec955e"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pub-attribs" name="pub-attribs"></a>
Public Attributes</h2></td></tr>
<tr class="memitem:a004e88ced27124365900597f2ec74fdb"><td class="memItemLeft" align="right" valign="top"><a id="a004e88ced27124365900597f2ec74fdb" name="a004e88ced27124365900597f2ec74fdb"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>config</b></td></tr>
<tr class="separator:a004e88ced27124365900597f2ec74fdb"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac4939061f29081ffca15953a91d3e842"><td class="memItemLeft" align="right" valign="top"><a id="ac4939061f29081ffca15953a91d3e842" name="ac4939061f29081ffca15953a91d3e842"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>wallet</b></td></tr>
<tr class="separator:ac4939061f29081ffca15953a91d3e842"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aba6742399ca4fb120ffc9437d82e0751"><td class="memItemLeft" align="right" valign="top"><a id="aba6742399ca4fb120ffc9437d82e0751" name="aba6742399ca4fb120ffc9437d82e0751"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>receptor_pool</b></td></tr>
<tr class="separator:aba6742399ca4fb120ffc9437d82e0751"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8e0f5727d0f7186963334e477ee0d243"><td class="memItemLeft" align="right" valign="top"><a id="a8e0f5727d0f7186963334e477ee0d243" name="a8e0f5727d0f7186963334e477ee0d243"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>manager</b></td></tr>
<tr class="separator:a8e0f5727d0f7186963334e477ee0d243"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:affd0640a693e4281a8a0927cd4058859"><td class="memItemLeft" align="right" valign="top"><a id="affd0640a693e4281a8a0927cd4058859" name="affd0640a693e4281a8a0927cd4058859"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>stats</b></td></tr>
<tr class="separator:affd0640a693e4281a8a0927cd4058859"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pro-methods" name="pro-methods"></a>
Protected Member Functions</h2></td></tr>
<tr class="memitem:ab0bedbfadc59c5895ad47f48f05b7b69"><td class="memItemLeft" align="right" valign="top">Tuple[List[torch.Tensor], List[torch.LongTensor], List[torch.FloatTensor]]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classbittensor_1_1__dendrite_1_1dendrite__impl_1_1_dendrite.html#ab0bedbfadc59c5895ad47f48f05b7b69">_forward</a> (self, List[ '<a class="el" href="classbittensor_1_1__endpoint_1_1endpoint__impl_1_1_endpoint.html">bittensor.Endpoint</a>'] endpoints, List[ '<a class="el" href="classbittensor_1_1__synapse_1_1synapse__impl_1_1_synapse.html">bittensor.Synapse</a>'] synapses, List[torch.Tensor] inputs, Optional[int] timeout=None, Optional[bool] requires_grad=None)</td></tr>
<tr class="separator:ab0bedbfadc59c5895ad47f48f05b7b69"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a021071b2859f5bc990a73803ac3f90f7"><td class="memItemLeft" align="right" valign="top"><a id="a021071b2859f5bc990a73803ac3f90f7" name="a021071b2859f5bc990a73803ac3f90f7"></a>
def&#160;</td><td class="memItemRight" valign="bottom"><b>_init_stats</b> (self)</td></tr>
<tr class="separator:a021071b2859f5bc990a73803ac3f90f7"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pro-attribs" name="pro-attribs"></a>
Protected Attributes</h2></td></tr>
<tr class="memitem:aa4f6aa19bbf9f43e52b04d2c9f86b7fe"><td class="memItemLeft" align="right" valign="top"><a id="aa4f6aa19bbf9f43e52b04d2c9f86b7fe" name="aa4f6aa19bbf9f43e52b04d2c9f86b7fe"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>_prometheus_uuid</b></td></tr>
<tr class="separator:aa4f6aa19bbf9f43e52b04d2c9f86b7fe"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><pre class="fragment"> This is the implementation class for a bittensor.dendrite(). The dendrite class operates as a normal torch autograd friendly operation
which accepts a list of bittensor.endpoints and a list of torch tensors. The passed endpoints are queried with the passed inputs and either return
results or zeros. The operation is fully differentiable with a torch computation graph such that calls to loss.backward() produce Backward calls on
the passed endpoints.

Args:
    config (:obj:`bittensor.Config`, `optional`, defaults to bittensor.dendrite.config()):
        config namespace object created by calling bittensor.dendrite.config()
    wallet (:obj:`bittensor.Wallet`, `optional`, defaults to bittensor.wallet( name = 'default', wallet ='default')):
        A bittensor wallet object containing a pair of cryptographic keys, the hot and coldkey, used for signing messages
        on the wire.
    receptor_pool (:obj:`bittensor.ReceptorPool`, `optional`, defaults to bittensor.receptor_pool()):
        A bittensor receptor pool object which maintains a set of connections to other peers in the network and operates as
        a normal torch.nn.Module. By default this object is created with the dendrite config.
</pre> </div><h2 class="groupheader">Constructor &amp; Destructor Documentation</h2>
<a id="a80e3be1a7ae6a4090f8689bd550949c5" name="a80e3be1a7ae6a4090f8689bd550949c5"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a80e3be1a7ae6a4090f8689bd550949c5">&#9670;&#160;</a></span>__init__()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def bittensor._dendrite.dendrite_impl.Dendrite.__init__ </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">'<a class="el" href="classbittensor_1_1__config_1_1config__impl_1_1_config.html">bittensor.Config</a>'&#160;</td>
          <td class="paramname"><em>config</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">'<a class="el" href="classbittensor_1_1__wallet_1_1wallet__impl_1_1_wallet.html">bittensor.Wallet</a>'&#160;</td>
          <td class="paramname"><em>wallet</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">'<a class="el" href="classbittensor_1_1__receptor_1_1receptor__pool__impl_1_1_receptor_pool.html">bittensor.ReceptorPool</a>'&#160;</td>
          <td class="paramname"><em>receptor_pool</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">'BaseManager' &#160;</td>
          <td class="paramname"><em>manager</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment"> Initializes a new Dendrite entry point.
        Args:
            receptor_pool (:obj:`bittensor.ReceptorPool`, `required`):
                bittensor receptor pool</pre> 
</div>
</div>
<h2 class="groupheader">Member Function Documentation</h2>
<a id="ab0bedbfadc59c5895ad47f48f05b7b69" name="ab0bedbfadc59c5895ad47f48f05b7b69"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab0bedbfadc59c5895ad47f48f05b7b69">&#9670;&#160;</a></span>_forward()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> Tuple [ List[ torch.Tensor ], List[ torch.LongTensor ], List [ torch.FloatTensor ]] bittensor._dendrite.dendrite_impl.Dendrite._forward </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">List [ '<a class="el" href="classbittensor_1_1__endpoint_1_1endpoint__impl_1_1_endpoint.html">bittensor.Endpoint</a>' ]&#160;</td>
          <td class="paramname"><em>endpoints</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">List[ '<a class="el" href="classbittensor_1_1__synapse_1_1synapse__impl_1_1_synapse.html">bittensor.Synapse</a>' ]&#160;</td>
          <td class="paramname"><em>synapses</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">List [ torch.Tensor ]&#160;</td>
          <td class="paramname"><em>inputs</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional [ int ]  &#160;</td>
          <td class="paramname"><em>timeout</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional [ bool ] &#160;</td>
          <td class="paramname"><em>requires_grad</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<pre class="fragment"> Internal Forward tensor inputs to a list of neuron endpoints.

Args:
    endpoints (:obj:`List[bittensor.Endpoint]` of shape :obj:`(num_endpoints)`, `required`):
        List of remote endpoints which match length of inputs. Tensors from inputs are sent forward to these endpoints.

    synapses (:obj:`List[ 'bittensor.Synapse' ]` of shape :obj:`(num_synapses)`, `required`):
        Bittensor synapse objects with arguments. Each corresponds to a synapse function on the axon.
        Responses are packed in this ordering. 

    inputs (:obj:`List[torch.Tensor]` of shape :obj:`(num_endpoints * [shape])`, `required`):
        List of tensors to send to corresponding endpoints. Tensors are of arbitrary type and shape depending on the
        synapse.

    timeout (int, default = dendrite.timeout, `optional`):
        request timeout.

    requires_grad (int, default = dendrite.requires_grad, `optional`):
        If true, the backward pass triggers passing gradients on the wire.

Returns:
    outputs (:obj:`List[torch.FloatTensor]` of shape :obj:`(batch_size, sequence_len, bittensor.__network_dim__)`, `required`):
        Output encodings of inputs produced by the remote endpoints. Non-responses are zeroes of common shape.

    codes (:obj:`List[torch.LongTensor]` of shape :obj:`[num_endpoints]`, `required`):
        Return codes per endpoint per synapse.

    times (:obj:`torch.FloatTensor` of shape :obj:`[ num_endpoints ]`, `required`):
        Call times per endpoint per synapse.</pre> 
</div>
</div>
<a id="a2bff366b9b2753b63a570c84ceec955e" name="a2bff366b9b2753b63a570c84ceec955e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2bff366b9b2753b63a570c84ceec955e">&#9670;&#160;</a></span>backward()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> Tuple[Optional[torch.Tensor], ...] bittensor._dendrite.dendrite_impl.Dendrite.backward </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>ctx</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">torch.FloatTensor&#160;</td>
          <td class="paramname"><em>unused_code_grads</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">torch.FloatTensor&#160;</td>
          <td class="paramname"><em>unused_time_grads</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">*torch.FloatTensor
    &#160;</td>
          <td class="paramname"><em>output_grads</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">static</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<pre class="fragment"> Internal autograd-friendly Backward RPC call to a list of neuron endpoints.

    Args:
        ctx: (:obj:`torch.autograd.ctx`, `required`):
            Autograd context, saves state information between forward and backward calls. i.e. inputs for gradient computation.

        unused_code_grads: (:obj:`List[torch.Tensor]` of shape :obj:`(shape)`, `required`):
            Gradients of this function's codes. (Unused)

        unused_time_grads: (:obj:`List[torch.Tensor]` of shape :obj:`(shape)`, `required`):
            Gradients of this function's query times. (Unused)

        grads (:obj:`List[torch.Tensor]` of shape :obj:`(shape)`, `required`):
            Gradients of this function's outputs computed during the loss.backward() call.
            This is a list item of size num_endpoints * num_synapses.
    
    Returns:
        DUMMY, None, None, None,
        outputs (:obj:`List[torch.FloatTensor], `optional`):
            Gradient results for each input.</pre> 
</div>
</div>
<a id="abab26bb7988e58f64548aef9489d8830" name="abab26bb7988e58f64548aef9489d8830"></a>
<h2 class="memtitle"><span class="permalink"><a href="#abab26bb7988e58f64548aef9489d8830">&#9670;&#160;</a></span>format_text_inputs()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> Tuple[ '<a class="el" href="classbittensor_1_1__endpoint_1_1endpoint__impl_1_1_endpoint.html">bittensor.Endpoint</a>', List[torch.LongTensor] ] bittensor._dendrite.dendrite_impl.Dendrite.format_text_inputs </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[ torch.LongTensor, List[torch.LongTensor], List['<a class="el" href="classbittensor_1_1__endpoint_1_1endpoint__impl_1_1_endpoint.html">bittensor.Endpoint</a>'], '<a class="el" href="classbittensor_1_1__endpoint_1_1endpoint__impl_1_1_endpoint.html">bittensor.Endpoint</a>' ]&#160;</td>
          <td class="paramname"><em>endpoints</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[str, List[str], List[torch.LongTensor], torch.LongTensor]&#160;</td>
          <td class="paramname"><em>inputs</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment"> Formats endpoint and inputs args to a common format.
    Args:
        endpoints (:obj:`Union[torch.LongTensor, List[torch.LongTensor], List[bittensor.Endpoint], bittensor.Endpoint]` of shape :obj:`(num_endpoints)`, `required`):
            Endpoints to send inputs to. Endpoint can be one of the following types:
                - a single endpoint tensor shape [250]
                - a set of endpoint tensors shape [n, 250]
                - a list of endpoints tensors each of shape [250]
                - a single endpoint object. Inputs will be sent to this endpoint alone.
                - a list of endpoint objects. All inputs will be sent to these endpoints.

        inputs (:obj:`Union[str,  List[str], List[torch.LongTensor], torch.LongTensor]` of shape :obj:`(num_endpoints * [batch_size, sequence_len])`, `required`):
            Tokenized sentences to send on the wire. Inputs can be one of the following types:
                - a single string: the string will be tokenized using the bittensor tokenizer.
                - a list of strings: the strings will be tokenized using the bittensor tokenizer.
                - a tensor with shape [batch_size, sequence_len], assumed to be the output of bittensor tokenizer.
                - a tensor with shape [n, batch_size, sequence_len], the operation will unbind the tensor and pass inputs to endpoints.
            If inputs are tensors they will be cast to int64 format before sending on the wire.

    Returns:
        formatted_endpoints (:obj:`Union[torch.LongTensor, List[torch.LongTensor], List[bittensor.Endpoint], bittensor.Endpoint]` of shape :obj:`(num_endpoints)`, `required`):
            A list of endpoint objects. All inputs will be sent to these endpoints.

        formatted_inputs (:obj:`Union[str,  List[str], List[torch.LongTensor], torch.LongTensor]` of shape :obj:`(num_endpoints * [batch_size, sequence_len])`, `required`):
            A list of tensor of type long each representing a tokenized sentence to be sent to each endpoint.
</pre> 
</div>
</div>
<a id="af5a6b8827b65e4e7991aecc1872de492" name="af5a6b8827b65e4e7991aecc1872de492"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af5a6b8827b65e4e7991aecc1872de492">&#9670;&#160;</a></span>forward()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"> Tuple[torch.Tensor, ...] bittensor._dendrite.dendrite_impl.Dendrite.forward </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>ctx</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">'<a class="el" href="classbittensor_1_1__dendrite_1_1dendrite__impl_1_1_dendrite.html">bittensor.Dendrite</a>'&#160;</td>
          <td class="paramname"><em>dendrite</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">torch.Tensor&#160;</td>
          <td class="paramname"><em>dummy</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">List['<a class="el" href="classbittensor_1_1__endpoint_1_1endpoint__impl_1_1_endpoint.html">bittensor.Endpoint</a>']&#160;</td>
          <td class="paramname"><em>endpoints</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">List[ '<a class="el" href="classbittensor_1_1__synapse_1_1synapse__impl_1_1_synapse.html">bittensor.Synapse</a>' ]&#160;</td>
          <td class="paramname"><em>synapses</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>timeout</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>requires_grad</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">*torch.Tensor
    &#160;</td>
          <td class="paramname"><em>inputs</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">static</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<pre class="fragment"> Internal autograd-friendly Forward RPC call to a list of neuron endpoints.

    Args:
        ctx: (:obj:`torch.autograd.ctx`, `required`):
            Autograd context, saves state information between forward and backward calls. i.e. inputs for gradient computation.

        dendrite: (:obj:`bittensor.Dendrite`, `required`):
            Pointer to a bittensor dendrite object on which we are creating the forward requests.

        dummy: (:obj:`torch.Tensor`, `required`):
            Dummy torch tensor used to ensure that torch.backward computation is called on this function 
            regardless of the input types.

        endpoints (:obj:`List[bittensor.Endpoint']` of shape :obj:`(n_endpoints)`, `required`):
            List of endpoints which match length of inputs. Inputs are sent forward to these endpoints.

        synapses (:obj:`List[ 'bittensor.Synapse' ]` of shape :obj:`(num_synapses)`, `required`):
            Bittensor synapse objects with arguments. Each corresponds to a synapse function on the axon.
            Responses are packed in this ordering. 

        timeout (int):
            request timeout.

        requires_grad (int, default = dendrite.requires_grad, `optional`):
            If true, the backward pass triggers passing gradients on the wire.

        inputs (:obj:`List[torch.Tensor]` of shape :obj:`(n_endpoints)`, `required`):
            List of torch tensors to be sent to the associated endpoints.

    Returns:
        codes (:obj:`torch.LongTensor` of shape :obj:`(n_endpoints)` `required`):
            Return code associated with forward call.

        times (:obj:`torch.FloatTensor` of shape :obj:`[ num_endpoints ]`, `required`):
            times per call.
        
        outputs (:obj:`List[torch.FloatTensor` of shape :obj:`num_synapses * n_endpoints * (-1, -1, -1) `, `required`):
            List of outputs from each synapses and each endpoint unfolded into a single list. Non-responses are zeroes of expected shape.
</pre> 
</div>
</div>
<a id="a13fd8cc44e4289f9662236107c6577b1" name="a13fd8cc44e4289f9662236107c6577b1"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a13fd8cc44e4289f9662236107c6577b1">&#9670;&#160;</a></span>generate()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> Tuple[ List[str], List[float], List[str] ] bittensor._dendrite.dendrite_impl.Dendrite.generate </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[ torch.LongTensor, List[torch.LongTensor], List['<a class="el" href="classbittensor_1_1__endpoint_1_1endpoint__impl_1_1_endpoint.html">bittensor.Endpoint</a>'], '<a class="el" href="classbittensor_1_1__endpoint_1_1endpoint__impl_1_1_endpoint.html">bittensor.Endpoint</a>' ]&#160;</td>
          <td class="paramname"><em>endpoints</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[str, List[str], List[torch.LongTensor], torch.LongTensor]&#160;</td>
          <td class="paramname"><em>prompt</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>timeout</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>topk</em> = <code>50</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>num_to_generate</em> = <code>256</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>num_beams</em> = <code>5</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>no_repeat_ngram_size</em> = <code>2</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool &#160;</td>
          <td class="paramname"><em>early_stopping</em> = <code>False</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>num_return_sequences</em> = <code>1</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool &#160;</td>
          <td class="paramname"><em>do_sample</em> = <code>False</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float &#160;</td>
          <td class="paramname"><em>top_p</em> = <code>0.95</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float &#160;</td>
          <td class="paramname"><em>temperature</em> = <code>1.0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float &#160;</td>
          <td class="paramname"><em>repetition_penalty</em> = <code>1.0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float &#160;</td>
          <td class="paramname"><em>length_penalty</em> = <code>1.0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float &#160;</td>
          <td class="paramname"><em>max_time</em> = <code>150</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>num_beam_groups</em> = <code>1</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Returns a tuple containing the prompt generations produced by endpoints with corresponding parsed codes and query times.

Args:
    endpoints (:obj:`Union[torch.LongTensor, List[torch.LongTensor], List[bittensor.Endpoint], bittensor.Endpoint]` of shape :obj:`(num_endpoints)`, `required`):
                Endpoints to send inputs to. Endpoint can be one of the following types:
                    - a single endpoint tensor shape [250]
                    - a set of endpoint tensors shape [n, 250]
                    - a list of endpoints tensors each of shape [250]
                    - a single endpoint object. Inputs will be sent to this endpoint alone.
                    - a list of endpoint objects. All inputs will be sent to these endpoints.

    prompts (:obj:`Union[str,  List[str], List[torch.LongTensor], torch.LongTensor]` of shape :obj:`(num_endpoints * [batch_size, sequence_len])`, `required`):
                Tokenized sentences to send on the wire. Inputs can be one of the following types:
                    - a single string: the string will be tokenized using the bittensor tokenizer.
                    - a list of strings: the strings will be tokenized using the bittensor tokenizer.
                    - a tensor with shape [batch_size, sequence_len], assumed to be the output of bittensor tokenizer.
                    - a tensor with shape [n, batch_size, sequence_len], the operation will unbind the tensor and pass inputs to endpoints.
                    - a list of tensors of type long each representing a tokenized sentence to be sent to each endpoint.
                If inputs are tensors they will be cast to int64 format before sending on the wire.

    timeout (:type:`int`, default = dendrite.timeout `optional`):
        Request timeout. Queries that do not respond will be replaced by zeros.

    Topk (:obj:int, :default: 50):
        The number of highest probability vocabulary tokens to keep for top-k-filtering. 
    num_to_generate (:obj: int, :default: 256):
        The number of tokens to generate using the language model
    num_beams (:obj: int, :default: 5):
        The number of beams to keep during beam search
    no_repeat_ngram_size (:obj: int, :default: 2):
        The number of repeat n gram allowed
    early_stopping: (:obj: bool, :default: True):
        If the model should early stop if the probabilty drops a certain threshold
    num_return_sequences: (:obj: int, :default: 1):
        How many sequences should the model return
    do_sample (:obj: bool, :default: False):
        If the model should do sample its probablity during generation
    top_p (:obj: float, :default: 0.95): 
        probability cutoff for top p sampling
    temperature: (:obj: float, :default: 1.0):
        The value used to module the next token probabilities for the softmax calculation
    repetition_penalty (:obj: float, :default: 1.0):
        The parameter for repetition penalty. 1.0 means no penalty.
    length_penalty (:obj: float, :default: 1.0): 
        The parameter for length penalty. 0.0 means no penalty, &lt;0 to encourage longer sequences.
    max_time (:obj: float, :default: 150): 
        The maximum time that a server can use to generate
    num_beam_groups (:obj: int, :default: 1):
        Number of groups to divide num_beams into in order to ensure diversity among different groups of beams. 
Returns:
    codes (:obj:`List[str]`, `required`):
        Parsed codes from each endpoint from query.

    times (:obj:`List[float]`, `required`):
        Query times for each call from each endpoint.

    generations (:obj:`List[str]`, `required`):
        Generations from each endpoint.
</pre> 
</div>
</div>
<a id="af64a1b06ea6d77b00c6b191f2b95daad" name="af64a1b06ea6d77b00c6b191f2b95daad"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af64a1b06ea6d77b00c6b191f2b95daad">&#9670;&#160;</a></span>text()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> Tuple[ Union[List[torch.FloatTensor], torch.FloatTensor], torch.LongTensor, torch.FloatTensor] bittensor._dendrite.dendrite_impl.Dendrite.text </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[ torch.LongTensor, List[torch.LongTensor], List['<a class="el" href="classbittensor_1_1__endpoint_1_1endpoint__impl_1_1_endpoint.html">bittensor.Endpoint</a>'], '<a class="el" href="classbittensor_1_1__endpoint_1_1endpoint__impl_1_1_endpoint.html">bittensor.Endpoint</a>' ]&#160;</td>
          <td class="paramname"><em>endpoints</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">List[ '<a class="el" href="classbittensor_1_1__synapse_1_1synapse__impl_1_1_synapse.html">bittensor.Synapse</a>' ]&#160;</td>
          <td class="paramname"><em>synapses</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[str, List[str], List[torch.LongTensor], torch.LongTensor]&#160;</td>
          <td class="paramname"><em>inputs</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>timeout</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool &#160;</td>
          <td class="paramname"><em>requires_grad</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment"> Forward text inputs to a list of neuron endpoints and returns logit encodings or timeout.

        Args:
            endpoints (:obj:`Union[torch.LongTensor, List[torch.LongTensor], List[bittensor.Endpoint], bittensor.Endpoint]` of shape :obj:`(num_endpoints)`, `required`):
                Endpoints to send inputs to. Endpoint can be one of the following types:
                    - a single endpoint tensor shape [250]
                    - a set of endpoint tensors shape [n, 250]
                    - a list of endpoints tensors each of shape [250]
                    - a single endpoint object. Inputs will be sent to this endpoint alone.
                    - a list of endpoint objects. All inputs will be sent to these endpoints.

            synapses (:obj:`List[ 'bittensor.Synapse' ]` of shape :obj:`(num_synapses)`, `required`):
                Bittensor synapse objects with arguments. Each corresponds to a synapse function on the axon.
                Responses are packed in this ordering. 

            inputs (:obj:`Union[str,  List[str], List[torch.LongTensor], torch.LongTensor]` of shape :obj:`(num_endpoints * [batch_size, sequence_len])`, `required`):
                Tokenized sentences to send on the wire. Inputs can be one of the following types:
                    - a single string: the string will be tokenized using the bittensor tokenizer.
                    - a list of strings: the strings will be tokenized using the bittensor tokenizer.
                    - a tensor with shape [batch_size, sequence_len], assumed to be the output of bittensor tokenizer.
                    - a tensor with shape [n, batch_size, sequence_len], the operation will unbind the tensor and pass inputs to endpoints.
                    - a list of tensors of type long each representing a tokenized sentence to be sent to each endpoint.
                If inputs are tensors they will be cast to int64 format before sending on the wire.

            timeout (:type:`int`, default = dendrite.timeout `optional`):
                Request timeout. Queries that do not respond will be replaced by zeros.

            requires_grad (:type:`int`, default = dendrite.requires_grad, `optional`):
                If true, the backward pass triggers passing gradients on the wire.

        Returns:
            outputs (:obj:`List[ List[ torch.FloatTensor ] ]` of shape :obj:`num_synapses * ( num_endpoints * ( -1, -1, -1 ) )`, `required`):
                List of outputs from synapses, each a list of size num_endpoints of tensors with relevant size. Non-responses are zeroes of relevant 
                synapse shape.

            codes (:obj:`List [ torch.LongTensor ]` of shape :obj:`[ num_endpoints ]`, `required`):
                Return code per call per synapse.

            times (:obj:`List [ torch.FloatTensor ]` of shape :obj:`[ num_endpoints ]`, `required`):
                Times per call per synapse.</pre> 
</div>
</div>
<a id="ad12fdba0d1db1b1e0eb4cd5a8e209d5b" name="ad12fdba0d1db1b1e0eb4cd5a8e209d5b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad12fdba0d1db1b1e0eb4cd5a8e209d5b">&#9670;&#160;</a></span>text_causal_lm()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> Tuple[Union[List[torch.FloatTensor], torch.FloatTensor], torch.LongTensor, torch.FloatTensor] bittensor._dendrite.dendrite_impl.Dendrite.text_causal_lm </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union [ torch.LongTensor, List [ torch.LongTensor ], List[ '<a class="el" href="classbittensor_1_1__endpoint_1_1endpoint__impl_1_1_endpoint.html">bittensor.Endpoint</a>' ], '<a class="el" href="classbittensor_1_1__endpoint_1_1endpoint__impl_1_1_endpoint.html">bittensor.Endpoint</a>' ]&#160;</td>
          <td class="paramname"><em>endpoints</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union [ str, List[ str ], List [ torch.LongTensor ], torch.LongTensor]&#160;</td>
          <td class="paramname"><em>inputs</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[ '<a class="el" href="classbittensor_1_1__synapse_1_1synapse.html#a238e9d58958a570ca7407f3d9aa3d35f">bittensor.synapse.TextCausalLM</a>' ] &#160;</td>
          <td class="paramname"><em>synapse</em> = <code><a class="el" href="classbittensor_1_1__synapse_1_1synapse.html#a238e9d58958a570ca7407f3d9aa3d35f">synapse.TextCausalLM</a>()</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional [ int ] &#160;</td>
          <td class="paramname"><em>timeout</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional [ bool ] &#160;</td>
          <td class="paramname"><em>requires_grad</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment"> Forward text inputs to a list of neuron endpoints and returns logit encodings or timeout.

        Args:
            endpoints (:obj:`Union[torch.LongTensor, List[torch.LongTensor], List[bittensor.Endpoint], bittensor.Endpoint]` of shape :obj:`(num_endpoints)`, `required`):
                Endpoints to send inputs to. Endpoint can be one of the following types:
                    - a single endpoint tensor shape [250]
                    - a set of endpoint tensors shape [n, 250]
                    - a list of endpoints tensors each of shape [250]
                    - a single endpoint object. Inputs will be sent to this endpoint alone.
                    - a list of endpoint objects. All inputs will be sent to these endpoints.


            inputs (:obj:`Union[str,  List[str], List[torch.LongTensor], torch.LongTensor]` of shape :obj:`(num_endpoints * [batch_size, sequence_len])`, `required`):
                Tokenized sentences to send on the wire. Inputs can be one of the following types:
                    - a single string: the string will be tokenized using the bittensor tokenizer.
                    - a list of strings: the strings will be tokenized using the bittensor tokenizer.
                    - a tensor with shape [batch_size, sequence_len], assumed to be the output of bittensor tokenizer.
                    - a tensor with shape [n, batch_size, sequence_len], the operation will unbind the tensor and pass inputs to endpoints.
                    - a list of tensors of type long each representing a tokenized sentence to be sent to each endpoint.
                If inputs are tensors they will be cast to int64 format before sending on the wire.

            synapse (:type:`'bittensor.synapse.TextCausalLM'`, default = bittensor.synapse.TextCausalLM(), `optional`):
                Synapse axon function call which defaults to bittensor.synapse.TextCausalLM().
            
            timeout (:type:`int`, default = dendrite.timeout `optional`):
                Request timeout. Queries that do not respond will be replaced by zeros.

            requires_grad (:type:`int`, default = dendrite.requires_grad, `optional`):
                If true, the backward pass triggers passing gradients on the wire.

        Returns:
            outputs (:obj:`List[ torch.FloatTensor ]` of shape :obj:`num_endpoints * (batch_size, sequence_len, bittensor.__vocab_size__ )`, `required`):
                List of output logit encodings of inputs produced by each remote endpoints. Non-responses are zeroes of input shape plus output dimension.
                The first dimension will match the number of endpoints queried.

            codes (:obj:`torch.LongTensor` of shape :obj:`[ num_endpoints ]`, `required`):
                dendrite call return ops.

            times (:obj:`torch.FloatTensor` of shape :obj:`[ num_endpoints ]`, `required`):
                times per call.
</pre> 
</div>
</div>
<a id="a0cf9d7590cb94638a20627417b057c78" name="a0cf9d7590cb94638a20627417b057c78"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a0cf9d7590cb94638a20627417b057c78">&#9670;&#160;</a></span>text_causal_lm_next()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> Tuple[Union[List[torch.FloatTensor], torch.FloatTensor], torch.LongTensor, torch.FloatTensor] bittensor._dendrite.dendrite_impl.Dendrite.text_causal_lm_next </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[torch.LongTensor, List[torch.LongTensor], List['<a class="el" href="classbittensor_1_1__endpoint_1_1endpoint__impl_1_1_endpoint.html">bittensor.Endpoint</a>'], '<a class="el" href="classbittensor_1_1__endpoint_1_1endpoint__impl_1_1_endpoint.html">bittensor.Endpoint</a>']&#160;</td>
          <td class="paramname"><em>endpoints</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[str, List[str], List[torch.LongTensor], torch.LongTensor]&#160;</td>
          <td class="paramname"><em>inputs</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional['<a class="el" href="classbittensor_1_1__synapse_1_1synapse.html#a53e1f1a97ac117a0618ee982aae7d38b">bittensor.synapse.TextCausalLMNext</a>'] &#160;</td>
          <td class="paramname"><em>synapse</em> = <code><a class="el" href="classbittensor_1_1__synapse_1_1synapse.html#a53e1f1a97ac117a0618ee982aae7d38b">synapse.TextCausalLMNext</a>()</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[int] &#160;</td>
          <td class="paramname"><em>timeout</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[bool] &#160;</td>
          <td class="paramname"><em>requires_grad</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment"> Forward text inputs to a list of neuron endpoints and returns logit encodings or timeout.

        Args:
            endpoints (:obj:`Union[torch.LongTensor, List[torch.LongTensor], List[bittensor.Endpoint], bittensor.Endpoint]` of shape :obj:`(num_endpoints)`, `required`):
                Endpoints to send inputs to. Endpoint can be one of the following types:
                    - a single endpoint tensor shape [250]
                    - a set of endpoint tensors shape [n, 250]
                    - a list of endpoints tensors each of shape [250]
                    - a single endpoint object. Inputs will be sent to this endpoint alone.
                    - a list of endpoint objects. All inputs will be sent to these endpoints.


            inputs (:obj:`Union[str,  List[str], List[torch.LongTensor], torch.LongTensor]` of shape :obj:`(num_endpoints * [batch_size, sequence_len])`, `required`):
                Tokenized sentences to send on the wire. Inputs can be one of the following types:
                    - a single string: the string will be tokenized using the bittensor tokenizer.
                    - a list of strings: the strings will be tokenized using the bittensor tokenizer.
                    - a tensor with shape [batch_size, sequence_len], assumed to be the output of bittensor tokenizer.
                    - a tensor with shape [n, batch_size, sequence_len], the operation will unbind the tensor and pass inputs to endpoints.
                    - a list of tensors of type long each representing a tokenized sentence to be sent to each endpoint.
                If inputs are tensors they will be cast to int64 format before sending on the wire.

            synapse (:type:`'bittensor.synapse.TextCausalLMNext'`, default = bittensor.synapse.TextCausalLMNext(), `optional`):
                Synapse axon function call which defaults to bittensor.synapse.TextCausalLMNext().

            timeout (:type:`int`, default = dendrite.timeout `optional`):
                Request timeout. Queries that do not respond will be replaced by zeros.

            requires_grad (:type:`int`, default = dendrite.requires_grad, `optional`):
                If true, the backward pass triggers passing gradients on the wire.

        Returns:
            outputs (:obj:`List[ torch.FloatTensor ]` of shape :obj:`num_endpoints * ( &gt;= batch_size * (2 * topk + 1) )`, `required`):
                List of output topk phrases encodings of inputs produced by each remote endpoints.
                Non-responses are zeroes of input shape plus output dimension.
                The first dimension will match the number of endpoints queried.

            codes (:obj:`torch.LongTensor` of shape :obj:`[ num_endpoints ]`, `required`):
                dendrite call return ops.

            times (:obj:`torch.FloatTensor` of shape :obj:`[ num_endpoints ]`, `required`):
                times per call.
</pre> 
</div>
</div>
<a id="a847c81a7312898dcca79439dab3e1420" name="a847c81a7312898dcca79439dab3e1420"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a847c81a7312898dcca79439dab3e1420">&#9670;&#160;</a></span>text_last_hidden_state()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> Tuple[Union[List[torch.FloatTensor], torch.FloatTensor], torch.LongTensor, torch.FloatTensor] bittensor._dendrite.dendrite_impl.Dendrite.text_last_hidden_state </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[ torch.LongTensor, List[torch.LongTensor], List['<a class="el" href="classbittensor_1_1__endpoint_1_1endpoint__impl_1_1_endpoint.html">bittensor.Endpoint</a>'], '<a class="el" href="classbittensor_1_1__endpoint_1_1endpoint__impl_1_1_endpoint.html">bittensor.Endpoint</a>' ]&#160;</td>
          <td class="paramname"><em>endpoints</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[str, List[str], List[torch.LongTensor], torch.LongTensor]&#160;</td>
          <td class="paramname"><em>inputs</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[ '<a class="el" href="classbittensor_1_1__synapse_1_1synapse.html#aa477e1b335c842bbc9ce3b90d05e7dd1">bittensor.synapse.TextLastHiddenState</a>' ] &#160;</td>
          <td class="paramname"><em>synapse</em> = <code><a class="el" href="classbittensor_1_1__synapse_1_1synapse.html#aa477e1b335c842bbc9ce3b90d05e7dd1">synapse.TextLastHiddenState</a>()</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>timeout</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool &#160;</td>
          <td class="paramname"><em>requires_grad</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment"> Forward text inputs to a list of neuron endpoints and block until last hidden state responses or timeout.

        Args:
            endpoints (:obj:`Union[torch.LongTensor, List[torch.LongTensor], List[bittensor.Endpoint], bittensor.Endpoint]` of shape :obj:`(num_endpoints)`, `required`):
                Endpoints to send inputs to. Endpoint can be one of the following types:
                    - a single endpoint tensor shape [250]
                    - a set of endpoint tensors shape [n, 250]
                    - a list of endpoints tensors each of shape [250]
                    - a single endpoint object. Inputs will be sent to this endpoint alone.
                    - a list of endpoint objects. All inputs will be sent to these endpoints.

            inputs (:obj:`Union[str,  List[str], List[torch.LongTensor], torch.LongTensor]` of shape :obj:`(num_endpoints * [batch_size, sequence_len])`, `required`):
                Tokenized sentences to send on the wire. Inputs can be one of the following types:
                    - a single string: the string will be tokenized using the bittensor tokenizer.
                    - a list of strings: the strings will be tokenized using the bittensor tokenizer.
                    - a tensor with shape [batch_size, sequence_len], assumed to be the output of bittensor tokenizer.
                    - a tensor with shape [n, batch_size, sequence_len], the operation will unbind the tensor and pass inputs to endpoints.
                If inputs are tensors they will be cast to int64 format before sending on the wire.

            synapse (:type:`'bittensor.synapse.TextLastHiddenState'`, default = bittensor.synapse.TextLastHiddenState(), `optional`):
                Synapse axon function call which defaults to bittensor.synapse.TextLastHiddenState().

            timeout (:type:`int`, default = dendrite.timeout `optional`):
                Request timeout. Queries that do not respond will be replaced by zeros.

            requires_grad (:type:`int`, default = dendrite.requires_grad, `optional`):
                If true, the backward pass triggers passing gradients on the wire.

        Returns:
            outputs (:obj:`List [ torch.FloatTensor ]` of shape :obj:` num_endpoints * ( -1, sequence_len, bittensor.__network_dim__ )`, `required`):
                List of output last hidden state encodings of inputs produced by remote endpoints. Non-responses are zeroes of input shape plus output dimension.
                The first dimension will match the number of endpoints queried.

            codes (:obj:`torch.LongTensor` of shape :obj:`[ num_endpoints ]`, `required`):
                dendrite call return ops.

            times (:obj:`torch.FloatTensor` of shape :obj:`[ num_endpoints ]`, `required`):
                times per call.
</pre> 
</div>
</div>
<a id="a162b4285d8ae80681dc18c33379aa38d" name="a162b4285d8ae80681dc18c33379aa38d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a162b4285d8ae80681dc18c33379aa38d">&#9670;&#160;</a></span>to_dataframe()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def bittensor._dendrite.dendrite_impl.Dendrite.to_dataframe </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>metagraph</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment"> Return a stats info as a pandas dataframe indexed by the metagraph or pubkey if not existend.
Args:
metagraph: (bittensor.Metagraph):
    Indexes the stats data using metagraph hotkeys.
Return:
dataframe (:obj:`pandas.Dataframe`)
</pre> 
</div>
</div>
<a id="a9489a72a71cd70e2b6bf11aa4a892d5b" name="a9489a72a71cd70e2b6bf11aa4a892d5b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a9489a72a71cd70e2b6bf11aa4a892d5b">&#9670;&#160;</a></span>to_wandb()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def bittensor._dendrite.dendrite_impl.Dendrite.to_wandb </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment"> Return a dictionary of dendrite stats as wandb logging info.
Args:
metagraph: (bittensor.Metagraph):
If not None, indexes the wandb data using int uids rather than string pubkeys.
Return:
wandb_info (:obj:`Dict`)
</pre> 
</div>
</div>
<a id="a52f56fdee119d8db1bc4528266623519" name="a52f56fdee119d8db1bc4528266623519"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a52f56fdee119d8db1bc4528266623519">&#9670;&#160;</a></span>update_stats()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def bittensor._dendrite.dendrite_impl.Dendrite.update_stats </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">List[ '<a class="el" href="classbittensor_1_1__endpoint_1_1endpoint__impl_1_1_endpoint.html">bittensor.Endpoint</a>']&#160;</td>
          <td class="paramname"><em>endpoints</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">List[ 'bittensor.proto.Synapse' ]&#160;</td>
          <td class="paramname"><em>synapses</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">List[torch.Tensor]&#160;</td>
          <td class="paramname"><em>inputs</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">List[ List[ torch.Tensor ] ]&#160;</td>
          <td class="paramname"><em>outputs</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">List [ List[ torch.LongTensor ] ]&#160;</td>
          <td class="paramname"><em>codes</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">List [ List[ torch.FloatTensor ] ]
        &#160;</td>
          <td class="paramname"><em>times</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment"> Update dendrite stat according to the response we get from peers. Updates were saved to self.stats.
    Args:
        endpoints (:obj:`List[bittensor.Endpoint]` of shape :obj:`(num_endpoints)`, `required`):
            The set of endpoints that dendrite sent request to.

        synapses (:obj:`List[ 'bittensor.Synapse' ]` of shape :obj:`(num_synapses)`, `required`):
            Bittensor synapse objects with arguments. Each corresponds to a synapse function on the axon.
            Responses are packed in this ordering. 

        inputs (:obj:`List[torch.Tensor]` of shape :obj:`(n_endpoints)`, `required`):
            List of torch tensors to be sent to the associated endpoints.

        outputs (:obj:`List[ List[ torch.FloatTensor ] ]` of shape :obj:`num_synapses * ( num_endpoints * ( -1, -1, -1 ) )`, `required`):
            List of outputs from synapses, each a list of size num_endpoints of tensors with relevant size. Non-responses are zeroes of relevant 
            synapse shape.

        codes (:obj:`List [ torch.LongTensor ]` of shape :obj:`[ num_endpoints ]`, `required`):
            Return code per call per synapse.

        times (:obj:`List [ torch.FloatTensor ]` of shape :obj:`[ num_endpoints ]`, `required`):
            Times per call per synapse.
</pre> 
</div>
</div>
<hr/>The documentation for this class was generated from the following file:<ul>
<li>/Users/macthrasher/bittensor/bittensor/_dendrite/dendrite_impl.py</li>
</ul>
</div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><b>bittensor</b></li><li class="navelem"><a class="el" href="namespacebittensor_1_1__dendrite.html">_dendrite</a></li><li class="navelem"><a class="el" href="namespacebittensor_1_1__dendrite_1_1dendrite__impl.html">dendrite_impl</a></li><li class="navelem"><a class="el" href="classbittensor_1_1__dendrite_1_1dendrite__impl_1_1_dendrite.html">Dendrite</a></li>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.6 </li>
  </ul>
</div>
</body>
</html>
