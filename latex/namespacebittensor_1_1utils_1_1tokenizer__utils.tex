\hypertarget{namespacebittensor_1_1utils_1_1tokenizer__utils}{}\doxysection{bittensor.\+utils.\+tokenizer\+\_\+utils Namespace Reference}
\label{namespacebittensor_1_1utils_1_1tokenizer__utils}\index{bittensor.utils.tokenizer\_utils@{bittensor.utils.tokenizer\_utils}}
\doxysubsection*{Functions}
\begin{DoxyCompactItemize}
\item 
Dict\mbox{[}int, tuple\mbox{]} \mbox{\hyperlink{namespacebittensor_1_1utils_1_1tokenizer__utils_a31406026136d3c9b3ec5469bd1e0c0f7}{get\+\_\+tokenizer\+\_\+alignment\+\_\+splits}} (List\mbox{[}tuple\mbox{]} offset\+\_\+mapping, List\mbox{[}tuple\mbox{]} offset\+\_\+mapping\+\_\+std)
\item 
List\mbox{[}tuple\mbox{]} \mbox{\hyperlink{namespacebittensor_1_1utils_1_1tokenizer__utils_a80673928d5022558ece6eb5cdc9e12be}{get\+\_\+tokenizer\+\_\+sequence\+\_\+mappings}} (List\mbox{[}tuple\mbox{]} offset\+\_\+mapping, List\mbox{[}tuple\mbox{]} offset\+\_\+mapping\+\_\+std)
\item 
List\mbox{[}Dict\mbox{[}str, torch.\+Long\+Tensor\mbox{]}\mbox{]} \mbox{\hyperlink{namespacebittensor_1_1utils_1_1tokenizer__utils_af76d2962aae10ddc4826d5f349fd9051}{get\+\_\+tokenizer\+\_\+depth\+\_\+split\+\_\+map}} (Pre\+Trained\+Tokenizer\+Base \mbox{\hyperlink{classbittensor_1_1__tokenizer_1_1tokenizer}{tokenizer}}, tuple depths)
\item 
torch.\+Float\+Tensor \mbox{\hyperlink{namespacebittensor_1_1utils_1_1tokenizer__utils_af3a7513c8021247073b9f0f68d3e751a}{split\+\_\+probs}} (torch.\+Float\+Tensor probs, List\mbox{[}Dict\mbox{[}str, torch.\+Tensor\mbox{]}\mbox{]} split\+\_\+map)
\item 
Tuple\mbox{[}torch.\+Float\+Tensor, List\mbox{[}tuple\mbox{]}, torch.\+Long\+Tensor\mbox{]} \mbox{\hyperlink{namespacebittensor_1_1utils_1_1tokenizer__utils_a8a5ed01b4d5f6883475c9db9f896edbe}{align\+\_\+tokenizer\+\_\+sequences}} (torch.\+Float\+Tensor probs, List\mbox{[}tuple\mbox{]} offset\+\_\+mapping, List\mbox{[}tuple\mbox{]} offset\+\_\+mapping\+\_\+std, Pre\+Trained\+Tokenizer\+Base \mbox{\hyperlink{classbittensor_1_1__tokenizer_1_1tokenizer}{tokenizer}}, Dict\mbox{[}tuple, List\mbox{[}Dict\mbox{[}str, torch.\+Tensor\mbox{]}\mbox{]}\mbox{]} split\+\_\+map\+\_\+cache, torch.\+Long\+Tensor tokens, torch.\+Long\+Tensor tokens\+\_\+std)
\item 
Dict\mbox{[}str, Any\mbox{]} \mbox{\hyperlink{namespacebittensor_1_1utils_1_1tokenizer__utils_a52d7949730e4e84f9a53fd0bff3e0f22}{get\+\_\+translation\+\_\+map}} (Pre\+Trained\+Tokenizer\+Base from\+\_\+tokenizer, Pre\+Trained\+Tokenizer\+Base to\+\_\+tokenizer)
\item 
None \mbox{\hyperlink{namespacebittensor_1_1utils_1_1tokenizer__utils_a127face1324f46e879f92ee881771d90}{translate\+\_\+one\+\_\+to\+\_\+many}} (torch.\+Float\+Tensor probs\+\_\+from, torch.\+Float\+Tensor probs\+\_\+to, Dict\mbox{[}str, Any\mbox{]} translation\+\_\+map)
\item 
None \mbox{\hyperlink{namespacebittensor_1_1utils_1_1tokenizer__utils_aacc9d76a7e49b18c20a5b44beafb8306}{translate\+\_\+many\+\_\+to\+\_\+one}} (torch.\+Float\+Tensor probs\+\_\+from, torch.\+Float\+Tensor probs\+\_\+to, Dict\mbox{[}str, Any\mbox{]} translation\+\_\+map)
\item 
None \mbox{\hyperlink{namespacebittensor_1_1utils_1_1tokenizer__utils_ac2ac0a649cfc204c56ad13913288a4f4}{translate\+\_\+tokenizer\+\_\+probs}} (torch.\+Float\+Tensor probs, torch.\+Float\+Tensor probs\+\_\+std, List\mbox{[}tuple\mbox{]} offset\+\_\+mapping, List\mbox{[}tuple\mbox{]} offset\+\_\+mapping\+\_\+std, Pre\+Trained\+Tokenizer\+Base \mbox{\hyperlink{classbittensor_1_1__tokenizer_1_1tokenizer}{tokenizer}}, Pre\+Trained\+Tokenizer\+Base std\+\_\+tokenizer, Dict\mbox{[}tuple, List\mbox{[}Dict\mbox{[}str, torch.\+Tensor\mbox{]}\mbox{]}\mbox{]} split\+\_\+map\+\_\+cache, Dict\mbox{[}str, Any\mbox{]} to\+\_\+translation\+\_\+map, Dict\mbox{[}str, Any\mbox{]} from\+\_\+translation\+\_\+map, torch.\+Long\+Tensor tokens, torch.\+Long\+Tensor tokens\+\_\+std)
\item 
str \mbox{\hyperlink{namespacebittensor_1_1utils_1_1tokenizer__utils_aa9eb42e9db4c2b1517d8ff7105c8ebd2}{get\+\_\+top\+\_\+probs}} (torch.\+Float\+Tensor probs, Pre\+Trained\+Tokenizer\+Base \mbox{\hyperlink{classbittensor_1_1__tokenizer_1_1tokenizer}{tokenizer}}, int amount=10)
\item 
torch.\+Float\+Tensor \mbox{\hyperlink{namespacebittensor_1_1utils_1_1tokenizer__utils_a7c296be466fd118c374bf1480d26904b}{translate\+\_\+logits\+\_\+to\+\_\+probs\+\_\+std}} (torch.\+Float\+Tensor logits, List\mbox{[}List\mbox{[}tuple\mbox{]}\mbox{]} offset\+\_\+mapping, List\mbox{[}List\mbox{[}tuple\mbox{]}\mbox{]} offset\+\_\+mapping\+\_\+std, Pre\+Trained\+Tokenizer\+Base \mbox{\hyperlink{classbittensor_1_1__tokenizer_1_1tokenizer}{tokenizer}}, Pre\+Trained\+Tokenizer\+Base std\+\_\+tokenizer, Dict\mbox{[}tuple, List\mbox{[}Dict\mbox{[}str, torch.\+Tensor\mbox{]}\mbox{]}\mbox{]} split\+\_\+map\+\_\+cache, Dict\mbox{[}str, Any\mbox{]} to\+\_\+translation\+\_\+map, Dict\mbox{[}str, Any\mbox{]} from\+\_\+translation\+\_\+map, torch.\+Long\+Tensor tokens, torch.\+Long\+Tensor tokens\+\_\+std, bool skip\+\_\+equivalent=True)
\item 
torch.\+Tensor \mbox{\hyperlink{namespacebittensor_1_1utils_1_1tokenizer__utils_a619c2a8870fee58a3993722ad5e2d7bb}{topk\+\_\+token\+\_\+phrases}} (torch.\+Tensor logits, Pre\+Trained\+Tokenizer\+Base \mbox{\hyperlink{classbittensor_1_1__tokenizer_1_1tokenizer}{tokenizer}}, int topk, int ignore\+\_\+index=-\/100)
\item 
def \mbox{\hyperlink{namespacebittensor_1_1utils_1_1tokenizer__utils_a335c0010cfa5615482ecc882408a8e0c}{compact\+\_\+topk\+\_\+token\+\_\+phrases}} (torch.\+Tensor topk\+\_\+tensor)
\item 
torch.\+Tensor \mbox{\hyperlink{namespacebittensor_1_1utils_1_1tokenizer__utils_a952b31d97e6588cd4166ee2ef319a1f0}{unravel\+\_\+topk\+\_\+token\+\_\+phrases}} (torch.\+Tensor compact\+\_\+topk, int topk, int ignore\+\_\+index=-\/100)
\item 
Tuple\mbox{[}torch.\+Tensor, torch.\+Tensor\mbox{]} \mbox{\hyperlink{namespacebittensor_1_1utils_1_1tokenizer__utils_ab00bac4739db21de5c408746fb2d631e}{phrase\+\_\+cross\+\_\+entropy}} (Union\mbox{[}List\mbox{[}List\mbox{[}int\mbox{]}\mbox{]}, torch.\+Tensor\mbox{]} target\+\_\+phrases, torch.\+Tensor topk\+\_\+tensor, int ignore\+\_\+index=-\/100, reduce=True, reduction=\textquotesingle{}mean\textquotesingle{}, int vocab\+\_\+size\+\_\+min=50257)
\item 
torch.\+Tensor \mbox{\hyperlink{namespacebittensor_1_1utils_1_1tokenizer__utils_a66ebcf1b07d1ee0f0ba07130967a40cc}{topk\+\_\+tokens\+\_\+to\+\_\+vocab\+\_\+size}} (torch.\+Tensor topk\+\_\+tensor, int vocab\+\_\+size\+\_\+std, int vocab\+\_\+size\+\_\+min=50257)
\item 
bool \mbox{\hyperlink{namespacebittensor_1_1utils_1_1tokenizer__utils_a54a25644b9cfa6b662c66e0707692a72}{check\+\_\+tokenizer\+\_\+equivalence}} (Pre\+Trained\+Tokenizer\+Base tokenizer\+\_\+to\+\_\+check, Pre\+Trained\+Tokenizer\+Base target\+\_\+tokenizer)
\item 
def \mbox{\hyperlink{namespacebittensor_1_1utils_1_1tokenizer__utils_abcdd922e5075b5eb136e9f056aa547c1}{prune\+\_\+tokens}} (torch.\+Float\+Tensor inputs, int prune\+\_\+len=1, int margin=3)
\item 
List\mbox{[}List\mbox{[}List\mbox{[}Any\mbox{]}\mbox{]}\mbox{]} \mbox{\hyperlink{namespacebittensor_1_1utils_1_1tokenizer__utils_a48b79b3d00a8fa64ade7ce9ba3db7410}{pad\+\_\+offsets}} (List\mbox{[}List\mbox{[}tuple\mbox{]}\mbox{]} offsets\+\_\+batch, List\mbox{[}List\mbox{[}List\mbox{[}Any\mbox{]}\mbox{]}\mbox{]} source\+\_\+offsets\+\_\+batch, List\mbox{[}List\mbox{[}List\mbox{[}Any\mbox{]}\mbox{]}\mbox{]} pad\+\_\+offsets\+\_\+batch)
\item 
List\mbox{[}List\mbox{[}int\mbox{]}\mbox{]} \mbox{\hyperlink{namespacebittensor_1_1utils_1_1tokenizer__utils_a58bc4ccb79ae645c72c7a5e4244b99d3}{find\+\_\+offsets}} (str string, str substring)
\item 
Tuple\mbox{[}str, List\mbox{[}List\mbox{[}int\mbox{]}\mbox{]}\mbox{]} \mbox{\hyperlink{namespacebittensor_1_1utils_1_1tokenizer__utils_a8e57c9718294c345d7f4ff730970723d}{replace\+\_\+at\+\_\+offsets}} (str string, List\mbox{[}List\mbox{[}Any\mbox{]}\mbox{]} offsets)
\item 
Dict\mbox{[}str, str\mbox{]} \mbox{\hyperlink{namespacebittensor_1_1utils_1_1tokenizer__utils_a4da460aad949bf91a0f414c1a0f30df1}{get\+\_\+special\+\_\+token\+\_\+pairings}} (Pre\+Trained\+Tokenizer\+Base from\+\_\+tokenizer, Pre\+Trained\+Tokenizer\+Base to\+\_\+tokenizer)
\item 
Tuple\mbox{[}List\mbox{[}str\mbox{]}, List\mbox{[}List\mbox{[}List\mbox{[}int\mbox{]}\mbox{]}\mbox{]}, List\mbox{[}List\mbox{[}List\mbox{[}int\mbox{]}\mbox{]}\mbox{]}, List\mbox{[}List\mbox{[}List\mbox{[}Any\mbox{]}\mbox{]}\mbox{]}\mbox{]} \mbox{\hyperlink{namespacebittensor_1_1utils_1_1tokenizer__utils_a6717d83c925189f86590b5ab3d88df17}{translate\+\_\+special\+\_\+token\+\_\+text}} (List\mbox{[}str\mbox{]} text\+\_\+batch, Pre\+Trained\+Tokenizer\+Base from\+\_\+tokenizer, Pre\+Trained\+Tokenizer\+Base to\+\_\+tokenizer)
\item 
def \mbox{\hyperlink{namespacebittensor_1_1utils_1_1tokenizer__utils_aef7aceb6bc8b3ad0d08f7fed25c876d8}{set\+\_\+vocab\+\_\+len}} (Pre\+Trained\+Tokenizer\+Base \mbox{\hyperlink{classbittensor_1_1__tokenizer_1_1tokenizer}{tokenizer}})
\item 
def \mbox{\hyperlink{namespacebittensor_1_1utils_1_1tokenizer__utils_a558123e572c10af14c002993c9b04f36}{set\+\_\+whitespace\+\_\+preserving}} (Pre\+Trained\+Tokenizer\+Base \mbox{\hyperlink{classbittensor_1_1__tokenizer_1_1tokenizer}{tokenizer}})
\item 
def \mbox{\hyperlink{namespacebittensor_1_1utils_1_1tokenizer__utils_a066a0913f46f47503972e5df32808063}{set\+\_\+std\+\_\+token\+\_\+phrases}} (\mbox{\hyperlink{classbittensor_1_1__tokenizer_1_1tokenizer}{tokenizer}}, std\+\_\+tokenizer)
\item 
\mbox{\Hypertarget{namespacebittensor_1_1utils_1_1tokenizer__utils_af7fd6b7e6dee158a3f991bdd93a1f559}\label{namespacebittensor_1_1utils_1_1tokenizer__utils_af7fd6b7e6dee158a3f991bdd93a1f559}} 
def {\bfseries prep\+\_\+tokenizer} (\mbox{\hyperlink{classbittensor_1_1__tokenizer_1_1tokenizer}{tokenizer}}, std\+\_\+tokenizer=None)
\end{DoxyCompactItemize}
\doxysubsection*{Variables}
\begin{DoxyCompactItemize}
\item 
\mbox{\Hypertarget{namespacebittensor_1_1utils_1_1tokenizer__utils_ace858cca96a45c6171798f08dfe507fb}\label{namespacebittensor_1_1utils_1_1tokenizer__utils_ace858cca96a45c6171798f08dfe507fb}} 
int {\bfseries EPSILON} = 1e-\/40
\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
\begin{DoxyVerb} Utils for tokenizer equivalence checking, logit translation, etc.
\end{DoxyVerb}
 

\doxysubsection{Function Documentation}
\mbox{\Hypertarget{namespacebittensor_1_1utils_1_1tokenizer__utils_a8a5ed01b4d5f6883475c9db9f896edbe}\label{namespacebittensor_1_1utils_1_1tokenizer__utils_a8a5ed01b4d5f6883475c9db9f896edbe}} 
\index{bittensor.utils.tokenizer\_utils@{bittensor.utils.tokenizer\_utils}!align\_tokenizer\_sequences@{align\_tokenizer\_sequences}}
\index{align\_tokenizer\_sequences@{align\_tokenizer\_sequences}!bittensor.utils.tokenizer\_utils@{bittensor.utils.tokenizer\_utils}}
\doxysubsubsection{\texorpdfstring{align\_tokenizer\_sequences()}{align\_tokenizer\_sequences()}}
{\footnotesize\ttfamily  Tuple\mbox{[}torch.\+Float\+Tensor,                                                                                                List\mbox{[}tuple\mbox{]},                                                                                                torch.\+Long\+Tensor\mbox{]} bittensor.\+utils.\+tokenizer\+\_\+utils.\+align\+\_\+tokenizer\+\_\+sequences (\begin{DoxyParamCaption}\item[{torch.\+Float\+Tensor}]{probs,  }\item[{List\mbox{[}tuple\mbox{]}}]{offset\+\_\+mapping,  }\item[{List\mbox{[}tuple\mbox{]}}]{offset\+\_\+mapping\+\_\+std,  }\item[{Pre\+Trained\+Tokenizer\+Base}]{tokenizer,  }\item[{Dict\mbox{[}tuple, List\mbox{[}Dict\mbox{[}str, torch.\+Tensor\mbox{]}\mbox{]}\mbox{]}}]{split\+\_\+map\+\_\+cache,  }\item[{torch.\+Long\+Tensor}]{tokens,  }\item[{torch.\+Long\+Tensor}]{tokens\+\_\+std }\end{DoxyParamCaption})}

\begin{DoxyVerb}Align an input tokenization distribution to standard tokenization segments by depth-splitting
the input distribution at greedily chosen locations. Prepares the input distribution for mapping to a standard
distribution.
Args:
probs (:obj:`torch.FloatTensor`, `required`):
[sequence_len, vocab_size] Input probability distribution over a tokenizer vocabulary.
offset_mapping (:obj:`List[tuple]`, `required`):
Tokenizer offset mappings for a specific sequence [(left_0, right_0), (left_1, right_1), ...].
offset_mapping_std (:obj:`List[tuple]`, `required`):
Standard tokenizer offset mappings for a specific sequence [(left_0, right_0), (left_1, right_1), ...]
tokenizer (:obj:`PreTrainedTokenizerBase`, `required`):
Source tokenizer.
split_map_cache (:obj:`Dict[tuple, List[Dict[str, torch.Tensor]]]`, `required`):
A dictionary of depths keying split_maps of mappings from original tokens to
target tokens at each depth of the split.
tokens (:obj:`torch.LongTensor`, `required`):
[sequence_len] A sequence of tokens produced by the source tokenizer.
tokens_std (:obj:`torch.LongTensor`, `required`):
[std_sequence_len] A sequence of tokens produced by the standard tokenizer.

Returns:
aligned_probs (:obj:`torch.FloatTensor`, `required`):
[new_sequence_len, vocab_size] Aligned probability distribution over a tokenizer vocabulary.
aligned_offset_mapping (:obj:`List[tuple]`, `required`):
Tokenizer aligned offset mappings for a specific sequence [(left_0, right_0), (left_1, right_1), ...].
aligned_tokens (:obj:`torch.LongTensor`, `required`):
A sequence of aligned tokens produced by the source tokenizer.
\end{DoxyVerb}
 \mbox{\Hypertarget{namespacebittensor_1_1utils_1_1tokenizer__utils_a54a25644b9cfa6b662c66e0707692a72}\label{namespacebittensor_1_1utils_1_1tokenizer__utils_a54a25644b9cfa6b662c66e0707692a72}} 
\index{bittensor.utils.tokenizer\_utils@{bittensor.utils.tokenizer\_utils}!check\_tokenizer\_equivalence@{check\_tokenizer\_equivalence}}
\index{check\_tokenizer\_equivalence@{check\_tokenizer\_equivalence}!bittensor.utils.tokenizer\_utils@{bittensor.utils.tokenizer\_utils}}
\doxysubsubsection{\texorpdfstring{check\_tokenizer\_equivalence()}{check\_tokenizer\_equivalence()}}
{\footnotesize\ttfamily  bool bittensor.\+utils.\+tokenizer\+\_\+utils.\+check\+\_\+tokenizer\+\_\+equivalence (\begin{DoxyParamCaption}\item[{Pre\+Trained\+Tokenizer\+Base}]{tokenizer\+\_\+to\+\_\+check,  }\item[{Pre\+Trained\+Tokenizer\+Base}]{target\+\_\+tokenizer }\end{DoxyParamCaption})}

\begin{DoxyVerb}Is tokenizer_to_check equivalent to target_tokenizer?
    Args:
        tokenizer_to_check (:obj:`PreTrainedTokenizerBase`, `required`):
            Tokenizer to check for equivalence.
        target_tokenizer (:obj:`PreTrainedTokenizerBase`, `required`):
            Target tokenizer to check equivalence against.

    Returns:
        result (:obj:`bool`, `required`)
\end{DoxyVerb}
 \mbox{\Hypertarget{namespacebittensor_1_1utils_1_1tokenizer__utils_a335c0010cfa5615482ecc882408a8e0c}\label{namespacebittensor_1_1utils_1_1tokenizer__utils_a335c0010cfa5615482ecc882408a8e0c}} 
\index{bittensor.utils.tokenizer\_utils@{bittensor.utils.tokenizer\_utils}!compact\_topk\_token\_phrases@{compact\_topk\_token\_phrases}}
\index{compact\_topk\_token\_phrases@{compact\_topk\_token\_phrases}!bittensor.utils.tokenizer\_utils@{bittensor.utils.tokenizer\_utils}}
\doxysubsubsection{\texorpdfstring{compact\_topk\_token\_phrases()}{compact\_topk\_token\_phrases()}}
{\footnotesize\ttfamily def bittensor.\+utils.\+tokenizer\+\_\+utils.\+compact\+\_\+topk\+\_\+token\+\_\+phrases (\begin{DoxyParamCaption}\item[{torch.\+Tensor}]{topk\+\_\+tensor }\end{DoxyParamCaption})}

\begin{DoxyVerb}Compact 2D topk_tensor [batch_size, (topk + 1), max_len] by removing ignore_index padding, and also offset
tokens by 2 to preserve [0, 1] for probabilities to allow for proper unraveling demarcated by
probability boundaries.
    Args:
        topk_tensor (:obj:`torch.Tensor`, `required`):
            [batch_size, (topk + 1), max_len] tensor includes topk token probabilities (prob_k) + floor_prob
            in first column with gradients attached, with std_tokens in remaining columns with ignore_index padding.
            Content structure:
            [[[prob_k=0_b=0, tok_0_k=0_b=0, tok_1_k=0_b=0, ..., ignore_index?],
              [prob_k=1_b=0, tok_0_k=1_b=0, tok_1_k=1_b=0, ..., ignore_index?],
              [...],
              [prob_floor_b=0, ignore_index, ..., ignore_index]],
             [[prob_k=0_b=1, tok_0_k=0_b=1, tok_1_k=0_b=1, ..., ignore_index?],
              [prob_k=1_b=1, tok_0_k=1_b=1, tok_1_k=1_b=1, ..., ignore_index?],
              [...],
              [prob_floor_b=1, ignore_index, ..., ignore_index]],
             [...]]

    Returns:
        compact_topk (:obj:`torch.Tensor`, `required`):
            [sum_b(sum_k(len(phrase_k) + 1)_b)] Compacted 1-D tensor >= batch_size * (2 * topk + 1),
            since 2 * topk + 1: topk x [probability, token sequence (at least one token)] +
            floor probability (rest).
            Content structure:
                [prob_k=0_b=0, tok_0_k=0_b=0, tok_1_k=0_b=0, ..., prob_k=1_b=0, tok_0_k=1_b=0, ..., prob_floor_b=0,
                 prob_k=0_b=1, tok_0_k=0_b=1, tok_1_k=0_b=1, ..., prob_k=1_b=1, tok_0_k=1_b=1, ..., prob_floor_b=1,
                 ...]
\end{DoxyVerb}
 \mbox{\Hypertarget{namespacebittensor_1_1utils_1_1tokenizer__utils_a58bc4ccb79ae645c72c7a5e4244b99d3}\label{namespacebittensor_1_1utils_1_1tokenizer__utils_a58bc4ccb79ae645c72c7a5e4244b99d3}} 
\index{bittensor.utils.tokenizer\_utils@{bittensor.utils.tokenizer\_utils}!find\_offsets@{find\_offsets}}
\index{find\_offsets@{find\_offsets}!bittensor.utils.tokenizer\_utils@{bittensor.utils.tokenizer\_utils}}
\doxysubsubsection{\texorpdfstring{find\_offsets()}{find\_offsets()}}
{\footnotesize\ttfamily  List\mbox{[}List\mbox{[}int\mbox{]}\mbox{]} bittensor.\+utils.\+tokenizer\+\_\+utils.\+find\+\_\+offsets (\begin{DoxyParamCaption}\item[{str}]{string,  }\item[{str}]{substring }\end{DoxyParamCaption})}

\begin{DoxyVerb}Finds all the [start, end] offsets of substring in string.
Assumes there is no overlap of substring, nor recursive overlap.
    Args:
        string (:obj:`str`, `required`):
            Main string to find offsets in.
        substring (:obj:`str`, `required`):
            Substring to search for in string.

    Returns:
        offsets (:obj:`List[List[int]]`, `required`):
            Offsets denoting the [start, end] positions of substring in string.
\end{DoxyVerb}
 \mbox{\Hypertarget{namespacebittensor_1_1utils_1_1tokenizer__utils_a4da460aad949bf91a0f414c1a0f30df1}\label{namespacebittensor_1_1utils_1_1tokenizer__utils_a4da460aad949bf91a0f414c1a0f30df1}} 
\index{bittensor.utils.tokenizer\_utils@{bittensor.utils.tokenizer\_utils}!get\_special\_token\_pairings@{get\_special\_token\_pairings}}
\index{get\_special\_token\_pairings@{get\_special\_token\_pairings}!bittensor.utils.tokenizer\_utils@{bittensor.utils.tokenizer\_utils}}
\doxysubsubsection{\texorpdfstring{get\_special\_token\_pairings()}{get\_special\_token\_pairings()}}
{\footnotesize\ttfamily  Dict\mbox{[}str, str\mbox{]} bittensor.\+utils.\+tokenizer\+\_\+utils.\+get\+\_\+special\+\_\+token\+\_\+pairings (\begin{DoxyParamCaption}\item[{Pre\+Trained\+Tokenizer\+Base}]{from\+\_\+tokenizer,  }\item[{Pre\+Trained\+Tokenizer\+Base}]{to\+\_\+tokenizer }\end{DoxyParamCaption})}

\begin{DoxyVerb}Determines a prioritized matching of special token texts between two tokenizers.
Purpose is to produce replacement pairs so special token test is correctly represented for target tokenizer.
    Args:
        from_tokenizer (:obj:`PreTrainedTokenizerBase`, `required`):
            From tokenizer.
        to_tokenizer (:obj:`PreTrainedTokenizerBase`, `required`):
            To tokenizer.

    Returns:
        pairings (:obj:`Dict[str, str]`, `required`):
            Prioritized dictionary of From_special_token_text -> To_special_token_text.
\end{DoxyVerb}
 \mbox{\Hypertarget{namespacebittensor_1_1utils_1_1tokenizer__utils_a31406026136d3c9b3ec5469bd1e0c0f7}\label{namespacebittensor_1_1utils_1_1tokenizer__utils_a31406026136d3c9b3ec5469bd1e0c0f7}} 
\index{bittensor.utils.tokenizer\_utils@{bittensor.utils.tokenizer\_utils}!get\_tokenizer\_alignment\_splits@{get\_tokenizer\_alignment\_splits}}
\index{get\_tokenizer\_alignment\_splits@{get\_tokenizer\_alignment\_splits}!bittensor.utils.tokenizer\_utils@{bittensor.utils.tokenizer\_utils}}
\doxysubsubsection{\texorpdfstring{get\_tokenizer\_alignment\_splits()}{get\_tokenizer\_alignment\_splits()}}
{\footnotesize\ttfamily  Dict\mbox{[}int, tuple\mbox{]} bittensor.\+utils.\+tokenizer\+\_\+utils.\+get\+\_\+tokenizer\+\_\+alignment\+\_\+splits (\begin{DoxyParamCaption}\item[{List\mbox{[}tuple\mbox{]}}]{offset\+\_\+mapping,  }\item[{List\mbox{[}tuple\mbox{]}}]{offset\+\_\+mapping\+\_\+std }\end{DoxyParamCaption})}

\begin{DoxyVerb}    Calculates split depths necessary for tokens to align input offsets to standard offsets.
    Only input offsets may be split, not standard offsets, to create one-to-one, one-to-many, or many-to-one
    token alignments between input-to-standard tokenization.
    Allows for multiple depth splits on a token.
        Args:
            offset_mapping (:obj:`List[tuple]`, `required`):
                Tokenizer offset mappings for a specific sequence [(left_0, right_0), (left_1, right_1), ...].
            offset_mapping_std (:obj:`List[tuple]`, `required`):
                Standard tokenizer offset mappings for a specific sequence [(left_0, right_0), (left_1, right_1), ...]

        Returns:
            splits (:obj:`Dict[int, tuple]`, `required`):
                For tokens that have to be split, {Token index: (split depth 1, split depth 2, ...), ...}.\end{DoxyVerb}
 \mbox{\Hypertarget{namespacebittensor_1_1utils_1_1tokenizer__utils_af76d2962aae10ddc4826d5f349fd9051}\label{namespacebittensor_1_1utils_1_1tokenizer__utils_af76d2962aae10ddc4826d5f349fd9051}} 
\index{bittensor.utils.tokenizer\_utils@{bittensor.utils.tokenizer\_utils}!get\_tokenizer\_depth\_split\_map@{get\_tokenizer\_depth\_split\_map}}
\index{get\_tokenizer\_depth\_split\_map@{get\_tokenizer\_depth\_split\_map}!bittensor.utils.tokenizer\_utils@{bittensor.utils.tokenizer\_utils}}
\doxysubsubsection{\texorpdfstring{get\_tokenizer\_depth\_split\_map()}{get\_tokenizer\_depth\_split\_map()}}
{\footnotesize\ttfamily  List\mbox{[}Dict\mbox{[}str, torch.\+Long\+Tensor\mbox{]}\mbox{]} bittensor.\+utils.\+tokenizer\+\_\+utils.\+get\+\_\+tokenizer\+\_\+depth\+\_\+split\+\_\+map (\begin{DoxyParamCaption}\item[{Pre\+Trained\+Tokenizer\+Base}]{tokenizer,  }\item[{tuple}]{depths }\end{DoxyParamCaption})}

\begin{DoxyVerb}Split individual token strings at specified depths, retokenize each resulting segment,
keep only the first token of each segment (if there is one).
Purpose is to provide targets for scattering probabilities when a single distribution requires a depth split.
Args:
tokenizer (:obj:`PreTrainedTokenizerBase`, `required`):
Tokenizer.
depths (:obj:`tuple`, `required`):
Tuple of depths at which tokens strings will be split.

Returns:
split_map (:obj:`List[Dict[str, torch.LongTensor]]`, `required`):
\end{DoxyVerb}
 \mbox{\Hypertarget{namespacebittensor_1_1utils_1_1tokenizer__utils_a80673928d5022558ece6eb5cdc9e12be}\label{namespacebittensor_1_1utils_1_1tokenizer__utils_a80673928d5022558ece6eb5cdc9e12be}} 
\index{bittensor.utils.tokenizer\_utils@{bittensor.utils.tokenizer\_utils}!get\_tokenizer\_sequence\_mappings@{get\_tokenizer\_sequence\_mappings}}
\index{get\_tokenizer\_sequence\_mappings@{get\_tokenizer\_sequence\_mappings}!bittensor.utils.tokenizer\_utils@{bittensor.utils.tokenizer\_utils}}
\doxysubsubsection{\texorpdfstring{get\_tokenizer\_sequence\_mappings()}{get\_tokenizer\_sequence\_mappings()}}
{\footnotesize\ttfamily  List\mbox{[}tuple\mbox{]} bittensor.\+utils.\+tokenizer\+\_\+utils.\+get\+\_\+tokenizer\+\_\+sequence\+\_\+mappings (\begin{DoxyParamCaption}\item[{List\mbox{[}tuple\mbox{]}}]{offset\+\_\+mapping,  }\item[{List\mbox{[}tuple\mbox{]}}]{offset\+\_\+mapping\+\_\+std }\end{DoxyParamCaption})}

\begin{DoxyVerb}Greedily determine the one-to-one, one-to-many, or many-to-one token alignments
between input-to-standard tokenizations.
Disallow many-to-many mappings, but allow for right-aligned overlapping tokens.
Args:
offset_mapping (:obj:`List[tuple]`, `required`):
Tokenizer offset mappings for a specific sequence [(left_0, right_0), (left_1, right_1), ...].
offset_mapping_std (:obj:`List[tuple]`, `required`):
Standard tokenizer offset mappings for a specific sequence [(left_0, right_0), (left_1, right_1), ...]

Returns:
mappings (:obj:`List[tuple]`, `required`):
List of mapping tuples:
[tuple( right_idx, right_idx_std,
        segment_count_base, segment_count_std_base,
        segment_count_overlap, segment_count_std_overlap), ...]
\end{DoxyVerb}
 \mbox{\Hypertarget{namespacebittensor_1_1utils_1_1tokenizer__utils_aa9eb42e9db4c2b1517d8ff7105c8ebd2}\label{namespacebittensor_1_1utils_1_1tokenizer__utils_aa9eb42e9db4c2b1517d8ff7105c8ebd2}} 
\index{bittensor.utils.tokenizer\_utils@{bittensor.utils.tokenizer\_utils}!get\_top\_probs@{get\_top\_probs}}
\index{get\_top\_probs@{get\_top\_probs}!bittensor.utils.tokenizer\_utils@{bittensor.utils.tokenizer\_utils}}
\doxysubsubsection{\texorpdfstring{get\_top\_probs()}{get\_top\_probs()}}
{\footnotesize\ttfamily  str bittensor.\+utils.\+tokenizer\+\_\+utils.\+get\+\_\+top\+\_\+probs (\begin{DoxyParamCaption}\item[{torch.\+Float\+Tensor}]{probs,  }\item[{Pre\+Trained\+Tokenizer\+Base}]{tokenizer,  }\item[{int }]{amount = {\ttfamily 10} }\end{DoxyParamCaption})}

\begin{DoxyVerb}Constructs output string with top amount of highest probability token strings.
Used to display the top probabilities.
Args:
    probs (:obj:`torch.FloatTensor`, `required`):
        [vocab_size] Probability distribution over a tokenizer vocabulary.
    tokenizer (:obj:`PreTrainedTokenizerBase`, `required`):
        Tokenizer.
    amount: (:obj:`int`, `optional`):
        Amount of top tokens to return

Returns:
    string (:obj:`str`, `required`):
    Highest probability token strings, prob[token-string] ...
\end{DoxyVerb}
 \mbox{\Hypertarget{namespacebittensor_1_1utils_1_1tokenizer__utils_a52d7949730e4e84f9a53fd0bff3e0f22}\label{namespacebittensor_1_1utils_1_1tokenizer__utils_a52d7949730e4e84f9a53fd0bff3e0f22}} 
\index{bittensor.utils.tokenizer\_utils@{bittensor.utils.tokenizer\_utils}!get\_translation\_map@{get\_translation\_map}}
\index{get\_translation\_map@{get\_translation\_map}!bittensor.utils.tokenizer\_utils@{bittensor.utils.tokenizer\_utils}}
\doxysubsubsection{\texorpdfstring{get\_translation\_map()}{get\_translation\_map()}}
{\footnotesize\ttfamily  Dict\mbox{[}str, Any\mbox{]} bittensor.\+utils.\+tokenizer\+\_\+utils.\+get\+\_\+translation\+\_\+map (\begin{DoxyParamCaption}\item[{Pre\+Trained\+Tokenizer\+Base}]{from\+\_\+tokenizer,  }\item[{Pre\+Trained\+Tokenizer\+Base}]{to\+\_\+tokenizer }\end{DoxyParamCaption})}

\begin{DoxyVerb}Map individual token phrases from a tokenizer to another tokenizer.
Args:
    from_tokenizer (:obj:`PreTrainedTokenizerBase`, `required`):
        From tokenizer.
    to_tokenizer (:obj:`PreTrainedTokenizerBase`, `required`):
        To tokenizer.

Returns:
    translation_map (:obj:`Dict[str, Any]`, `required`):
        Maps for each observed length, a source token to a token sequence of that length,
        with source index to target indices.
\end{DoxyVerb}
 \mbox{\Hypertarget{namespacebittensor_1_1utils_1_1tokenizer__utils_a48b79b3d00a8fa64ade7ce9ba3db7410}\label{namespacebittensor_1_1utils_1_1tokenizer__utils_a48b79b3d00a8fa64ade7ce9ba3db7410}} 
\index{bittensor.utils.tokenizer\_utils@{bittensor.utils.tokenizer\_utils}!pad\_offsets@{pad\_offsets}}
\index{pad\_offsets@{pad\_offsets}!bittensor.utils.tokenizer\_utils@{bittensor.utils.tokenizer\_utils}}
\doxysubsubsection{\texorpdfstring{pad\_offsets()}{pad\_offsets()}}
{\footnotesize\ttfamily  List\mbox{[}List\mbox{[}List\mbox{[}Any\mbox{]}\mbox{]}\mbox{]} bittensor.\+utils.\+tokenizer\+\_\+utils.\+pad\+\_\+offsets (\begin{DoxyParamCaption}\item[{List\mbox{[}List\mbox{[}tuple\mbox{]}\mbox{]}}]{offsets\+\_\+batch,  }\item[{List\mbox{[}List\mbox{[}List\mbox{[}Any\mbox{]}\mbox{]}\mbox{]}}]{source\+\_\+offsets\+\_\+batch,  }\item[{List\mbox{[}List\mbox{[}List\mbox{[}Any\mbox{]}\mbox{]}\mbox{]}}]{pad\+\_\+offsets\+\_\+batch }\end{DoxyParamCaption})}

\begin{DoxyVerb}Pads specific tuples in offsets_batch, selected by source_offsets_batch with
associated paddings in pad_offsets_batch.
Purpose is typically to add padding to align two tokenization offsets at special tokens.
    Args:
        offsets_batch (:obj:`List[List[tuple]]`, `required`):
                Batch of full input tokenizer offset mappings to be used for alteration
                [[(left_0, right_0), (left_1, right_1), ...], ...].
        source_offsets_batch (:obj:`List[List[List[Any]]]`, `required`):
                Batch of tokenizer offset mappings indicating replacement tuples in offsets_batch
                [[(left_0, right_0), (left_1, right_1), ...], ...].
        pad_offsets_batch (:obj:`List[List[List[Any]]]`, `required`):
                Batch of offset paddings associated with each source_offsets_batch replacement tuple
                [[(left_pad_0, right_pad_0), (left_pad_1, right_pad_1), ...], ...].

    Returns:
        new_offsets_batch (:obj:`List[List[List[Any]]]`, `required`):
                Batch of padded full input tokenizer offset mappings
                [[(left_0, right_0), (left_1, right_1), ...], ...].
\end{DoxyVerb}
 \mbox{\Hypertarget{namespacebittensor_1_1utils_1_1tokenizer__utils_ab00bac4739db21de5c408746fb2d631e}\label{namespacebittensor_1_1utils_1_1tokenizer__utils_ab00bac4739db21de5c408746fb2d631e}} 
\index{bittensor.utils.tokenizer\_utils@{bittensor.utils.tokenizer\_utils}!phrase\_cross\_entropy@{phrase\_cross\_entropy}}
\index{phrase\_cross\_entropy@{phrase\_cross\_entropy}!bittensor.utils.tokenizer\_utils@{bittensor.utils.tokenizer\_utils}}
\doxysubsubsection{\texorpdfstring{phrase\_cross\_entropy()}{phrase\_cross\_entropy()}}
{\footnotesize\ttfamily  Tuple\mbox{[}torch.\+Tensor, torch.\+Tensor\mbox{]} bittensor.\+utils.\+tokenizer\+\_\+utils.\+phrase\+\_\+cross\+\_\+entropy (\begin{DoxyParamCaption}\item[{Union\mbox{[}List\mbox{[}List\mbox{[}int\mbox{]}\mbox{]}, torch.\+Tensor\mbox{]}}]{target\+\_\+phrases,  }\item[{torch.\+Tensor}]{topk\+\_\+tensor,  }\item[{int }]{ignore\+\_\+index = {\ttfamily -\/100},  }\item[{}]{reduce = {\ttfamily True},  }\item[{}]{reduction = {\ttfamily \textquotesingle{}mean\textquotesingle{}},  }\item[{int }]{vocab\+\_\+size\+\_\+min = {\ttfamily 50257} }\end{DoxyParamCaption})}

\begin{DoxyVerb}Calculates the cross entropy of a phrase prediction against a target phrase, so that this is a multi-token
extension of typical cross entropy calculated for next token prediction.
    Args:
        target_phrases (:obj:`List[List[int]]`, `required`):
            [batch_size, *] Target phrases in standard token sequence list.
        topk_tensor (:obj:`torch.Tensor`, `required`):
            [batch_size, (topk + 1), max_len] tensor includes topk token probabilities (prob_k) + floor_prob
            in first column with gradients attached, with std_tokens in remaining columns with ignore_index padding.
            Content structure:
            [[[prob_k=0_b=0, tok_0_k=0_b=0, tok_1_k=0_b=0, ..., ignore_index?],
              [prob_k=1_b=0, tok_0_k=1_b=0, tok_1_k=1_b=0, ..., ignore_index?],
              [...],
              [prob_floor_b=0, ignore_index, ..., ignore_index]],
             [[prob_k=0_b=1, tok_0_k=0_b=1, tok_1_k=0_b=1, ..., ignore_index?],
              [prob_k=1_b=1, tok_0_k=1_b=1, tok_1_k=1_b=1, ..., ignore_index?],
              [...],
              [prob_floor_b=1, ignore_index, ..., ignore_index]],
             [...]]
        ignore_index (:obj:`int`, `optional`):
            Padding value to use for unfilled token positions in a shorter token phrase.
        reduce (:obj:`bool`, `optional`):
            Whether to reduce the cross entropy over the batch dimension.
        reduction (:obj:`str`, `optional`):
            Reduction function to perform when reduce is True.
        vocab_size_min (:obj:`int`, `optional`):
            Minimum server vocab_size expected, should set to nominal 50257,
            used to prevent the floor_probs from being too large.
    Returns:
        loss_val (:obj:`torch.Tensor`, `required`):
            Validation cross entropy loss, either scalar if reduce or [batch_size].
        loss (:obj:`torch.Tensor`, `required`):
            Phrase cross entropy loss, either scalar if reduce or [batch_size].
\end{DoxyVerb}
 \mbox{\Hypertarget{namespacebittensor_1_1utils_1_1tokenizer__utils_abcdd922e5075b5eb136e9f056aa547c1}\label{namespacebittensor_1_1utils_1_1tokenizer__utils_abcdd922e5075b5eb136e9f056aa547c1}} 
\index{bittensor.utils.tokenizer\_utils@{bittensor.utils.tokenizer\_utils}!prune\_tokens@{prune\_tokens}}
\index{prune\_tokens@{prune\_tokens}!bittensor.utils.tokenizer\_utils@{bittensor.utils.tokenizer\_utils}}
\doxysubsubsection{\texorpdfstring{prune\_tokens()}{prune\_tokens()}}
{\footnotesize\ttfamily def bittensor.\+utils.\+tokenizer\+\_\+utils.\+prune\+\_\+tokens (\begin{DoxyParamCaption}\item[{torch.\+Float\+Tensor}]{inputs,  }\item[{int }]{prune\+\_\+len = {\ttfamily 1},  }\item[{int }]{margin = {\ttfamily 3} }\end{DoxyParamCaption})}

\begin{DoxyVerb}Prune tokens from a batch of sequences randomly by removing prune_len tokens from each sequence,
leaving the end margin intact.
    Args:
        inputs (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, seq_len)`, `required`):
            Tensor inputs to have tokens pruned.
        prune_len (:obj:`int`, `optional`):
            Number of tokens to prune from each validation input sequence.
        margin (:obj:`int`, `optional`):
            Number of tokens at the end of the sequence to leave unpruned.
    Returns:
        pruned_inputs (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, seq_len - prune_len)`, `required`)
\end{DoxyVerb}
 \mbox{\Hypertarget{namespacebittensor_1_1utils_1_1tokenizer__utils_a8e57c9718294c345d7f4ff730970723d}\label{namespacebittensor_1_1utils_1_1tokenizer__utils_a8e57c9718294c345d7f4ff730970723d}} 
\index{bittensor.utils.tokenizer\_utils@{bittensor.utils.tokenizer\_utils}!replace\_at\_offsets@{replace\_at\_offsets}}
\index{replace\_at\_offsets@{replace\_at\_offsets}!bittensor.utils.tokenizer\_utils@{bittensor.utils.tokenizer\_utils}}
\doxysubsubsection{\texorpdfstring{replace\_at\_offsets()}{replace\_at\_offsets()}}
{\footnotesize\ttfamily  Tuple\mbox{[}str, List\mbox{[}List\mbox{[}int\mbox{]}\mbox{]}\mbox{]} bittensor.\+utils.\+tokenizer\+\_\+utils.\+replace\+\_\+at\+\_\+offsets (\begin{DoxyParamCaption}\item[{str}]{string,  }\item[{List\mbox{[}List\mbox{[}Any\mbox{]}\mbox{]}}]{offsets }\end{DoxyParamCaption})}

\begin{DoxyVerb}Replace indicated [left, right] offset positions with a new substring, by
deleting [left, right] content and adding [left, left+len(substring)] substring,
adjusting offsets incrementally.
Assumes an incremental ordered, non-overlapping list of offsets, constructing
the new string incrementally and recording new offsets.
    Args:
        string (:obj:`str`, `required`):
            Main string to perform replacements for.
        offsets (:obj:`List[List[Any]]`, `required`):
            Offsets where replacements are made with replacement substring
            [[left_0, right_0, substring_0], ...]

    Returns:
        new_string (:obj:`str`, `required`):
            New string where replacements were made.
        new_offsets (:obj:`List[List[Any]]`, `required`):
            New offsets where replacements are now located
            [[left_0, right_0], [left_1, right_1], ...]
\end{DoxyVerb}
 \mbox{\Hypertarget{namespacebittensor_1_1utils_1_1tokenizer__utils_a066a0913f46f47503972e5df32808063}\label{namespacebittensor_1_1utils_1_1tokenizer__utils_a066a0913f46f47503972e5df32808063}} 
\index{bittensor.utils.tokenizer\_utils@{bittensor.utils.tokenizer\_utils}!set\_std\_token\_phrases@{set\_std\_token\_phrases}}
\index{set\_std\_token\_phrases@{set\_std\_token\_phrases}!bittensor.utils.tokenizer\_utils@{bittensor.utils.tokenizer\_utils}}
\doxysubsubsection{\texorpdfstring{set\_std\_token\_phrases()}{set\_std\_token\_phrases()}}
{\footnotesize\ttfamily def bittensor.\+utils.\+tokenizer\+\_\+utils.\+set\+\_\+std\+\_\+token\+\_\+phrases (\begin{DoxyParamCaption}\item[{}]{tokenizer,  }\item[{}]{std\+\_\+tokenizer }\end{DoxyParamCaption})}

\begin{DoxyVerb}Sets std_token_phrases which are the tokenizer token strings tokenized with std_tokenizer, so
the std_tokenizer equivalent of the tokenizer token strings.
Used for converting model predictions/logits into std_tokenizer representations, for example in TextCausalLMNext.
    Args:
        tokenizer(:obj:`PreTrainedTokenizerBase`, `required`):
            Tokenizer to set std_token_phrases for.
        std_tokenizer(:obj:`PreTrainedTokenizerBase`, `required`):
            Standard bittensor tokenizer to convert to.

    Returns:\end{DoxyVerb}
 \mbox{\Hypertarget{namespacebittensor_1_1utils_1_1tokenizer__utils_aef7aceb6bc8b3ad0d08f7fed25c876d8}\label{namespacebittensor_1_1utils_1_1tokenizer__utils_aef7aceb6bc8b3ad0d08f7fed25c876d8}} 
\index{bittensor.utils.tokenizer\_utils@{bittensor.utils.tokenizer\_utils}!set\_vocab\_len@{set\_vocab\_len}}
\index{set\_vocab\_len@{set\_vocab\_len}!bittensor.utils.tokenizer\_utils@{bittensor.utils.tokenizer\_utils}}
\doxysubsubsection{\texorpdfstring{set\_vocab\_len()}{set\_vocab\_len()}}
{\footnotesize\ttfamily def bittensor.\+utils.\+tokenizer\+\_\+utils.\+set\+\_\+vocab\+\_\+len (\begin{DoxyParamCaption}\item[{Pre\+Trained\+Tokenizer\+Base}]{tokenizer }\end{DoxyParamCaption})}

\begin{DoxyVerb}Sets the tokenizer.vocab_len if unset, to store the real vocabulary size according to the vocab or encoder.
    Args:
        tokenizer (:obj:`PreTrainedTokenizerBase`, `required`):
            Tokenizer to set vocab_len for.
    Returns:\end{DoxyVerb}
 \mbox{\Hypertarget{namespacebittensor_1_1utils_1_1tokenizer__utils_a558123e572c10af14c002993c9b04f36}\label{namespacebittensor_1_1utils_1_1tokenizer__utils_a558123e572c10af14c002993c9b04f36}} 
\index{bittensor.utils.tokenizer\_utils@{bittensor.utils.tokenizer\_utils}!set\_whitespace\_preserving@{set\_whitespace\_preserving}}
\index{set\_whitespace\_preserving@{set\_whitespace\_preserving}!bittensor.utils.tokenizer\_utils@{bittensor.utils.tokenizer\_utils}}
\doxysubsubsection{\texorpdfstring{set\_whitespace\_preserving()}{set\_whitespace\_preserving()}}
{\footnotesize\ttfamily def bittensor.\+utils.\+tokenizer\+\_\+utils.\+set\+\_\+whitespace\+\_\+preserving (\begin{DoxyParamCaption}\item[{Pre\+Trained\+Tokenizer\+Base}]{tokenizer }\end{DoxyParamCaption})}

\begin{DoxyVerb}Sets the tokenizer.whitespace_preserving if unset, indicates if tokenizer preserves whitespace like GPT-style,
or not like BERT-style.
    Args:
        tokenizer (:obj:`PreTrainedTokenizerBase`, `required`):
            Tokenizer to set vocab_len for.
    Returns:\end{DoxyVerb}
 \mbox{\Hypertarget{namespacebittensor_1_1utils_1_1tokenizer__utils_af3a7513c8021247073b9f0f68d3e751a}\label{namespacebittensor_1_1utils_1_1tokenizer__utils_af3a7513c8021247073b9f0f68d3e751a}} 
\index{bittensor.utils.tokenizer\_utils@{bittensor.utils.tokenizer\_utils}!split\_probs@{split\_probs}}
\index{split\_probs@{split\_probs}!bittensor.utils.tokenizer\_utils@{bittensor.utils.tokenizer\_utils}}
\doxysubsubsection{\texorpdfstring{split\_probs()}{split\_probs()}}
{\footnotesize\ttfamily  torch.\+Float\+Tensor bittensor.\+utils.\+tokenizer\+\_\+utils.\+split\+\_\+probs (\begin{DoxyParamCaption}\item[{torch.\+Float\+Tensor}]{probs,  }\item[{List\mbox{[}Dict\mbox{[}str, torch.\+Tensor\mbox{]}\mbox{]}}]{split\+\_\+map }\end{DoxyParamCaption})}

\begin{DoxyVerb}Split a given probability distribution over a tokenizer vocabulary, given a split_map
of mappings from original tokens to target tokens at each depth of the split.
Args:
probs (:obj:`torch.FloatTensor`, `required`):
[vocab_size] Input probability distribution over a tokenizer vocabulary.
split_map (:obj:`List[Dict[str, torch.Tensor]]`, `required`):
A split_map of mappings from original tokens to target tokens at each depth of the split.

Returns:
new_probs (:obj:`torch.FloatTensor`, `required`):
[splits, vocab_size] A new tensor with resultant probability distribution at each index
of the first dim, representing corresponding split depth.
\end{DoxyVerb}
 \mbox{\Hypertarget{namespacebittensor_1_1utils_1_1tokenizer__utils_a619c2a8870fee58a3993722ad5e2d7bb}\label{namespacebittensor_1_1utils_1_1tokenizer__utils_a619c2a8870fee58a3993722ad5e2d7bb}} 
\index{bittensor.utils.tokenizer\_utils@{bittensor.utils.tokenizer\_utils}!topk\_token\_phrases@{topk\_token\_phrases}}
\index{topk\_token\_phrases@{topk\_token\_phrases}!bittensor.utils.tokenizer\_utils@{bittensor.utils.tokenizer\_utils}}
\doxysubsubsection{\texorpdfstring{topk\_token\_phrases()}{topk\_token\_phrases()}}
{\footnotesize\ttfamily  torch.\+Tensor bittensor.\+utils.\+tokenizer\+\_\+utils.\+topk\+\_\+token\+\_\+phrases (\begin{DoxyParamCaption}\item[{torch.\+Tensor}]{logits,  }\item[{Pre\+Trained\+Tokenizer\+Base}]{tokenizer,  }\item[{int}]{topk,  }\item[{int }]{ignore\+\_\+index = {\ttfamily -\/100} }\end{DoxyParamCaption})}

\begin{DoxyVerb}Select topk tokenizer logits/phrases and include std_token_phrases counterparts (std_tokenization of token text)
in topk_tensor output of shape [batch_size, (topk + 1), max_len], where max len of all phrase lists
(with prob in front) is max_{b,k}(len([prob_k, tok_0_k, tok_1_k, ...])).
The output topk_tensor also includes a floor_prob for each batch item. The floor probability is the
mean probability of token phrases not captured in topk, required since the tokenizer vocab_size may
not be known to the receiver.
Requires prep_tokenizer(tokenizer, std_tokenizer) to set_std_token_phrases first, to make
std_token_phrases available here.
    Args:
        logits (:obj:`torch.Tensor`, `required`):
            [batch_size, vocab_size] Input source logits for last token over a source tokenizer vocabulary.
        tokenizer (:obj:`PreTrainedTokenizerBase`, `required`):
            Source tokenizer (usually server tokenizer)
        topk (:obj:`int`, `required`):
            Amount of top phrases to expect (to check for mismatch)
        ignore_index (:obj:`int`, `optional`):
            Padding value to use for unfilled token positions in a shorter token phrase.

    Returns:
        topk_tensor (:obj:`torch.Tensor`, `required`):
            [batch_size, (topk + 1), max_len] tensor includes topk token probabilities (prob_k) + floor_prob
            in first column with gradients attached, with std_tokens in remaining columns with ignore_index padding.
            Content structure:
            [[[prob_k=0_b=0, tok_0_k=0_b=0, tok_1_k=0_b=0, ..., ignore_index?],
              [prob_k=1_b=0, tok_0_k=1_b=0, tok_1_k=1_b=0, ..., ignore_index?],
              [...],
              [prob_floor_b=0, ignore_index, ..., ignore_index]],
             [[prob_k=0_b=1, tok_0_k=0_b=1, tok_1_k=0_b=1, ..., ignore_index?],
              [prob_k=1_b=1, tok_0_k=1_b=1, tok_1_k=1_b=1, ..., ignore_index?],
              [...],
              [prob_floor_b=1, ignore_index, ..., ignore_index]],
             [...]]
\end{DoxyVerb}
 \mbox{\Hypertarget{namespacebittensor_1_1utils_1_1tokenizer__utils_a66ebcf1b07d1ee0f0ba07130967a40cc}\label{namespacebittensor_1_1utils_1_1tokenizer__utils_a66ebcf1b07d1ee0f0ba07130967a40cc}} 
\index{bittensor.utils.tokenizer\_utils@{bittensor.utils.tokenizer\_utils}!topk\_tokens\_to\_vocab\_size@{topk\_tokens\_to\_vocab\_size}}
\index{topk\_tokens\_to\_vocab\_size@{topk\_tokens\_to\_vocab\_size}!bittensor.utils.tokenizer\_utils@{bittensor.utils.tokenizer\_utils}}
\doxysubsubsection{\texorpdfstring{topk\_tokens\_to\_vocab\_size()}{topk\_tokens\_to\_vocab\_size()}}
{\footnotesize\ttfamily  torch.\+Tensor bittensor.\+utils.\+tokenizer\+\_\+utils.\+topk\+\_\+tokens\+\_\+to\+\_\+vocab\+\_\+size (\begin{DoxyParamCaption}\item[{torch.\+Tensor}]{topk\+\_\+tensor,  }\item[{int}]{vocab\+\_\+size\+\_\+std,  }\item[{int }]{vocab\+\_\+size\+\_\+min = {\ttfamily 50257} }\end{DoxyParamCaption})}

\begin{DoxyVerb}Convert topk_tokens first token probabilities into a standard logits tensor shape [batch_size, vocab_size_std].
Args:
topk_tensor (:obj:`torch.Tensor`, `required`):
    [batch_size, (topk + 1), max_len] tensor includes topk token probabilities (prob_k) + floor_prob
    in first column with gradients attached, with std_tokens in remaining columns with ignore_index padding.
    Content structure:
    [[[prob_k=0_b=0, tok_0_k=0_b=0, tok_1_k=0_b=0, ..., ignore_index?],
      [prob_k=1_b=0, tok_0_k=1_b=0, tok_1_k=1_b=0, ..., ignore_index?],
      [...],
      [prob_floor_b=0, ignore_index, ..., ignore_index]],
     [[prob_k=0_b=1, tok_0_k=0_b=1, tok_1_k=0_b=1, ..., ignore_index?],
      [prob_k=1_b=1, tok_0_k=1_b=1, tok_1_k=1_b=1, ..., ignore_index?],
      [...],
      [prob_floor_b=1, ignore_index, ..., ignore_index]],
     [...]]
vocab_size_std (:obj:`int`, `optional`):
    Standard tokenizer vocab_size for forming logits.
vocab_size_min (:obj:`int`, `optional`):
    Minimum server vocab_size expected, should set to nominal 50257,
    used to prevent the floor_probs from being too large.
Returns:
logits (:obj:`torch.Tensor`, `required`):
    [batch_size, vocab_size_std] Standard logits.
\end{DoxyVerb}
 \mbox{\Hypertarget{namespacebittensor_1_1utils_1_1tokenizer__utils_a7c296be466fd118c374bf1480d26904b}\label{namespacebittensor_1_1utils_1_1tokenizer__utils_a7c296be466fd118c374bf1480d26904b}} 
\index{bittensor.utils.tokenizer\_utils@{bittensor.utils.tokenizer\_utils}!translate\_logits\_to\_probs\_std@{translate\_logits\_to\_probs\_std}}
\index{translate\_logits\_to\_probs\_std@{translate\_logits\_to\_probs\_std}!bittensor.utils.tokenizer\_utils@{bittensor.utils.tokenizer\_utils}}
\doxysubsubsection{\texorpdfstring{translate\_logits\_to\_probs\_std()}{translate\_logits\_to\_probs\_std()}}
{\footnotesize\ttfamily  torch.\+Float\+Tensor bittensor.\+utils.\+tokenizer\+\_\+utils.\+translate\+\_\+logits\+\_\+to\+\_\+probs\+\_\+std (\begin{DoxyParamCaption}\item[{torch.\+Float\+Tensor}]{logits,  }\item[{List\mbox{[}List\mbox{[}tuple\mbox{]}\mbox{]}}]{offset\+\_\+mapping,  }\item[{List\mbox{[}List\mbox{[}tuple\mbox{]}\mbox{]}}]{offset\+\_\+mapping\+\_\+std,  }\item[{Pre\+Trained\+Tokenizer\+Base}]{tokenizer,  }\item[{Pre\+Trained\+Tokenizer\+Base}]{std\+\_\+tokenizer,  }\item[{Dict\mbox{[}tuple, List\mbox{[}Dict\mbox{[}str, torch.\+Tensor\mbox{]}\mbox{]}\mbox{]}}]{split\+\_\+map\+\_\+cache,  }\item[{Dict\mbox{[}str, Any\mbox{]}}]{to\+\_\+translation\+\_\+map,  }\item[{Dict\mbox{[}str, Any\mbox{]}}]{from\+\_\+translation\+\_\+map,  }\item[{torch.\+Long\+Tensor}]{tokens,  }\item[{torch.\+Long\+Tensor}]{tokens\+\_\+std,  }\item[{bool }]{skip\+\_\+equivalent = {\ttfamily True} }\end{DoxyParamCaption})}

\begin{DoxyVerb}Translates source token logit scores to probability distributions over the standard tokenizer.
    Args:
        logits (:obj:`torch.FloatTensor`, `required`):
            [batch_size, sequence_len, vocab_size] Input source logits over a source tokenizer vocabulary.
        offset_mapping (:obj:`List[List[tuple]]`, `required`):
            Batch of tokenizer offset mappings
            [[(left_0, right_0), (left_1, right_1), ...], ...].
        offset_mapping_std (:obj:`List[List[tuple]]`, `required`):
            Batch of standard tokenizer offset mappings
            [[(left_0, right_0), (left_1, right_1), ...], ...].
        tokenizer (:obj:`PreTrainedTokenizerBase`, `required`):
            Source tokenizer.
        std_tokenizer (:obj:`PreTrainedTokenizerBase`, `required`):
            Standard/target tokenizer.
        split_map_cache (:obj:`Dict[tuple, List[Dict[str, torch.Tensor]]]`, `required`):
            A dictionary of depths keying split_maps of mappings from original tokens to
            target tokens at each depth of the split. Adds split_maps to cache for faster future recall.
        tokens (:obj:`torch.LongTensor`, `required`):
            [batch_size, sequence_len] A sequence of tokens produced by the source tokenizer.
        tokens_std (:obj:`torch.LongTensor`, `required`):
            [batch_size, std_sequence_len] A sequence of tokens produced by the standard tokenizer.
        to_translation_map (:obj:`Dict[str, Any]`, `required`):
            Maps for each observed length, a source token to a token sequence of that length,
            with source index to target indices.
        from_translation_map (:obj:`Dict[str, Any]`, `required`):
            Maps for each observed length, a source token to a token sequence of that length,
            from target index to source indices.
        skip_equivalent (:obj:`bool`, `optional`):
            Skips translation if tokenizer and std_tokenizer are equivalent.

    Returns:
        probs_std (:obj:`torch.FloatTensor`, `required`):
            [batch_size, std_sequence_len, std_vocab_size] Output probability distribution over the
            standard tokenizer vocabulary.
\end{DoxyVerb}
 \mbox{\Hypertarget{namespacebittensor_1_1utils_1_1tokenizer__utils_aacc9d76a7e49b18c20a5b44beafb8306}\label{namespacebittensor_1_1utils_1_1tokenizer__utils_aacc9d76a7e49b18c20a5b44beafb8306}} 
\index{bittensor.utils.tokenizer\_utils@{bittensor.utils.tokenizer\_utils}!translate\_many\_to\_one@{translate\_many\_to\_one}}
\index{translate\_many\_to\_one@{translate\_many\_to\_one}!bittensor.utils.tokenizer\_utils@{bittensor.utils.tokenizer\_utils}}
\doxysubsubsection{\texorpdfstring{translate\_many\_to\_one()}{translate\_many\_to\_one()}}
{\footnotesize\ttfamily  None bittensor.\+utils.\+tokenizer\+\_\+utils.\+translate\+\_\+many\+\_\+to\+\_\+one (\begin{DoxyParamCaption}\item[{torch.\+Float\+Tensor}]{probs\+\_\+from,  }\item[{torch.\+Float\+Tensor}]{probs\+\_\+to,  }\item[{Dict\mbox{[}str, Any\mbox{]}}]{translation\+\_\+map }\end{DoxyParamCaption})}

\begin{DoxyVerb}    Translate a sequence of token probability distributions from a source tokenization to a
    single token probability distribution over a target tokenization.
        Args:
            probs_from (:obj:`torch.FloatTensor`, `required`):
                [many, vocab_size] Input probability distributions over a from-tokenizer vocabulary.
            probs_to (:obj:`torch.FloatTensor`, `required`):
                [vocab_size] Output probability distribution over a to-tokenizer vocabulary.
            translation_map (:obj:`Dict[str, Any]`, `required`):
                Maps for each observed length, a source token to a token sequence of that length,
                with source index to target indices.

        Returns:\end{DoxyVerb}
 \mbox{\Hypertarget{namespacebittensor_1_1utils_1_1tokenizer__utils_a127face1324f46e879f92ee881771d90}\label{namespacebittensor_1_1utils_1_1tokenizer__utils_a127face1324f46e879f92ee881771d90}} 
\index{bittensor.utils.tokenizer\_utils@{bittensor.utils.tokenizer\_utils}!translate\_one\_to\_many@{translate\_one\_to\_many}}
\index{translate\_one\_to\_many@{translate\_one\_to\_many}!bittensor.utils.tokenizer\_utils@{bittensor.utils.tokenizer\_utils}}
\doxysubsubsection{\texorpdfstring{translate\_one\_to\_many()}{translate\_one\_to\_many()}}
{\footnotesize\ttfamily  None bittensor.\+utils.\+tokenizer\+\_\+utils.\+translate\+\_\+one\+\_\+to\+\_\+many (\begin{DoxyParamCaption}\item[{torch.\+Float\+Tensor}]{probs\+\_\+from,  }\item[{torch.\+Float\+Tensor}]{probs\+\_\+to,  }\item[{Dict\mbox{[}str, Any\mbox{]}}]{translation\+\_\+map }\end{DoxyParamCaption})}

\begin{DoxyVerb}Translate a single token probability distribution from a source tokenization to a
sequence of probability distributions over a target tokenization.
Args:
    probs_from (:obj:`torch.FloatTensor`, `required`):
        [vocab_size] Input probability distribution over a from-tokenizer vocabulary.
    probs_to (:obj:`torch.FloatTensor`, `required`):
        [many, vocab_size] Output probability distributions over a to-tokenizer vocabulary.
    translation_map (:obj:`Dict[str, Any]`, `required`):
        Maps for each observed length, a source token to a token sequence of that length,
        with source index to target indices.

Returns:\end{DoxyVerb}
 \mbox{\Hypertarget{namespacebittensor_1_1utils_1_1tokenizer__utils_a6717d83c925189f86590b5ab3d88df17}\label{namespacebittensor_1_1utils_1_1tokenizer__utils_a6717d83c925189f86590b5ab3d88df17}} 
\index{bittensor.utils.tokenizer\_utils@{bittensor.utils.tokenizer\_utils}!translate\_special\_token\_text@{translate\_special\_token\_text}}
\index{translate\_special\_token\_text@{translate\_special\_token\_text}!bittensor.utils.tokenizer\_utils@{bittensor.utils.tokenizer\_utils}}
\doxysubsubsection{\texorpdfstring{translate\_special\_token\_text()}{translate\_special\_token\_text()}}
{\footnotesize\ttfamily  Tuple\mbox{[}List\mbox{[}str\mbox{]},                                                                                  List\mbox{[}List\mbox{[}List\mbox{[}int\mbox{]}\mbox{]}\mbox{]},                                                                                  List\mbox{[}List\mbox{[}List\mbox{[}int\mbox{]}\mbox{]}\mbox{]},                                                                                  List\mbox{[}List\mbox{[}List\mbox{[}Any\mbox{]}\mbox{]}\mbox{]}\mbox{]} bittensor.\+utils.\+tokenizer\+\_\+utils.\+translate\+\_\+special\+\_\+token\+\_\+text (\begin{DoxyParamCaption}\item[{List\mbox{[}str\mbox{]}}]{text\+\_\+batch,  }\item[{Pre\+Trained\+Tokenizer\+Base}]{from\+\_\+tokenizer,  }\item[{Pre\+Trained\+Tokenizer\+Base}]{to\+\_\+tokenizer }\end{DoxyParamCaption})}

\begin{DoxyVerb}Translates special_token signifier text in from_tokenizer to to_tokenizer special_token text, for
a given text_batch. Resulting to_text_batch can then be to_tokenized where special_tokens should
map to its single corresponding token, despite signifier text difference compared to from_tokenizer.
    Args:
        text_batch (:obj:`List[str]`, `required`):
            List of strings to translate special tokens for.
        from_tokenizer (:obj:`PreTrainedTokenizerBase`, `required`):
            From tokenizer.
        to_tokenizer (:obj:`PreTrainedTokenizerBase`, `required`):
            To tokenizer.

    Returns:
        to_text_batch (:obj:`List[str]`, `required`):
            List of strings where special text has been replaced.
        from_offsets_batch (:obj:`List[List[List[int]]]`, `required`):
            Batch of tokenizer offset mappings selecting replacement tuples in from_tokenizer text
                [[(left_0, right_0), (left_1, right_1), ...], ...].
        to_offsets_batch (:obj:`List[List[List[int]]]`, `required`):
            Batch of tokenizer offset mappings selecting replacement tuples in to_tokenizer text
                [[(left_0, right_0), (left_1, right_1), ...], ...].
        pad_offsets_batch (:obj:`List[List[List[Any]]]`, `required`):
            Batch of offset paddings associated with each replacement tuple
                [[(left_pad_0, right_pad_0), (left_pad_1, right_pad_1), ...], ...].
\end{DoxyVerb}
 \mbox{\Hypertarget{namespacebittensor_1_1utils_1_1tokenizer__utils_ac2ac0a649cfc204c56ad13913288a4f4}\label{namespacebittensor_1_1utils_1_1tokenizer__utils_ac2ac0a649cfc204c56ad13913288a4f4}} 
\index{bittensor.utils.tokenizer\_utils@{bittensor.utils.tokenizer\_utils}!translate\_tokenizer\_probs@{translate\_tokenizer\_probs}}
\index{translate\_tokenizer\_probs@{translate\_tokenizer\_probs}!bittensor.utils.tokenizer\_utils@{bittensor.utils.tokenizer\_utils}}
\doxysubsubsection{\texorpdfstring{translate\_tokenizer\_probs()}{translate\_tokenizer\_probs()}}
{\footnotesize\ttfamily  None bittensor.\+utils.\+tokenizer\+\_\+utils.\+translate\+\_\+tokenizer\+\_\+probs (\begin{DoxyParamCaption}\item[{torch.\+Float\+Tensor}]{probs,  }\item[{torch.\+Float\+Tensor}]{probs\+\_\+std,  }\item[{List\mbox{[}tuple\mbox{]}}]{offset\+\_\+mapping,  }\item[{List\mbox{[}tuple\mbox{]}}]{offset\+\_\+mapping\+\_\+std,  }\item[{Pre\+Trained\+Tokenizer\+Base}]{tokenizer,  }\item[{Pre\+Trained\+Tokenizer\+Base}]{std\+\_\+tokenizer,  }\item[{Dict\mbox{[}tuple, List\mbox{[}Dict\mbox{[}str, torch.\+Tensor\mbox{]}\mbox{]}\mbox{]}}]{split\+\_\+map\+\_\+cache,  }\item[{Dict\mbox{[}str, Any\mbox{]}}]{to\+\_\+translation\+\_\+map,  }\item[{Dict\mbox{[}str, Any\mbox{]}}]{from\+\_\+translation\+\_\+map,  }\item[{torch.\+Long\+Tensor}]{tokens,  }\item[{torch.\+Long\+Tensor}]{tokens\+\_\+std }\end{DoxyParamCaption})}

\begin{DoxyVerb}Translates source token probability distributions to target probability distributions, by
aligning segments through source token splits, then greedily performing one-to-one,
one-to-many, many-to-one distribution mappings.
    Args:
        probs (:obj:`torch.FloatTensor`, `required`):
            [sequence_len, vocab_size] Input probability distribution over a source tokenizer vocabulary.
        probs_std (:obj:`torch.FloatTensor`, `required`):
            [std_sequence_len, std_vocab_size] Output probability distribution over a target tokenizer vocabulary.
            Reference that will be written in-place.
        offset_mapping (:obj:`List[tuple]`, `required`):
            Tokenizer offset mappings for a specific sequence [(left_0, right_0), (left_1, right_1), ...].
        offset_mapping_std (:obj:`List[tuple]`, `required`):
            Standard tokenizer offset mappings for a specific sequence [(left_0, right_0), (left_1, right_1), ...]
        tokenizer (:obj:`PreTrainedTokenizerBase`, `required`):
            Source tokenizer.
        std_tokenizer (:obj:`PreTrainedTokenizerBase`, `required`):
            Standard/target tokenizer.
        split_map_cache (:obj:`Dict[tuple, List[Dict[str, torch.Tensor]]]`, `required`):
            A dictionary of depths keying split_maps of mappings from original tokens to
            target tokens at each depth of the split. Adds split_maps to cache for faster future recall.
        tokens (:obj:`torch.LongTensor`, `required`):
            [sequence_len] A sequence of tokens produced by the source tokenizer.
        tokens_std (:obj:`torch.LongTensor`, `required`):
            [std_sequence_len] A sequence of tokens produced by the standard tokenizer.
        to_translation_map (:obj:`Dict[str, Any]`, `required`):
            Maps for each observed length, a source token to a token sequence of that length,
            with source index to target indices.
        from_translation_map (:obj:`Dict[str, Any]`, `required`):
            Maps for each observed length, a source token to a token sequence of that length,
            from target index to source indices.

    Returns:\end{DoxyVerb}
 \mbox{\Hypertarget{namespacebittensor_1_1utils_1_1tokenizer__utils_a952b31d97e6588cd4166ee2ef319a1f0}\label{namespacebittensor_1_1utils_1_1tokenizer__utils_a952b31d97e6588cd4166ee2ef319a1f0}} 
\index{bittensor.utils.tokenizer\_utils@{bittensor.utils.tokenizer\_utils}!unravel\_topk\_token\_phrases@{unravel\_topk\_token\_phrases}}
\index{unravel\_topk\_token\_phrases@{unravel\_topk\_token\_phrases}!bittensor.utils.tokenizer\_utils@{bittensor.utils.tokenizer\_utils}}
\doxysubsubsection{\texorpdfstring{unravel\_topk\_token\_phrases()}{unravel\_topk\_token\_phrases()}}
{\footnotesize\ttfamily  torch.\+Tensor bittensor.\+utils.\+tokenizer\+\_\+utils.\+unravel\+\_\+topk\+\_\+token\+\_\+phrases (\begin{DoxyParamCaption}\item[{torch.\+Tensor}]{compact\+\_\+topk,  }\item[{int}]{topk,  }\item[{int }]{ignore\+\_\+index = {\ttfamily -\/100} }\end{DoxyParamCaption})}

\begin{DoxyVerb}Unravel topk token phrases input_tensor from 1-D to [batch_size, (topk + 1), max_len] topk_tensor, which
includes topk token probabilities (prob_k) + floor_prob in first column with gradients attached, with
std_tokens in remaining columns with ignore_index padding.
    Args:
        compact_topk (:obj:`torch.Tensor`, `required`):
            [sum_b(sum_k(len(phrase_k) + 1)_b)] Compacted 1-D tensor >= batch_size * (2 * topk + 1),
            since 2 * topk + 1: topk x [probability, token sequence (at least one token)] +
            floor probability (rest).
            Content structure:
                [prob_k=0_b=0, tok_0_k=0_b=0, tok_1_k=0_b=0, ..., prob_k=1_b=0, tok_0_k=1_b=0, ..., prob_floor_b=0,
                 prob_k=0_b=1, tok_0_k=0_b=1, tok_1_k=0_b=1, ..., prob_k=1_b=1, tok_0_k=1_b=1, ..., prob_floor_b=1,
                 ...]
        topk (:obj:`int`, `required`):
            Amount of top phrases to expect (to check for mismatch)
        ignore_index (:obj:`int`, `optional`):
            Padding value to use for unfilled token positions in a shorter token phrase.
    Returns:
        topk_tensor (:obj:`torch.Tensor`, `required`):
            [batch_size, (topk + 1), max_len] tensor includes topk token probabilities (prob_k) + floor_prob
            in first column with gradients attached, with std_tokens in remaining columns with ignore_index padding.
            Content structure:
            [[[prob_k=0_b=0, tok_0_k=0_b=0, tok_1_k=0_b=0, ..., ignore_index?],
              [prob_k=1_b=0, tok_0_k=1_b=0, tok_1_k=1_b=0, ..., ignore_index?],
              [...],
              [prob_floor_b=0, ignore_index, ..., ignore_index]],
             [[prob_k=0_b=1, tok_0_k=0_b=1, tok_1_k=0_b=1, ..., ignore_index?],
              [prob_k=1_b=1, tok_0_k=1_b=1, tok_1_k=1_b=1, ..., ignore_index?],
              [...],
              [prob_floor_b=1, ignore_index, ..., ignore_index]],
             [...]]
\end{DoxyVerb}
 