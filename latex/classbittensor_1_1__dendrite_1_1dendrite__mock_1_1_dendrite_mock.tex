\hypertarget{classbittensor_1_1__dendrite_1_1dendrite__mock_1_1_dendrite_mock}{}\doxysection{bittensor.\+\_\+dendrite.\+dendrite\+\_\+mock.\+Dendrite\+Mock Class Reference}
\label{classbittensor_1_1__dendrite_1_1dendrite__mock_1_1_dendrite_mock}\index{bittensor.\_dendrite.dendrite\_mock.DendriteMock@{bittensor.\_dendrite.dendrite\_mock.DendriteMock}}
Inheritance diagram for bittensor.\+\_\+dendrite.\+dendrite\+\_\+mock.\+Dendrite\+Mock\+:\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[height=2.000000cm]{classbittensor_1_1__dendrite_1_1dendrite__mock_1_1_dendrite_mock}
\end{center}
\end{figure}
\doxysubsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
def \mbox{\hyperlink{classbittensor_1_1__dendrite_1_1dendrite__mock_1_1_dendrite_mock_a81ffefc2213c7563e28f2e2de997f5d5}{\+\_\+\+\_\+init\+\_\+\+\_\+}} (self, \textquotesingle{}\mbox{\hyperlink{classbittensor_1_1__config_1_1config__impl_1_1_config}{bittensor.\+Config}}\textquotesingle{} \mbox{\hyperlink{classbittensor_1_1__config_1_1config}{config}}, \textquotesingle{}\mbox{\hyperlink{classbittensor_1_1__wallet_1_1wallet__impl_1_1_wallet}{bittensor.\+Wallet}}\textquotesingle{} \mbox{\hyperlink{classbittensor_1_1__wallet_1_1wallet}{wallet}})
\item 
\mbox{\Hypertarget{classbittensor_1_1__dendrite_1_1dendrite__mock_1_1_dendrite_mock_ae632c778cd7e16859863ceeb57b1b96f}\label{classbittensor_1_1__dendrite_1_1dendrite__mock_1_1_dendrite_mock_ae632c778cd7e16859863ceeb57b1b96f}} 
def {\bfseries \+\_\+\+\_\+str\+\_\+\+\_\+} (self)
\item 
\mbox{\Hypertarget{classbittensor_1_1__dendrite_1_1dendrite__mock_1_1_dendrite_mock_ad557a24a1819db6c705133390b0c115c}\label{classbittensor_1_1__dendrite_1_1dendrite__mock_1_1_dendrite_mock_ad557a24a1819db6c705133390b0c115c}} 
def {\bfseries \+\_\+\+\_\+repr\+\_\+\+\_\+} (self)
\item 
\mbox{\Hypertarget{classbittensor_1_1__dendrite_1_1dendrite__mock_1_1_dendrite_mock_a808509a1c1618ec006377e3131becf1a}\label{classbittensor_1_1__dendrite_1_1dendrite__mock_1_1_dendrite_mock_a808509a1c1618ec006377e3131becf1a}} 
def {\bfseries \+\_\+\+\_\+del\+\_\+\+\_\+} (self)
\item 
Tuple\mbox{[}Union\mbox{[}List\mbox{[}torch.\+Float\+Tensor\mbox{]}, torch.\+Float\+Tensor\mbox{]}, torch.\+Long\+Tensor, torch.\+Float\+Tensor\mbox{]} \mbox{\hyperlink{classbittensor_1_1__dendrite_1_1dendrite__mock_1_1_dendrite_mock_aba4861e1bd42622eced10ec450fddfb5}{forward\+\_\+image}} (self, Union\mbox{[}List\mbox{[}\textquotesingle{}\mbox{\hyperlink{classbittensor_1_1__endpoint_1_1endpoint__impl_1_1_endpoint}{bittensor.\+Endpoint}}\textquotesingle{}\mbox{]}, \textquotesingle{}\mbox{\hyperlink{classbittensor_1_1__endpoint_1_1endpoint__impl_1_1_endpoint}{bittensor.\+Endpoint}}\textquotesingle{}\mbox{]} endpoints, List\mbox{[}torch.\+Float\+Tensor\mbox{]} inputs, int timeout=None, bool requires\+\_\+grad=None)
\item 
Tuple\mbox{[}Union\mbox{[}List\mbox{[}torch.\+Float\+Tensor\mbox{]}, torch.\+Float\+Tensor\mbox{]}, torch.\+Long\+Tensor, torch.\+Float\+Tensor\mbox{]} \mbox{\hyperlink{classbittensor_1_1__dendrite_1_1dendrite__mock_1_1_dendrite_mock_a596804548b4ab12580fa1ce97fe36e54}{forward\+\_\+tensor}} (self, Union\mbox{[}List\mbox{[}\textquotesingle{}\mbox{\hyperlink{classbittensor_1_1__endpoint_1_1endpoint__impl_1_1_endpoint}{bittensor.\+Endpoint}}\textquotesingle{}\mbox{]}, \textquotesingle{}\mbox{\hyperlink{classbittensor_1_1__endpoint_1_1endpoint__impl_1_1_endpoint}{bittensor.\+Endpoint}}\textquotesingle{}\mbox{]} endpoints, List\mbox{[}torch.\+Float\+Tensor\mbox{]} inputs, int timeout=None, bool requires\+\_\+grad=None)
\item 
Tuple\mbox{[}Union\mbox{[}List\mbox{[}torch.\+Float\+Tensor\mbox{]}, torch.\+Float\+Tensor\mbox{]}, torch.\+Long\+Tensor, torch.\+Float\+Tensor\mbox{]} \mbox{\hyperlink{classbittensor_1_1__dendrite_1_1dendrite__mock_1_1_dendrite_mock_a05b1862d3fcc5a34924ff9d89424585d}{forward\+\_\+text}} (self, Union\mbox{[}torch.\+Long\+Tensor, List\mbox{[}torch.\+Long\+Tensor\mbox{]}, List\mbox{[}\textquotesingle{}\mbox{\hyperlink{classbittensor_1_1__endpoint_1_1endpoint__impl_1_1_endpoint}{bittensor.\+Endpoint}}\textquotesingle{}\mbox{]}, \textquotesingle{}\mbox{\hyperlink{classbittensor_1_1__endpoint_1_1endpoint__impl_1_1_endpoint}{bittensor.\+Endpoint}}\textquotesingle{}\mbox{]} endpoints, Union\mbox{[}str, List\mbox{[}str\mbox{]}, List\mbox{[}torch.\+Long\+Tensor\mbox{]}, torch.\+Long\+Tensor\mbox{]} inputs, int timeout=None, bool requires\+\_\+grad=None)
\item 
def \mbox{\hyperlink{classbittensor_1_1__dendrite_1_1dendrite__mock_1_1_dendrite_mock_ae078e5bf68d851035d9c4e7a125f3611}{update\+\_\+stats}} (self, endpoints, requests, responses, return\+\_\+ops, query\+\_\+times)
\item 
def \mbox{\hyperlink{classbittensor_1_1__dendrite_1_1dendrite__mock_1_1_dendrite_mock_a7308a89e8472c87b19e6b1955d4b6a2f}{to\+\_\+dataframe}} (self, \mbox{\hyperlink{classbittensor_1_1__metagraph_1_1metagraph}{metagraph}})
\item 
def \mbox{\hyperlink{classbittensor_1_1__dendrite_1_1dendrite__mock_1_1_dendrite_mock_a19bd830edb5d9b7ec5224fb8fe613f63}{to\+\_\+wandb}} (self)
\end{DoxyCompactItemize}
\doxysubsection*{Static Public Member Functions}
\begin{DoxyCompactItemize}
\item 
Tuple\mbox{[}torch.\+Tensor,...\mbox{]} \mbox{\hyperlink{classbittensor_1_1__dendrite_1_1dendrite__mock_1_1_dendrite_mock_ae81c0740aa0705e26e7aad62de72e2f3}{forward}} (ctx, \textquotesingle{}\mbox{\hyperlink{classbittensor_1_1__dendrite_1_1dendrite__impl_1_1_dendrite}{bittensor.\+Dendrite}}\textquotesingle{} \mbox{\hyperlink{classbittensor_1_1__dendrite_1_1dendrite}{dendrite}}, torch.\+Tensor dummy, List\mbox{[}\textquotesingle{}\mbox{\hyperlink{classbittensor_1_1__endpoint_1_1endpoint__impl_1_1_endpoint}{bittensor.\+Endpoint}}\textquotesingle{}\mbox{]} endpoints, bittensor.\+proto.\+Modality modality, int timeout, bool requires\+\_\+grad, $\ast$torch.\+Tensor inputs)
\item 
Tuple\mbox{[}Optional\mbox{[}torch.\+Tensor\mbox{]},...\mbox{]} \mbox{\hyperlink{classbittensor_1_1__dendrite_1_1dendrite__mock_1_1_dendrite_mock_ab918018e0982ed5cc000b773a158155b}{backward}} (ctx, torch.\+Float\+Tensor unused\+\_\+code\+\_\+grads, torch.\+Float\+Tensor unused\+\_\+time\+\_\+grads, $\ast$torch.\+Float\+Tensor output\+\_\+grads)
\end{DoxyCompactItemize}
\doxysubsection*{Public Attributes}
\begin{DoxyCompactItemize}
\item 
\mbox{\Hypertarget{classbittensor_1_1__dendrite_1_1dendrite__mock_1_1_dendrite_mock_a9f892c0ae3ad0c4215c37e32c06fc9c6}\label{classbittensor_1_1__dendrite_1_1dendrite__mock_1_1_dendrite_mock_a9f892c0ae3ad0c4215c37e32c06fc9c6}} 
{\bfseries config}
\item 
\mbox{\Hypertarget{classbittensor_1_1__dendrite_1_1dendrite__mock_1_1_dendrite_mock_a319e9d2b826f7d5e6c1d6eeace3181fb}\label{classbittensor_1_1__dendrite_1_1dendrite__mock_1_1_dendrite_mock_a319e9d2b826f7d5e6c1d6eeace3181fb}} 
{\bfseries wallet}
\item 
\mbox{\Hypertarget{classbittensor_1_1__dendrite_1_1dendrite__mock_1_1_dendrite_mock_aebb0242acf69b207f575854795c00403}\label{classbittensor_1_1__dendrite_1_1dendrite__mock_1_1_dendrite_mock_aebb0242acf69b207f575854795c00403}} 
{\bfseries stats}
\end{DoxyCompactItemize}
\doxysubsection*{Protected Member Functions}
\begin{DoxyCompactItemize}
\item 
Tuple\mbox{[}List\mbox{[}torch.\+Tensor\mbox{]}, torch.\+Long\+Tensor, torch.\+Float\+Tensor\mbox{]} \mbox{\hyperlink{classbittensor_1_1__dendrite_1_1dendrite__mock_1_1_dendrite_mock_ab55cc43ecfbdfbe6c12477c357e1f45a}{\+\_\+forward}} (self, List\mbox{[}\textquotesingle{}\mbox{\hyperlink{classbittensor_1_1__endpoint_1_1endpoint__impl_1_1_endpoint}{bittensor.\+Endpoint}}\textquotesingle{}\mbox{]} endpoints, List\mbox{[}torch.\+Tensor\mbox{]} inputs, bittensor.\+proto.\+Modality modality, int timeout=None, bool requires\+\_\+grad=None)
\item 
\mbox{\Hypertarget{classbittensor_1_1__dendrite_1_1dendrite__mock_1_1_dendrite_mock_a769f1dd732c542a449bf729438db3267}\label{classbittensor_1_1__dendrite_1_1dendrite__mock_1_1_dendrite_mock_a769f1dd732c542a449bf729438db3267}} 
def {\bfseries \+\_\+init\+\_\+stats} (self)
\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
\begin{DoxyVerb} Mocked Dendrite returns random results 50% of the time.
\end{DoxyVerb}
 

\doxysubsection{Constructor \& Destructor Documentation}
\mbox{\Hypertarget{classbittensor_1_1__dendrite_1_1dendrite__mock_1_1_dendrite_mock_a81ffefc2213c7563e28f2e2de997f5d5}\label{classbittensor_1_1__dendrite_1_1dendrite__mock_1_1_dendrite_mock_a81ffefc2213c7563e28f2e2de997f5d5}} 
\index{bittensor.\_dendrite.dendrite\_mock.DendriteMock@{bittensor.\_dendrite.dendrite\_mock.DendriteMock}!\_\_init\_\_@{\_\_init\_\_}}
\index{\_\_init\_\_@{\_\_init\_\_}!bittensor.\_dendrite.dendrite\_mock.DendriteMock@{bittensor.\_dendrite.dendrite\_mock.DendriteMock}}
\doxysubsubsection{\texorpdfstring{\_\_init\_\_()}{\_\_init\_\_()}}
{\footnotesize\ttfamily def bittensor.\+\_\+dendrite.\+dendrite\+\_\+mock.\+Dendrite\+Mock.\+\_\+\+\_\+init\+\_\+\+\_\+ (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{\textquotesingle{}\mbox{\hyperlink{classbittensor_1_1__config_1_1config__impl_1_1_config}{bittensor.\+Config}}\textquotesingle{}}]{config,  }\item[{\textquotesingle{}\mbox{\hyperlink{classbittensor_1_1__wallet_1_1wallet__impl_1_1_wallet}{bittensor.\+Wallet}}\textquotesingle{}}]{wallet }\end{DoxyParamCaption})}

\begin{DoxyVerb} Initializes a new Mock Dendrite entry point.\end{DoxyVerb}
 

\doxysubsection{Member Function Documentation}
\mbox{\Hypertarget{classbittensor_1_1__dendrite_1_1dendrite__mock_1_1_dendrite_mock_ab55cc43ecfbdfbe6c12477c357e1f45a}\label{classbittensor_1_1__dendrite_1_1dendrite__mock_1_1_dendrite_mock_ab55cc43ecfbdfbe6c12477c357e1f45a}} 
\index{bittensor.\_dendrite.dendrite\_mock.DendriteMock@{bittensor.\_dendrite.dendrite\_mock.DendriteMock}!\_forward@{\_forward}}
\index{\_forward@{\_forward}!bittensor.\_dendrite.dendrite\_mock.DendriteMock@{bittensor.\_dendrite.dendrite\_mock.DendriteMock}}
\doxysubsubsection{\texorpdfstring{\_forward()}{\_forward()}}
{\footnotesize\ttfamily  Tuple\mbox{[}List\mbox{[}torch.\+Tensor\mbox{]}, torch.\+Long\+Tensor, torch.\+Float\+Tensor\mbox{]} bittensor.\+\_\+dendrite.\+dendrite\+\_\+mock.\+Dendrite\+Mock.\+\_\+forward (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{List\mbox{[}\textquotesingle{}\mbox{\hyperlink{classbittensor_1_1__endpoint_1_1endpoint__impl_1_1_endpoint}{bittensor.\+Endpoint}}\textquotesingle{}\mbox{]}}]{endpoints,  }\item[{List\mbox{[}torch.\+Tensor\mbox{]}}]{inputs,  }\item[{bittensor.\+proto.\+Modality}]{modality,  }\item[{int }]{timeout = {\ttfamily None},  }\item[{bool }]{requires\+\_\+grad = {\ttfamily None} }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb} Internal Forward tensor inputs to a list of neuron endpoints.

    Args:
        endpoints (:obj:`List[bittensor.Endpoint]` of shape :obj:`(num_endpoints)`, `required`):
            List of remote endpoints which match length of inputs. Tensors from inputs are sent forward to these endpoints.

        inputs (:obj:`List[torch.Tensor]` of shape :obj:`(num_endpoints * [shape])`, `required`):
            List of tensors to send to corresponding endpoints. Tensors are of arbitrary type and shape depending on the
            modality.

        modality (:obj:`bittensor.proto.Modality` of shape :obj:`(1)`, `required`):
            Bittensor forward modality type. Enum in [TEXT, IMAGE, TENSOR]

        timeout (int, default = dendrite.timeout, `required`):
            request timeout.

        requires_grad (int, default = dendrite.requires_grad, `optional`):
            If true, the backward pass triggers passing gradients on the wire.

    Returns:
        responses (:obj:`List[torch.FloatTensor]` of shape :obj:`(batch_size, sequence_len, bittensor.__network_dim__)`, `required`):
            Output encodings of inputs produced by the remote endpoints. Non-responses are zeroes of common shape.

        codes (:obj:`List[torch.LongTensor]` of shape :obj:`[num_endpoints]`, `required`):
            dendrite call return codes.

        times (:obj:`torch.FloatTensor` of shape :obj:`[ num_endpoints ]`, `required`):
            times per call.\end{DoxyVerb}
 \mbox{\Hypertarget{classbittensor_1_1__dendrite_1_1dendrite__mock_1_1_dendrite_mock_ab918018e0982ed5cc000b773a158155b}\label{classbittensor_1_1__dendrite_1_1dendrite__mock_1_1_dendrite_mock_ab918018e0982ed5cc000b773a158155b}} 
\index{bittensor.\_dendrite.dendrite\_mock.DendriteMock@{bittensor.\_dendrite.dendrite\_mock.DendriteMock}!backward@{backward}}
\index{backward@{backward}!bittensor.\_dendrite.dendrite\_mock.DendriteMock@{bittensor.\_dendrite.dendrite\_mock.DendriteMock}}
\doxysubsubsection{\texorpdfstring{backward()}{backward()}}
{\footnotesize\ttfamily  Tuple\mbox{[}Optional\mbox{[}torch.\+Tensor\mbox{]}, ...\mbox{]} bittensor.\+\_\+dendrite.\+dendrite\+\_\+mock.\+Dendrite\+Mock.\+backward (\begin{DoxyParamCaption}\item[{}]{ctx,  }\item[{torch.\+Float\+Tensor}]{unused\+\_\+code\+\_\+grads,  }\item[{torch.\+Float\+Tensor}]{unused\+\_\+time\+\_\+grads,  }\item[{$\ast$torch.\+Float\+Tensor     }]{output\+\_\+grads }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [static]}}

\begin{DoxyVerb} Internal autograd-friendly Backward RPC call to a list of neuron endpoints.

    Args:
        ctx: (:obj:`torch.autograd.ctx`, `required`):
            Autograd context, saves state information between forward and backward calls. i.e. inputs for gradient computation.

        unused_code_grads: (:obj:`List[torch.Tensor]` of shape :obj:`(shape)`, `required`):
            Gradients of this function's codes. (Unused)

        unused_time_grads: (:obj:`List[torch.Tensor]` of shape :obj:`(shape)`, `required`):
            Gradients of this function's query times. (Unused)

        grads (:obj:`List[torch.Tensor]` of shape :obj:`(shape)`, `required`):
            Gradients of this function's outputs computed during the loss.backward() call.
    
    Returns:
        DUMMY, None, None, None,
        outputs (:obj:`List[torch.FloatTensor], `optional`):
            Gradient results for each input.\end{DoxyVerb}
 \mbox{\Hypertarget{classbittensor_1_1__dendrite_1_1dendrite__mock_1_1_dendrite_mock_ae81c0740aa0705e26e7aad62de72e2f3}\label{classbittensor_1_1__dendrite_1_1dendrite__mock_1_1_dendrite_mock_ae81c0740aa0705e26e7aad62de72e2f3}} 
\index{bittensor.\_dendrite.dendrite\_mock.DendriteMock@{bittensor.\_dendrite.dendrite\_mock.DendriteMock}!forward@{forward}}
\index{forward@{forward}!bittensor.\_dendrite.dendrite\_mock.DendriteMock@{bittensor.\_dendrite.dendrite\_mock.DendriteMock}}
\doxysubsubsection{\texorpdfstring{forward()}{forward()}}
{\footnotesize\ttfamily  Tuple\mbox{[}torch.\+Tensor, ...\mbox{]} bittensor.\+\_\+dendrite.\+dendrite\+\_\+mock.\+Dendrite\+Mock.\+forward (\begin{DoxyParamCaption}\item[{}]{ctx,  }\item[{\textquotesingle{}\mbox{\hyperlink{classbittensor_1_1__dendrite_1_1dendrite__impl_1_1_dendrite}{bittensor.\+Dendrite}}\textquotesingle{}}]{dendrite,  }\item[{torch.\+Tensor}]{dummy,  }\item[{List\mbox{[}\textquotesingle{}\mbox{\hyperlink{classbittensor_1_1__endpoint_1_1endpoint__impl_1_1_endpoint}{bittensor.\+Endpoint}}\textquotesingle{}\mbox{]}}]{endpoints,  }\item[{bittensor.\+proto.\+Modality}]{modality,  }\item[{int}]{timeout,  }\item[{bool}]{requires\+\_\+grad,  }\item[{$\ast$torch.\+Tensor     }]{inputs }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [static]}}

\begin{DoxyVerb} Internal autograd-friendly Forward RPC call to a list of neuron endpoints.

    Args:
        ctx: (:obj:`torch.autograd.ctx`, `required`):
            Autograd context, saves state information between forward and backward calls. i.e. inputs for gradient computation.

        dendrite: (:obj:`bittensor.Dendrite`, `required`):
            Pointer to a bittensor dendrite object on which we are creating the forward requests.

        dummy: (:obj:`torch.Tensor`, `required`):
            Dummy torch tensor used to ensure that torch.backward computation is called on this function 
            regardless of the input types.

        endpoints (:obj:`List[bittensor.Endpoint']` of shape :obj:`(n_endpoints)`, `required`):
            List of endpoints which match length of inputs. Inputs are sent forward to these endpoints.

        modality (:obj:`bittensor.proto.Modality` of shape :obj:`(1)`, `required`):
            Bittensor forward modality or type ENUM [TEXT, IMAGE, TENSOR]

        inputs (:obj:`List[torch.Tensor]` of shape :obj:`(n_endpoints)`, `required`):
            List of torch tensors to be sent to the associated endpoints.

        timeout (int):
            request timeout.

        requires_grad (int, default = dendrite.requires_grad, `optional`):
            If true, the backward pass triggers passing gradients on the wire.

    Returns:
        codes (:obj:`torch.LongTensor` of shape :obj:`(n_endpoints)` `required`):
            Return code associated with forward call.

        times (:obj:`torch.FloatTensor` of shape :obj:`[ num_endpoints ]`, `required`):
            times per call.
        
        outputs (:obj:`List[torch.FloatTensor]` of shape :obj:`n_endpoints * (batch_size, sequence_len, bittensor.__network_dim__)`, `required`):
                Output encodings of inputs produced by the remote endpoints. Non-responses are zeroes of common shape.
\end{DoxyVerb}
 \mbox{\Hypertarget{classbittensor_1_1__dendrite_1_1dendrite__mock_1_1_dendrite_mock_aba4861e1bd42622eced10ec450fddfb5}\label{classbittensor_1_1__dendrite_1_1dendrite__mock_1_1_dendrite_mock_aba4861e1bd42622eced10ec450fddfb5}} 
\index{bittensor.\_dendrite.dendrite\_mock.DendriteMock@{bittensor.\_dendrite.dendrite\_mock.DendriteMock}!forward\_image@{forward\_image}}
\index{forward\_image@{forward\_image}!bittensor.\_dendrite.dendrite\_mock.DendriteMock@{bittensor.\_dendrite.dendrite\_mock.DendriteMock}}
\doxysubsubsection{\texorpdfstring{forward\_image()}{forward\_image()}}
{\footnotesize\ttfamily  Tuple\mbox{[}Union\mbox{[}List\mbox{[}torch.\+Float\+Tensor\mbox{]}, torch.\+Float\+Tensor\mbox{]}, torch.\+Long\+Tensor, torch.\+Float\+Tensor\mbox{]} bittensor.\+\_\+dendrite.\+dendrite\+\_\+mock.\+Dendrite\+Mock.\+forward\+\_\+image (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{Union\mbox{[}List\mbox{[}\textquotesingle{}\mbox{\hyperlink{classbittensor_1_1__endpoint_1_1endpoint__impl_1_1_endpoint}{bittensor.\+Endpoint}}\textquotesingle{}\mbox{]}, \textquotesingle{}\mbox{\hyperlink{classbittensor_1_1__endpoint_1_1endpoint__impl_1_1_endpoint}{bittensor.\+Endpoint}}\textquotesingle{}\mbox{]}}]{endpoints,  }\item[{List\mbox{[}torch.\+Float\+Tensor\mbox{]}}]{inputs,  }\item[{int }]{timeout = {\ttfamily None},  }\item[{bool }]{requires\+\_\+grad = {\ttfamily None} }\end{DoxyParamCaption})}

\begin{DoxyVerb} Forward image inputs to endpoints.

  Args:
        endpoints (:obj:`Union[List[bittensor.Endpoint], bittensor.Endpoint]` of shape :obj:`(num_endpoints)`, `required`):
            List or single of endpoints which match the length of inputs. Inputs are sent forward to these endpoints.

        inputs (:obj:`Union[List[torch.FloatTensor], torch.FloatTensor]` of shape :obj:`(num_endpoints * [ batch_size, sequence_len, channels, rows, cols ])`, `required`):
            List or single of image-tensors to send to corresponding endpoints. Tensors are images encoded using the
            torch.toTensor() or other encoding which produces the shape [batch_size, channels, rows, cols].

        timeout (int, default = dendrite.timeout `optional`):
            Request timeout.

        requires_grad (int, default = dendrite.requires_grad, `optional`):
            If true, the backward pass triggers passing gradients on the wire.

    Returns:
        responses (:obj:`Union[ List[torch.FloatTensor], torch.FloatTensor] ` of shape :obj:`(batch_size, sequence_len, bittensor.__network_dim__)`, `required`):
            Output encodings of inputs produced by remote endpoints. Non-responses are zeroes of input shape plus output dimension.

        codes (:obj:`torch.LongTensor` of shape :obj:`[ num_endpoints ]`, `required`):
            dendrite call return ops.

        times (:obj:`torch.FloatTensor` of shape :obj:`[ num_endpoints ]`, `required`):
            times per call.
\end{DoxyVerb}
 \mbox{\Hypertarget{classbittensor_1_1__dendrite_1_1dendrite__mock_1_1_dendrite_mock_a596804548b4ab12580fa1ce97fe36e54}\label{classbittensor_1_1__dendrite_1_1dendrite__mock_1_1_dendrite_mock_a596804548b4ab12580fa1ce97fe36e54}} 
\index{bittensor.\_dendrite.dendrite\_mock.DendriteMock@{bittensor.\_dendrite.dendrite\_mock.DendriteMock}!forward\_tensor@{forward\_tensor}}
\index{forward\_tensor@{forward\_tensor}!bittensor.\_dendrite.dendrite\_mock.DendriteMock@{bittensor.\_dendrite.dendrite\_mock.DendriteMock}}
\doxysubsubsection{\texorpdfstring{forward\_tensor()}{forward\_tensor()}}
{\footnotesize\ttfamily  Tuple\mbox{[}Union\mbox{[}List\mbox{[}torch.\+Float\+Tensor\mbox{]}, torch.\+Float\+Tensor\mbox{]}, torch.\+Long\+Tensor, torch.\+Float\+Tensor\mbox{]} bittensor.\+\_\+dendrite.\+dendrite\+\_\+mock.\+Dendrite\+Mock.\+forward\+\_\+tensor (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{Union\mbox{[}List\mbox{[}\textquotesingle{}\mbox{\hyperlink{classbittensor_1_1__endpoint_1_1endpoint__impl_1_1_endpoint}{bittensor.\+Endpoint}}\textquotesingle{}\mbox{]}, \textquotesingle{}\mbox{\hyperlink{classbittensor_1_1__endpoint_1_1endpoint__impl_1_1_endpoint}{bittensor.\+Endpoint}}\textquotesingle{}\mbox{]}}]{endpoints,  }\item[{List\mbox{[}torch.\+Float\+Tensor\mbox{]}}]{inputs,  }\item[{int }]{timeout = {\ttfamily None},  }\item[{bool }]{requires\+\_\+grad = {\ttfamily None} }\end{DoxyParamCaption})}

\begin{DoxyVerb} Forward tensor inputs to endpoints.

    Args:
        endpoints (:obj:`Union[List[bittensor.Endpoint], bittensor.Endpoint]` of shape :obj:`(num_endpoints)`, `required`):
            List or single of endpoints which match the length of inputs. Inputs are sent forward to these endpoints.

        inputs (:obj:`Union[List[torch.LongTensor], torch.LongTensor]` of shape :obj:`(num_endpoints * [batch_size, sequence_len])`, `required`):
            List or single tensors to send to corresponding endpoints. Tensors are of float type and
            with shape [batch_size, sequence_len, bittensor.__network_dim__].

        timeout (int, default = dendrite.timeout `optional`):
            Request timeout.

        requires_grad (int, default = dendrite.requires_grad, `optional`):
            If true, the backward pass triggers passing gradients on the wire.

    Returns:
        responses (:obj:`Union[ List[torch.FloatTensor], torch.FloatTensor] ` of shape :obj:`(batch_size, sequence_len, bittensor.__network_dim__)`, `required`):
            Output encodings of inputs produced by remote endpoints. Non-responses are zeroes of input shape plus output dimension.

        codes (:obj:`torch.LongTensor` of shape :obj:`[ num_endpoints ]`, `required`):
            dendrite call return ops.

        times (:obj:`torch.FloatTensor` of shape :obj:`[ num_endpoints ]`, `required`):
            times per call.
\end{DoxyVerb}
 \mbox{\Hypertarget{classbittensor_1_1__dendrite_1_1dendrite__mock_1_1_dendrite_mock_a05b1862d3fcc5a34924ff9d89424585d}\label{classbittensor_1_1__dendrite_1_1dendrite__mock_1_1_dendrite_mock_a05b1862d3fcc5a34924ff9d89424585d}} 
\index{bittensor.\_dendrite.dendrite\_mock.DendriteMock@{bittensor.\_dendrite.dendrite\_mock.DendriteMock}!forward\_text@{forward\_text}}
\index{forward\_text@{forward\_text}!bittensor.\_dendrite.dendrite\_mock.DendriteMock@{bittensor.\_dendrite.dendrite\_mock.DendriteMock}}
\doxysubsubsection{\texorpdfstring{forward\_text()}{forward\_text()}}
{\footnotesize\ttfamily  Tuple\mbox{[}Union\mbox{[}List\mbox{[}torch.\+Float\+Tensor\mbox{]}, torch.\+Float\+Tensor\mbox{]}, torch.\+Long\+Tensor, torch.\+Float\+Tensor\mbox{]} bittensor.\+\_\+dendrite.\+dendrite\+\_\+mock.\+Dendrite\+Mock.\+forward\+\_\+text (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{Union\mbox{[}                 torch.\+Long\+Tensor, List\mbox{[}torch.\+Long\+Tensor\mbox{]}, List\mbox{[}\textquotesingle{}\mbox{\hyperlink{classbittensor_1_1__endpoint_1_1endpoint__impl_1_1_endpoint}{bittensor.\+Endpoint}}\textquotesingle{}\mbox{]}, \textquotesingle{}\mbox{\hyperlink{classbittensor_1_1__endpoint_1_1endpoint__impl_1_1_endpoint}{bittensor.\+Endpoint}}\textquotesingle{}\mbox{]}}]{endpoints,  }\item[{Union\mbox{[}str, List\mbox{[}str\mbox{]}, List\mbox{[}torch.\+Long\+Tensor\mbox{]}, torch.\+Long\+Tensor\mbox{]}}]{inputs,  }\item[{int }]{timeout = {\ttfamily None},  }\item[{bool }]{requires\+\_\+grad = {\ttfamily None} }\end{DoxyParamCaption})}

\begin{DoxyVerb} Forward text inputs to a list of neuron endpoints and block until responses or timeout.

        Args:
            endpoints (:obj:`Union[torch.LongTensor, List[torch.LongTensor], List[bittensor.Endpoint], bittensor.Endpoint]` of shape :obj:`(num_endpoints)`, `required`):
                Endpoints to send inputs to. Endpoint can be one of the following types:
                    - a single endpoint tensor shape [250]
                    - a set of endpoint tensors shape [n, 250]
                    - a list of endpoints tensors each of shape [250]
                    - a single endpoint object. Inputs will be sent to this endpoint alone.
                    - a list of endpoint objects. All inputs will be sent to these endpoints.

            inputs (:obj:`Union[str,  List[str], List[torch.LongTensor], torch.LongTensor]` of shape :obj:`(num_endpoints * [batch_size, sequence_len])`, `required`):
                Tokenized sentences to send on the wire. Inputs can be one of the following types:
                    - a single string: the string will be tokenized using the bittensor tokenizer.
                    - a list of strings: the strings will be tokenized using the bittensor tokenizer.
                    - a tensor with shape [batch_size, sequence_len], assumed to be the output of bittensor tokenizer.
                    - a tensor with shape [n, batch_size, sequence_len], the operation will unbind the tensor and pass inputs to endpoints.
                If inputs are tensors they will be cast to int64 format before sending on the wire.

            timeout (:type:`int`, default = dendrite.timeout `optional`):
                Request timeout. Queries that do not respond will be replaced by zeros.

            requires_grad (:type:`int`, default = dendrite.requires_grad, `optional`):
                If true, the backward pass triggers passing gradients on the wire.

        Returns:
            responses (:obj:`torch.FloatTensor` of shape :obj:`(n, batch_size, sequence_len, bittensor.__network_dim__)`, `required`):
                Output encodings of inputs produced by remote endpoints. Non-responses are zeroes of input shape plus output dimension.
                The first dimension will match the number of endpoints queried.

            codes (:obj:`torch.LongTensor` of shape :obj:`[ num_endpoints ]`, `required`):
                dendrite call return ops.

            times (:obj:`torch.FloatTensor` of shape :obj:`[ num_endpoints ]`, `required`):
                times per call.
\end{DoxyVerb}
 \mbox{\Hypertarget{classbittensor_1_1__dendrite_1_1dendrite__mock_1_1_dendrite_mock_a7308a89e8472c87b19e6b1955d4b6a2f}\label{classbittensor_1_1__dendrite_1_1dendrite__mock_1_1_dendrite_mock_a7308a89e8472c87b19e6b1955d4b6a2f}} 
\index{bittensor.\_dendrite.dendrite\_mock.DendriteMock@{bittensor.\_dendrite.dendrite\_mock.DendriteMock}!to\_dataframe@{to\_dataframe}}
\index{to\_dataframe@{to\_dataframe}!bittensor.\_dendrite.dendrite\_mock.DendriteMock@{bittensor.\_dendrite.dendrite\_mock.DendriteMock}}
\doxysubsubsection{\texorpdfstring{to\_dataframe()}{to\_dataframe()}}
{\footnotesize\ttfamily def bittensor.\+\_\+dendrite.\+dendrite\+\_\+mock.\+Dendrite\+Mock.\+to\+\_\+dataframe (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{metagraph }\end{DoxyParamCaption})}

\begin{DoxyVerb} Return a stats info as a pandas dataframe indexed by the metagraph or pubkey if not existend.
Args:
metagraph: (bittensor.Metagraph):
    Indexes the stats data using metagraph hotkeys.
Return:
dataframe (:obj:`pandas.Dataframe`)
\end{DoxyVerb}
 \mbox{\Hypertarget{classbittensor_1_1__dendrite_1_1dendrite__mock_1_1_dendrite_mock_a19bd830edb5d9b7ec5224fb8fe613f63}\label{classbittensor_1_1__dendrite_1_1dendrite__mock_1_1_dendrite_mock_a19bd830edb5d9b7ec5224fb8fe613f63}} 
\index{bittensor.\_dendrite.dendrite\_mock.DendriteMock@{bittensor.\_dendrite.dendrite\_mock.DendriteMock}!to\_wandb@{to\_wandb}}
\index{to\_wandb@{to\_wandb}!bittensor.\_dendrite.dendrite\_mock.DendriteMock@{bittensor.\_dendrite.dendrite\_mock.DendriteMock}}
\doxysubsubsection{\texorpdfstring{to\_wandb()}{to\_wandb()}}
{\footnotesize\ttfamily def bittensor.\+\_\+dendrite.\+dendrite\+\_\+mock.\+Dendrite\+Mock.\+to\+\_\+wandb (\begin{DoxyParamCaption}\item[{}]{self }\end{DoxyParamCaption})}

\begin{DoxyVerb} Return a dictionary of dendrite stats as wandb logging info.
Args:
metagraph: (bittensor.Metagraph):
If not None, indexes the wandb data using int uids rather than string pubkeys.
Return:
wandb_info (:obj:`Dict`)
\end{DoxyVerb}
 \mbox{\Hypertarget{classbittensor_1_1__dendrite_1_1dendrite__mock_1_1_dendrite_mock_ae078e5bf68d851035d9c4e7a125f3611}\label{classbittensor_1_1__dendrite_1_1dendrite__mock_1_1_dendrite_mock_ae078e5bf68d851035d9c4e7a125f3611}} 
\index{bittensor.\_dendrite.dendrite\_mock.DendriteMock@{bittensor.\_dendrite.dendrite\_mock.DendriteMock}!update\_stats@{update\_stats}}
\index{update\_stats@{update\_stats}!bittensor.\_dendrite.dendrite\_mock.DendriteMock@{bittensor.\_dendrite.dendrite\_mock.DendriteMock}}
\doxysubsubsection{\texorpdfstring{update\_stats()}{update\_stats()}}
{\footnotesize\ttfamily def bittensor.\+\_\+dendrite.\+dendrite\+\_\+mock.\+Dendrite\+Mock.\+update\+\_\+stats (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{endpoints,  }\item[{}]{requests,  }\item[{}]{responses,  }\item[{}]{return\+\_\+ops,  }\item[{}]{query\+\_\+times }\end{DoxyParamCaption})}

\begin{DoxyVerb} Update dendrite stat according to the response we get from peers. Updates were saved to self.stats.
    Args:
        endpoints (:obj:`List[bittensor.Endpoint]` of shape :obj:`(num_endpoints)`, `required`):
            The set of endpoints that dendrite sent request to.

        requests (List[torch.Tensor] of shape :obj:`[ num_endpoints ]`, `required`):
            Requests from the call.

        responses (List[torch.FloatTensor] of shape :obj:`[ num_endpoints ]`, `required`):
            Responses from the call.

        return_ops (:obj:`torch.LongTensor` of shape :obj:`[ num_endpoints ]`, `required`):
            Dendrite call return ops.

        query_times (:obj:`torch.FloatTensor` of shape :obj:`[ num_endpoints ]`, `required`):
            Times per call.
\end{DoxyVerb}
 

The documentation for this class was generated from the following file\+:\begin{DoxyCompactItemize}
\item 
/\+Users/macthrasher/bittensor/bittensor/\+\_\+dendrite/dendrite\+\_\+mock.\+py\end{DoxyCompactItemize}
